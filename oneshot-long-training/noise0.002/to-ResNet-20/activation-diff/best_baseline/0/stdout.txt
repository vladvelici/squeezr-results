start iteration 0
[activation diff]: block to remove picked: 33, with score 0.007062. All blocks and scores: [(33, 0.007061996380798519), (32, 0.009233050746843219), (30, 0.010039400891400874), (31, 0.010361599968746305), (34, 0.013312276219949126), (29, 0.013541154330596328), (35, 0.01601846213452518), (26, 0.016037590568885207), (28, 0.017728674924001098), (27, 0.01912704948335886), (43, 0.020232456270605326), (46, 0.02104453998617828), (25, 0.021972602233290672), (23, 0.022379535483196378), (41, 0.022826648084446788), (44, 0.023395078722387552), (40, 0.024025025311857462), (45, 0.024295411072671413), (21, 0.024924597702920437), (22, 0.025168768595904112), (48, 0.025341259315609932), (24, 0.025899536907672882), (50, 0.026409971993416548), (42, 0.026674100197851658), (20, 0.02685900731012225), (49, 0.02703716466203332), (47, 0.029306468553841114), (39, 0.031570712802931666), (38, 0.03163787117227912), (15, 0.03192339092493057), (7, 0.03228544630110264), (19, 0.0326285962946713), (37, 0.03796026110649109), (51, 0.04173417342826724), (9, 0.04340187972411513), (6, 0.04660903150215745), (4, 0.047493684105575085), (14, 0.04783663572743535), (2, 0.05454846564680338), (3, 0.057224276941269636), (13, 0.05892290221527219), (11, 0.05924912728369236), (17, 0.06095684785395861), (0, 0.06300980923697352), (1, 0.06676734331995249), (52, 0.06862937472760677), (8, 0.07467832136899233), (10, 0.0803448436781764), (16, 0.08408282697200775), (12, 0.09042049385607243), (5, 0.10667387302964926), (36, 0.43758000805974007), (18, 0.5108212903141975), (53, 0.8211489021778107)]
computing accuracy for after removing block 33 . block score: 0.007061996380798519
removed block 33 current accuracy 1.0 loss from initial  0.0
since last training loss: 0.0 threshold 999.0 training needed False
start iteration 1
[activation diff]: block to remove picked: 32, with score 0.009233. All blocks and scores: [(32, 0.009233050514012575), (30, 0.01003940065857023), (31, 0.010361599852330983), (34, 0.01313394762109965), (29, 0.013541154912672937), (26, 0.016037590336054564), (35, 0.016169289825484157), (28, 0.01772867562249303), (27, 0.019127048552036285), (43, 0.020072476472705603), (46, 0.020731385331600904), (25, 0.02197260269895196), (41, 0.02234709309414029), (23, 0.022379535250365734), (44, 0.023235687287524343), (40, 0.0238410672172904), (45, 0.023965541971847415), (48, 0.024917916161939502), (21, 0.024924598168581724), (22, 0.02516876789741218), (50, 0.025840812595561147), (24, 0.025899537838995457), (42, 0.02631532307714224), (49, 0.02665567398071289), (20, 0.026859006844460964), (47, 0.02872879756614566), (39, 0.03131764242425561), (38, 0.031380363274365664), (15, 0.03192339092493057), (7, 0.03228544630110264), (19, 0.0326285962946713), (37, 0.03802584344521165), (51, 0.04122393997386098), (9, 0.04340187832713127), (6, 0.04660903196781874), (4, 0.0474936836399138), (14, 0.047836634796112776), (2, 0.054548466578125954), (3, 0.057224276941269636), (13, 0.05892290407791734), (11, 0.059249129611998796), (17, 0.06095685018226504), (0, 0.06300981063395739), (1, 0.06676734052598476), (52, 0.06745155062526464), (8, 0.07467832323163748), (10, 0.0803448399528861), (16, 0.08408282604068518), (12, 0.0904204910621047), (5, 0.10667386837303638), (36, 0.43538709729909897), (18, 0.5108212903141975), (53, 0.8222573921084404)]
computing accuracy for after removing block 32 . block score: 0.009233050514012575
removed block 32 current accuracy 0.9998 loss from initial  0.00019999999999997797
since last training loss: 0.00019999999999997797 threshold 999.0 training needed False
start iteration 2
[activation diff]: block to remove picked: 30, with score 0.010039. All blocks and scores: [(30, 0.01003940065857023), (31, 0.010361599968746305), (34, 0.012765232939273119), (29, 0.013541154796257615), (35, 0.015992750879377127), (26, 0.016037590568885207), (28, 0.017728675389662385), (27, 0.019127048319205642), (43, 0.020075132371857762), (46, 0.020841406425461173), (25, 0.021972602931782603), (41, 0.02231976669281721), (23, 0.022379535483196378), (44, 0.023154049646109343), (40, 0.023885682923719287), (45, 0.024071688996627927), (48, 0.024877464631572366), (21, 0.02492459793575108), (22, 0.02516876789741218), (50, 0.02569117769598961), (24, 0.025899537838995457), (42, 0.026123747695237398), (49, 0.02647942234762013), (20, 0.026859007542952895), (47, 0.02869313140399754), (38, 0.031236796407029033), (39, 0.031295292312279344), (15, 0.03192339139059186), (7, 0.03228544630110264), (19, 0.032628596760332584), (37, 0.038376690819859505), (51, 0.041114033199846745), (9, 0.04340188065543771), (6, 0.04660903010517359), (4, 0.04749368457123637), (14, 0.0478366338647902), (2, 0.05454846518114209), (3, 0.05722427787259221), (13, 0.058922900818288326), (11, 0.05924913054332137), (17, 0.060956848319619894), (0, 0.06300981063395739), (1, 0.06676734145730734), (52, 0.06700456328690052), (8, 0.0746783223003149), (10, 0.08034484274685383), (16, 0.08408282697200775), (12, 0.090420494787395), (5, 0.10667387302964926), (36, 0.43640001118183136), (18, 0.5108213052153587), (53, 0.8289348930120468)]
computing accuracy for after removing block 30 . block score: 0.01003940065857023
removed block 30 current accuracy 0.9998 loss from initial  0.00019999999999997797
since last training loss: 0.00019999999999997797 threshold 999.0 training needed False
start iteration 3
[activation diff]: block to remove picked: 31, with score 0.010375. All blocks and scores: [(31, 0.010375372134149075), (34, 0.012387836701236665), (29, 0.013541154796257615), (35, 0.016008096281439066), (26, 0.016037590568885207), (28, 0.01772867562249303), (27, 0.019127048086374998), (43, 0.02008363325148821), (46, 0.020704444032162428), (25, 0.02197260269895196), (41, 0.02225319715216756), (23, 0.022379535250365734), (44, 0.02326776087284088), (40, 0.02401388017460704), (45, 0.024092992302030325), (48, 0.024665280943736434), (21, 0.024924597702920437), (22, 0.025168768595904112), (50, 0.025459734722971916), (42, 0.02565571293234825), (24, 0.025899537140503526), (49, 0.026287756161764264), (20, 0.02685900731012225), (47, 0.028363423654809594), (38, 0.031047647586092353), (39, 0.03138077212497592), (15, 0.031923390459269285), (7, 0.03228544630110264), (19, 0.032628596760332584), (37, 0.03897124528884888), (51, 0.040756202302873135), (9, 0.043401881121098995), (6, 0.04660903196781874), (4, 0.04749368550255895), (14, 0.04783663433045149), (2, 0.05454846424981952), (3, 0.05722427926957607), (13, 0.058922900818288326), (11, 0.05924912774935365), (17, 0.06095684785395861), (0, 0.06300980923697352), (52, 0.06586316134780645), (1, 0.06676734052598476), (8, 0.07467832136899233), (10, 0.08034484460949898), (16, 0.0840828288346529), (12, 0.09042049571871758), (5, 0.10667387023568153), (36, 0.4389924593269825), (18, 0.5108213052153587), (53, 0.8391561582684517)]
computing accuracy for after removing block 31 . block score: 0.010375372134149075
removed block 31 current accuracy 0.9998 loss from initial  0.00019999999999997797
since last training loss: 0.00019999999999997797 threshold 999.0 training needed False
start iteration 4
[activation diff]: block to remove picked: 34, with score 0.012490. All blocks and scores: [(34, 0.012489619781263173), (29, 0.013541154563426971), (26, 0.01603759010322392), (35, 0.01605736347846687), (28, 0.01772867562249303), (27, 0.019127047853544354), (43, 0.020049349404871464), (46, 0.0205529872328043), (25, 0.021972602466121316), (41, 0.02206748374737799), (23, 0.022379535250365734), (44, 0.02297913283109665), (40, 0.023858346976339817), (45, 0.024124702205881476), (48, 0.024386122822761536), (21, 0.024924598401412368), (50, 0.025042241672053933), (22, 0.025168768828734756), (42, 0.025414508301764727), (49, 0.025842699455097318), (24, 0.025899537606164813), (20, 0.026859007542952895), (47, 0.028050734428688884), (38, 0.031040059635415673), (39, 0.031500803073868155), (15, 0.03192339278757572), (7, 0.032285446766763926), (19, 0.03262859582901001), (37, 0.03911284822970629), (51, 0.04024627385661006), (9, 0.04340188018977642), (6, 0.046609032433480024), (4, 0.04749368503689766), (14, 0.0478366338647902), (2, 0.05454846564680338), (3, 0.05722428020089865), (13, 0.0589228980243206), (11, 0.05924912774935365), (17, 0.06095685018226504), (0, 0.06300980970263481), (52, 0.06486208830028772), (1, 0.06676734052598476), (8, 0.07467832136899233), (10, 0.08034484181553125), (16, 0.08408282976597548), (12, 0.09042049199342728), (5, 0.10667387209832668), (36, 0.4381278343498707), (18, 0.5108213126659393), (53, 0.8458427712321281)]
computing accuracy for after removing block 34 . block score: 0.012489619781263173
removed block 34 current accuracy 0.9998 loss from initial  0.00019999999999997797
since last training loss: 0.00019999999999997797 threshold 999.0 training needed False
start iteration 5
[activation diff]: block to remove picked: 29, with score 0.013541. All blocks and scores: [(29, 0.013541154679842293), (26, 0.016037591034546494), (35, 0.01665342040359974), (28, 0.017728675855323672), (27, 0.019127047853544354), (43, 0.02050345577299595), (46, 0.020725322421640158), (25, 0.021972602466121316), (23, 0.02237953571602702), (41, 0.022452629171311855), (44, 0.023364474764093757), (48, 0.024290355388075113), (45, 0.02443871204741299), (40, 0.02447055815719068), (21, 0.024924598401412368), (50, 0.025042172987014055), (22, 0.02516876789741218), (49, 0.025875969557091594), (24, 0.025899537140503526), (42, 0.02620540652424097), (20, 0.026859006145969033), (47, 0.028178582666441798), (15, 0.031923392321914434), (38, 0.03208350157365203), (7, 0.032285446766763926), (39, 0.032337441109120846), (19, 0.032628594897687435), (51, 0.03994725877419114), (37, 0.040739682503044605), (9, 0.04340187972411513), (6, 0.04660903150215745), (4, 0.047493684105575085), (14, 0.0478366338647902), (2, 0.05454846704378724), (3, 0.05722427647560835), (13, 0.058922902680933475), (11, 0.059249128215014935), (17, 0.06095684925094247), (0, 0.06300980970263481), (52, 0.06433630175888538), (1, 0.06676734331995249), (8, 0.07467832323163748), (10, 0.0803448436781764), (16, 0.08408282790333033), (12, 0.09042049385607243), (5, 0.10667387023568153), (36, 0.45053430646657944), (18, 0.5108212903141975), (53, 0.8443200513720512)]
computing accuracy for after removing block 29 . block score: 0.013541154679842293
removed block 29 current accuracy 0.9996 loss from initial  0.00039999999999995595
since last training loss: 0.00039999999999995595 threshold 999.0 training needed False
start iteration 6
[activation diff]: block to remove picked: 26, with score 0.016038. All blocks and scores: [(26, 0.016037590568885207), (35, 0.016470608301460743), (28, 0.017728674924001098), (27, 0.019127049017697573), (43, 0.020046867663040757), (46, 0.020376994041725993), (41, 0.02172324270941317), (25, 0.021972603164613247), (23, 0.022379534784704447), (44, 0.02302833623252809), (48, 0.023771876003593206), (40, 0.023930813651531935), (45, 0.024178663035854697), (50, 0.024390298640355468), (21, 0.024924598401412368), (22, 0.02516876789741218), (42, 0.025188251631334424), (49, 0.02536152838729322), (24, 0.025899537140503526), (20, 0.026859006378799677), (47, 0.027363278903067112), (38, 0.03136561857536435), (15, 0.03192339139059186), (39, 0.032127685844898224), (7, 0.03228544583544135), (19, 0.0326285962946713), (51, 0.03893592394888401), (37, 0.040206342469900846), (9, 0.043401879258453846), (6, 0.04660903150215745), (4, 0.047493684105575085), (14, 0.047836633399128914), (2, 0.05454846564680338), (3, 0.05722427647560835), (13, 0.05892290314659476), (11, 0.059249126352369785), (17, 0.06095684738829732), (52, 0.06232854910194874), (0, 0.06300980970263481), (1, 0.06676734331995249), (8, 0.07467832043766975), (10, 0.08034484181553125), (16, 0.08408282604068518), (12, 0.090420494787395), (5, 0.10667387116700411), (36, 0.4444201923906803), (18, 0.5108213052153587), (53, 0.8537911996245384)]
computing accuracy for after removing block 26 . block score: 0.016037590568885207
removed block 26 current accuracy 0.9986 loss from initial  0.0013999999999999568
since last training loss: 0.0013999999999999568 threshold 999.0 training needed False
start iteration 7
[activation diff]: block to remove picked: 35, with score 0.015597. All blocks and scores: [(35, 0.015597365912981331), (28, 0.017088500317186117), (27, 0.018882447155192494), (43, 0.019595165038481355), (46, 0.020073581021279097), (41, 0.020961584523320198), (25, 0.02197260269895196), (23, 0.02237953571602702), (44, 0.022814956260845065), (48, 0.02312816120684147), (40, 0.023345196153968573), (50, 0.02375614712946117), (42, 0.02384730288758874), (45, 0.02387388003990054), (21, 0.024924597470089793), (49, 0.02496031648479402), (22, 0.025168768828734756), (24, 0.025899537140503526), (47, 0.026855542790144682), (20, 0.02685900731012225), (38, 0.030424013501033187), (39, 0.031514044385403395), (15, 0.03192339139059186), (7, 0.032285446766763926), (19, 0.0326285962946713), (51, 0.03782488079741597), (37, 0.039368352852761745), (9, 0.04340187972411513), (6, 0.04660903150215745), (4, 0.04749368457123637), (14, 0.047836634796112776), (2, 0.054548464715480804), (3, 0.05722428113222122), (13, 0.058922902680933475), (11, 0.05924912774935365), (52, 0.060332820285111666), (17, 0.06095684925094247), (0, 0.06300980923697352), (1, 0.06676734238862991), (8, 0.07467832509428263), (10, 0.08034484274685383), (16, 0.08408282790333033), (12, 0.09042049292474985), (5, 0.10667386930435896), (36, 0.4360685423016548), (18, 0.5108213052153587), (53, 0.8749377280473709)]
computing accuracy for after removing block 35 . block score: 0.015597365912981331
removed block 35 current accuracy 0.9964 loss from initial  0.0036000000000000476
since last training loss: 0.0036000000000000476 threshold 999.0 training needed False
start iteration 8
[activation diff]: block to remove picked: 28, with score 0.017088. All blocks and scores: [(28, 0.01708849985152483), (43, 0.018555945251137018), (27, 0.01888244692236185), (46, 0.019160084892064333), (41, 0.019424294820055366), (48, 0.02146727219223976), (25, 0.021972602931782603), (44, 0.022026916965842247), (40, 0.022179661318659782), (42, 0.02220643009059131), (50, 0.022256129421293736), (23, 0.022379535250365734), (45, 0.022931481711566448), (49, 0.02370851277373731), (21, 0.024924597702920437), (22, 0.025168768130242825), (47, 0.025829139398410916), (24, 0.025899536907672882), (20, 0.02685900661163032), (38, 0.02895654644817114), (39, 0.029667828930541873), (15, 0.03192339092493057), (7, 0.03228544630110264), (19, 0.0326285962946713), (51, 0.03600902622565627), (37, 0.03651238651946187), (9, 0.04340187879279256), (6, 0.04660903196781874), (4, 0.04749368457123637), (14, 0.047836634796112776), (2, 0.054548466112464666), (52, 0.05610728729516268), (3, 0.05722427740693092), (13, 0.05892290221527219), (11, 0.05924912774935365), (17, 0.06095685018226504), (0, 0.06300980923697352), (1, 0.06676734238862991), (8, 0.07467832509428263), (10, 0.08034484274685383), (16, 0.08408282790333033), (12, 0.0904204910621047), (5, 0.10667387116700411), (36, 0.4175764471292496), (18, 0.5108213126659393), (53, 0.9117145165801048)]
computing accuracy for after removing block 28 . block score: 0.01708849985152483
removed block 28 current accuracy 0.9962 loss from initial  0.0038000000000000256
since last training loss: 0.0038000000000000256 threshold 999.0 training needed False
start iteration 9
[activation diff]: block to remove picked: 43, with score 0.018140. All blocks and scores: [(43, 0.018140302505344152), (46, 0.01865610247477889), (41, 0.01884901849552989), (27, 0.01888244692236185), (48, 0.020903734490275383), (42, 0.02143200463615358), (40, 0.0218324214220047), (44, 0.021840530913323164), (50, 0.021869864081963897), (25, 0.021972602466121316), (23, 0.02237953501753509), (45, 0.02249284810386598), (49, 0.02312349807471037), (21, 0.024924597702920437), (47, 0.02506713941693306), (22, 0.025168768595904112), (24, 0.025899537606164813), (20, 0.026859007077291608), (38, 0.028114068554714322), (39, 0.02920690947212279), (15, 0.031923392321914434), (7, 0.032285446766763926), (19, 0.03262859582901001), (51, 0.0354543374851346), (37, 0.035977639723569155), (9, 0.04340187972411513), (6, 0.04660903103649616), (4, 0.04749368503689766), (14, 0.04783663293346763), (2, 0.05454846424981952), (52, 0.05469645978882909), (3, 0.05722427787259221), (13, 0.058922900818288326), (11, 0.05924912774935365), (17, 0.060956848319619894), (0, 0.06300980737432837), (1, 0.06676734331995249), (8, 0.07467832136899233), (10, 0.08034484460949898), (16, 0.0840828288346529), (12, 0.09042049292474985), (5, 0.10667387302964926), (36, 0.4135979115962982), (18, 0.5108212977647781), (53, 0.924663245677948)]
computing accuracy for after removing block 43 . block score: 0.018140302505344152
removed block 43 current accuracy 0.9952 loss from initial  0.0048000000000000265
since last training loss: 0.0048000000000000265 threshold 999.0 training needed False
start iteration 10
[activation diff]: block to remove picked: 41, with score 0.018849. All blocks and scores: [(41, 0.018849018029868603), (27, 0.018882447155192494), (46, 0.019302030326798558), (42, 0.021432004403322935), (48, 0.021544843912124634), (40, 0.021832421654835343), (50, 0.02194626978598535), (25, 0.02197260269895196), (23, 0.022379535483196378), (49, 0.023006869480013847), (44, 0.02310851006768644), (45, 0.023535606916993856), (21, 0.024924597702920437), (22, 0.025168768130242825), (47, 0.0258204466663301), (24, 0.025899537140503526), (20, 0.026859008008614182), (38, 0.028114069486036897), (39, 0.02920691017061472), (15, 0.03192339185625315), (7, 0.0322854476980865), (19, 0.03262859536334872), (51, 0.035091488622128963), (37, 0.03597763925790787), (9, 0.04340188018977642), (6, 0.04660903150215745), (4, 0.0474936836399138), (14, 0.04783663433045149), (52, 0.05332902818918228), (2, 0.054548464715480804), (3, 0.057224278803914785), (13, 0.0589229017496109), (11, 0.059249128215014935), (17, 0.060956848319619894), (0, 0.0630098101682961), (1, 0.06676734331995249), (8, 0.07467832323163748), (10, 0.0803448436781764), (16, 0.0840828288346529), (12, 0.090420494787395), (5, 0.10667387116700411), (36, 0.4135979153215885), (18, 0.5108213052153587), (53, 0.9678284153342247)]
computing accuracy for after removing block 41 . block score: 0.018849018029868603
removed block 41 current accuracy 0.993 loss from initial  0.007000000000000006
since last training loss: 0.007000000000000006 threshold 999.0 training needed False
start iteration 11
[activation diff]: block to remove picked: 27, with score 0.018882. All blocks and scores: [(27, 0.01888244692236185), (46, 0.019070088863372803), (48, 0.020678168395534158), (50, 0.021344397449865937), (40, 0.021832421654835343), (25, 0.021972602931782603), (42, 0.02198694017715752), (23, 0.02237953618168831), (49, 0.022534747608006), (45, 0.023929917020723224), (44, 0.02405400271527469), (21, 0.024924598401412368), (22, 0.025168768130242825), (24, 0.025899537606164813), (47, 0.026043936843052506), (20, 0.026859008241444826), (38, 0.02811406902037561), (39, 0.029206908773630857), (15, 0.03192339185625315), (7, 0.03228544630110264), (19, 0.032628596760332584), (51, 0.03379448037594557), (37, 0.03597764018923044), (9, 0.04340187972411513), (6, 0.04660903196781874), (4, 0.0474936836399138), (14, 0.047836634796112776), (52, 0.050476094242185354), (2, 0.054548464715480804), (3, 0.05722427787259221), (13, 0.05892290221527219), (11, 0.05924912868067622), (17, 0.06095685018226504), (0, 0.06300981063395739), (1, 0.06676734331995249), (8, 0.0746783223003149), (10, 0.08034484647214413), (16, 0.08408282697200775), (12, 0.09042049292474985), (5, 0.10667386930435896), (36, 0.4135979041457176), (18, 0.5108212903141975), (53, 1.0278180092573166)]
computing accuracy for after removing block 27 . block score: 0.01888244692236185
removed block 27 current accuracy 0.987 loss from initial  0.013000000000000012
since last training loss: 0.013000000000000012 threshold 999.0 training needed False
start iteration 12
[activation diff]: block to remove picked: 46, with score 0.018664. All blocks and scores: [(46, 0.018664462491869926), (48, 0.01998970750719309), (50, 0.020775062032043934), (40, 0.02108595333993435), (42, 0.021369647700339556), (49, 0.021910030394792557), (25, 0.021972603630274534), (23, 0.022379535250365734), (44, 0.02323931222781539), (45, 0.02358530880883336), (21, 0.02492459793575108), (47, 0.025076948339119554), (22, 0.025168767431750894), (24, 0.025899537140503526), (20, 0.026859006844460964), (38, 0.02718336065299809), (39, 0.028580758487805724), (15, 0.03192339139059186), (7, 0.03228544723242521), (19, 0.03262859536334872), (51, 0.03281425987370312), (37, 0.03542024362832308), (9, 0.04340187879279256), (6, 0.04660903103649616), (4, 0.04749368317425251), (14, 0.04783663293346763), (52, 0.04852363234385848), (2, 0.054548466578125954), (3, 0.05722427787259221), (13, 0.058922901283949614), (11, 0.059249126352369785), (17, 0.060956848319619894), (0, 0.06300980923697352), (1, 0.06676734238862991), (8, 0.07467832043766975), (10, 0.08034484181553125), (16, 0.08408282697200775), (12, 0.090420494787395), (5, 0.10667387302964926), (36, 0.4065233990550041), (18, 0.5108212977647781), (53, 1.0384205132722855)]
computing accuracy for after removing block 46 . block score: 0.018664462491869926
removed block 46 current accuracy 0.98 loss from initial  0.020000000000000018
since last training loss: 0.020000000000000018 threshold 999.0 training needed False
start iteration 13
[activation diff]: block to remove picked: 48, with score 0.020328. All blocks and scores: [(48, 0.020327560603618622), (50, 0.02083116164430976), (40, 0.02108595333993435), (42, 0.021369647700339556), (25, 0.02197260200046003), (23, 0.02237953571602702), (49, 0.022536989534273744), (44, 0.023239311296492815), (45, 0.023585307877510786), (21, 0.02492459723725915), (22, 0.02516876789741218), (24, 0.02589953737333417), (47, 0.026583049213513732), (20, 0.02685900777578354), (38, 0.027183360420167446), (39, 0.028580758487805724), (15, 0.031923392321914434), (7, 0.03228544723242521), (19, 0.0326285962946713), (51, 0.03285081218928099), (37, 0.035420244093984365), (9, 0.04340188065543771), (6, 0.04660903103649616), (4, 0.04749368270859122), (14, 0.04783663572743535), (52, 0.04812479903921485), (2, 0.05454846564680338), (3, 0.05722427647560835), (13, 0.0589229017496109), (11, 0.059249126352369785), (17, 0.060956849716603756), (0, 0.06300980923697352), (1, 0.06676734238862991), (8, 0.07467832416296005), (10, 0.08034484460949898), (16, 0.0840828288346529), (12, 0.09042049385607243), (5, 0.10667387116700411), (36, 0.4065234027802944), (18, 0.5108213052153587), (53, 1.1537711322307587)]
computing accuracy for after removing block 48 . block score: 0.020327560603618622
removed block 48 current accuracy 0.974 loss from initial  0.026000000000000023
since last training loss: 0.026000000000000023 threshold 999.0 training needed False
start iteration 14
[activation diff]: block to remove picked: 40, with score 0.021086. All blocks and scores: [(40, 0.021085953572764993), (42, 0.021369647700339556), (25, 0.021972602931782603), (23, 0.022379534784704447), (50, 0.02247006306424737), (44, 0.023239311762154102), (45, 0.023585308343172073), (21, 0.024924598401412368), (22, 0.025168768130242825), (49, 0.025234102504327893), (24, 0.025899536907672882), (47, 0.026583049446344376), (20, 0.026859006844460964), (38, 0.027183361118659377), (39, 0.02858075895346701), (15, 0.03192339185625315), (7, 0.032285446766763926), (19, 0.03262859582901001), (51, 0.03296921215951443), (37, 0.03542024455964565), (9, 0.04340187972411513), (6, 0.04660903196781874), (4, 0.04749368550255895), (14, 0.047836634796112776), (52, 0.05089045129716396), (2, 0.05454846518114209), (3, 0.057224276941269636), (13, 0.058922900818288326), (11, 0.059249129611998796), (17, 0.06095685018226504), (0, 0.06300980970263481), (1, 0.06676734331995249), (8, 0.07467832323163748), (10, 0.08034484088420868), (16, 0.08408282790333033), (12, 0.09042049292474985), (5, 0.10667387116700411), (36, 0.4065233916044235), (18, 0.5108212903141975), (53, 1.2663909047842026)]
computing accuracy for after removing block 40 . block score: 0.021085953572764993
removed block 40 current accuracy 0.9598 loss from initial  0.040200000000000014
since last training loss: 0.040200000000000014 threshold 999.0 training needed False
start iteration 15
[activation diff]: block to remove picked: 42, with score 0.020969. All blocks and scores: [(42, 0.020968681201338768), (50, 0.021284765796735883), (25, 0.021972603164613247), (23, 0.022379535250365734), (45, 0.023098317673429847), (44, 0.024240857688710093), (49, 0.024500868748873472), (21, 0.024924597470089793), (22, 0.025168768828734756), (24, 0.02589953737333417), (47, 0.026519698090851307), (20, 0.026859007077291608), (38, 0.027183360885828733), (39, 0.02858075825497508), (15, 0.03192339139059186), (51, 0.03222084930166602), (7, 0.032285446766763926), (19, 0.03262859582901001), (37, 0.03542024316266179), (9, 0.043401881121098995), (6, 0.04660903103649616), (4, 0.04749368457123637), (14, 0.047836633399128914), (52, 0.048857572954148054), (2, 0.054548467975109816), (3, 0.05722427740693092), (13, 0.05892290221527219), (11, 0.05924912914633751), (17, 0.06095684785395861), (0, 0.06300980783998966), (1, 0.06676734145730734), (8, 0.07467832043766975), (10, 0.08034484088420868), (16, 0.08408282790333033), (12, 0.09042049292474985), (5, 0.10667387209832668), (36, 0.4065234027802944), (18, 0.5108213126659393), (53, 1.3718615770339966)]
computing accuracy for after removing block 42 . block score: 0.020968681201338768
removed block 42 current accuracy 0.946 loss from initial  0.05400000000000005
since last training loss: 0.05400000000000005 threshold 999.0 training needed False
start iteration 16
[activation diff]: block to remove picked: 50, with score 0.021203. All blocks and scores: [(50, 0.021202658535912633), (25, 0.021972602931782603), (23, 0.02237953571602702), (45, 0.02376196626573801), (49, 0.02460233890451491), (44, 0.02471218118444085), (21, 0.024924597702920437), (22, 0.025168768363073468), (24, 0.02589953737333417), (47, 0.02622047532349825), (20, 0.026859006378799677), (38, 0.027183361817151308), (39, 0.028580758487805724), (51, 0.03127906727604568), (15, 0.03192339139059186), (7, 0.03228544583544135), (19, 0.03262859582901001), (37, 0.03542024316266179), (9, 0.04340187879279256), (52, 0.046101709827780724), (6, 0.04660903103649616), (4, 0.04749368550255895), (14, 0.047836634796112776), (2, 0.05454846890643239), (3, 0.05722427740693092), (13, 0.058922901283949614), (11, 0.059249129611998796), (17, 0.060956848319619894), (0, 0.06300980923697352), (1, 0.06676734145730734), (8, 0.07467832323163748), (10, 0.08034484181553125), (16, 0.08408282790333033), (12, 0.09042049199342728), (5, 0.10667387116700411), (36, 0.4065233953297138), (18, 0.5108213201165199), (53, 1.4178234040737152)]
computing accuracy for after removing block 50 . block score: 0.021202658535912633
removed block 50 current accuracy 0.9264 loss from initial  0.0736
since last training loss: 0.0736 threshold 999.0 training needed False
start iteration 17
[activation diff]: block to remove picked: 25, with score 0.021973. All blocks and scores: [(25, 0.02197260269895196), (23, 0.02237953571602702), (45, 0.023761966498568654), (49, 0.02460233890451491), (44, 0.02471218118444085), (21, 0.024924598168581724), (22, 0.025168768363073468), (24, 0.025899536907672882), (47, 0.026220475090667605), (20, 0.026859007077291608), (38, 0.027183360420167446), (39, 0.02858075895346701), (15, 0.03192339278757572), (7, 0.03228544630110264), (19, 0.03262859582901001), (51, 0.03344302158802748), (37, 0.03542024502530694), (9, 0.04340187832713127), (6, 0.04660903289914131), (4, 0.047493685968220234), (14, 0.04783663433045149), (52, 0.052651794627308846), (2, 0.054548464715480804), (3, 0.057224276941269636), (13, 0.0589229017496109), (11, 0.05924912914633751), (17, 0.060956848319619894), (0, 0.06300980970263481), (1, 0.06676734238862991), (8, 0.0746783223003149), (10, 0.08034484181553125), (16, 0.0840828288346529), (12, 0.09042049292474985), (5, 0.10667387023568153), (36, 0.4065233953297138), (18, 0.5108213052153587), (53, 1.6287681311368942)]
computing accuracy for after removing block 25 . block score: 0.02197260269895196
removed block 25 current accuracy 0.9132 loss from initial  0.08679999999999999
since last training loss: 0.08679999999999999 threshold 999.0 training needed False
start iteration 18
[activation diff]: block to remove picked: 23, with score 0.022380. All blocks and scores: [(23, 0.022379535250365734), (45, 0.0233820837456733), (49, 0.02386031555943191), (44, 0.02394806663505733), (21, 0.02492459793575108), (22, 0.025168768130242825), (47, 0.025361903477460146), (24, 0.02589953737333417), (38, 0.0265332052949816), (20, 0.026859006145969033), (39, 0.028472805628553033), (15, 0.03192339185625315), (7, 0.03228544583544135), (51, 0.03247324749827385), (19, 0.03262859582901001), (37, 0.03485476737841964), (9, 0.04340188065543771), (6, 0.04660903103649616), (4, 0.04749368457123637), (14, 0.0478366338647902), (52, 0.05042571295052767), (2, 0.05454846564680338), (3, 0.05722427787259221), (13, 0.058922902680933475), (11, 0.05924912868067622), (17, 0.06095684692263603), (0, 0.06300980737432837), (1, 0.06676734331995249), (8, 0.0746783223003149), (10, 0.0803448436781764), (16, 0.08408282976597548), (12, 0.09042049199342728), (5, 0.10667387116700411), (36, 0.3996613621711731), (18, 0.5108212977647781), (53, 1.631172239780426)]
computing accuracy for after removing block 23 . block score: 0.022379535250365734
removed block 23 current accuracy 0.8946 loss from initial  0.10540000000000005
since last training loss: 0.10540000000000005 threshold 999.0 training needed False
start iteration 19
[activation diff]: block to remove picked: 44, with score 0.023563. All blocks and scores: [(44, 0.02356328465975821), (45, 0.023582331370562315), (49, 0.02370715793222189), (24, 0.02455138321965933), (47, 0.024688831064850092), (21, 0.024924597470089793), (22, 0.025168768130242825), (38, 0.026409979443997145), (20, 0.02685900661163032), (39, 0.02843296993523836), (15, 0.031923392321914434), (7, 0.03228544630110264), (51, 0.03235368151217699), (19, 0.0326285962946713), (37, 0.03590833907946944), (9, 0.043401879258453846), (6, 0.04660903103649616), (4, 0.047493685968220234), (14, 0.04783663433045149), (52, 0.04885636828839779), (2, 0.054548464715480804), (3, 0.05722427787259221), (13, 0.05892290221527219), (11, 0.05924912681803107), (17, 0.06095684878528118), (0, 0.06300980830565095), (1, 0.06676734145730734), (8, 0.07467832416296005), (10, 0.0803448436781764), (16, 0.08408282697200775), (12, 0.09042049292474985), (5, 0.10667387396097183), (36, 0.40237869694828987), (18, 0.5108213052153587), (53, 1.617948293685913)]
computing accuracy for after removing block 44 . block score: 0.02356328465975821
removed block 44 current accuracy 0.8612 loss from initial  0.13880000000000003
since last training loss: 0.13880000000000003 threshold 999.0 training needed False
start iteration 20
[activation diff]: block to remove picked: 45, with score 0.023247. All blocks and scores: [(45, 0.023246752563863993), (49, 0.02354176272638142), (24, 0.024551384150981903), (21, 0.024924598401412368), (22, 0.0251687690615654), (47, 0.025985259097069502), (38, 0.026409979909658432), (20, 0.026859007542952895), (39, 0.028432969003915787), (15, 0.03192339185625315), (51, 0.032049125991761684), (7, 0.03228544583544135), (19, 0.032628594897687435), (37, 0.035908338613808155), (9, 0.04340188018977642), (6, 0.04660903103649616), (4, 0.0474936836399138), (14, 0.04783663433045149), (52, 0.048162919003516436), (2, 0.05454846518114209), (3, 0.05722427740693092), (13, 0.05892290035262704), (11, 0.059249128215014935), (17, 0.060956848319619894), (0, 0.06300980970263481), (1, 0.06676734425127506), (8, 0.07467832136899233), (10, 0.08034484181553125), (16, 0.08408282790333033), (12, 0.090420494787395), (5, 0.10667387209832668), (36, 0.4023786894977093), (18, 0.5108213275671005), (53, 1.7482214719057083)]
computing accuracy for after removing block 45 . block score: 0.023246752563863993
removed block 45 current accuracy 0.8162 loss from initial  0.18379999999999996
since last training loss: 0.18379999999999996 threshold 999.0 training needed False
start iteration 21
[activation diff]: block to remove picked: 49, with score 0.024157. All blocks and scores: [(49, 0.02415705192834139), (24, 0.024551384849473834), (21, 0.02492459793575108), (22, 0.025168768130242825), (38, 0.026409979909658432), (20, 0.02685900777578354), (47, 0.027429411187767982), (39, 0.028432969469577074), (51, 0.03189300233498216), (15, 0.03192339092493057), (7, 0.032285446766763926), (19, 0.03262859536334872), (37, 0.03590833814814687), (9, 0.04340188018977642), (6, 0.04660903196781874), (4, 0.04749368317425251), (14, 0.0478366338647902), (52, 0.04907965287566185), (2, 0.05454846424981952), (3, 0.05722427926957607), (13, 0.0589229017496109), (11, 0.05924912868067622), (17, 0.06095684878528118), (0, 0.06300980923697352), (1, 0.06676734145730734), (8, 0.07467832695692778), (10, 0.08034484088420868), (16, 0.0840828288346529), (12, 0.09042049292474985), (5, 0.10667387209832668), (36, 0.40237870067358017), (18, 0.5108213052153587), (53, 1.895566999912262)]
computing accuracy for after removing block 49 . block score: 0.02415705192834139
removed block 49 current accuracy 0.7464 loss from initial  0.25360000000000005
since last training loss: 0.25360000000000005 threshold 999.0 training needed False
start iteration 22
[activation diff]: block to remove picked: 24, with score 0.024551. All blocks and scores: [(24, 0.024551384849473834), (21, 0.024924598401412368), (22, 0.025168768595904112), (38, 0.026409979443997145), (20, 0.026859008008614182), (47, 0.02742941095493734), (39, 0.028432969702407718), (15, 0.03192339185625315), (7, 0.03228544630110264), (19, 0.0326285962946713), (51, 0.03331554401665926), (37, 0.03590833814814687), (9, 0.04340188018977642), (6, 0.04660903289914131), (4, 0.04749368457123637), (14, 0.04783663526177406), (2, 0.05454846564680338), (52, 0.055587494280189276), (3, 0.05722427740693092), (13, 0.058922899421304464), (11, 0.05924912914633751), (17, 0.06095684785395861), (0, 0.06300980923697352), (1, 0.06676734145730734), (8, 0.0746783223003149), (10, 0.0803448436781764), (16, 0.08408282976597548), (12, 0.09042049292474985), (5, 0.10667386930435896), (36, 0.40237872675061226), (18, 0.5108213052153587), (53, 2.039414942264557)]
computing accuracy for after removing block 24 . block score: 0.024551384849473834
removed block 24 current accuracy 0.707 loss from initial  0.29300000000000004
since last training loss: 0.29300000000000004 threshold 999.0 training needed False
start iteration 23
[activation diff]: block to remove picked: 21, with score 0.024925. All blocks and scores: [(21, 0.024924597470089793), (22, 0.025168768595904112), (38, 0.025905107147991657), (47, 0.026392598170787096), (20, 0.026859007542952895), (39, 0.027921549510210752), (15, 0.03192339092493057), (7, 0.03228544723242521), (51, 0.032609300687909126), (19, 0.032628596760332584), (37, 0.035631618928164244), (9, 0.04340187972411513), (6, 0.04660903196781874), (4, 0.04749368550255895), (14, 0.047836633399128914), (52, 0.0538765680976212), (2, 0.05454846750944853), (3, 0.05722427787259221), (13, 0.05892290221527219), (11, 0.05924912914633751), (17, 0.06095684925094247), (0, 0.06300980923697352), (1, 0.06676734238862991), (8, 0.07467832136899233), (10, 0.0803448436781764), (16, 0.08408282604068518), (12, 0.09042049199342728), (5, 0.10667387116700411), (36, 0.39359472692012787), (18, 0.5108212828636169), (53, 2.040825754404068)]
computing accuracy for after removing block 21 . block score: 0.024924597470089793
removed block 21 current accuracy 0.675 loss from initial  0.32499999999999996
since last training loss: 0.32499999999999996 threshold 999.0 training needed False
start iteration 24
[activation diff]: block to remove picked: 22, with score 0.023414. All blocks and scores: [(22, 0.02341386047191918), (38, 0.025072284042835236), (47, 0.025673908181488514), (20, 0.026859006844460964), (39, 0.02757857320830226), (15, 0.031923392321914434), (51, 0.0322328070178628), (7, 0.0322854476980865), (19, 0.03262859582901001), (37, 0.035206692293286324), (9, 0.04340188018977642), (6, 0.04660903103649616), (4, 0.04749368457123637), (14, 0.04783663433045149), (52, 0.052523485384881496), (2, 0.054548464715480804), (3, 0.057224278803914785), (13, 0.0589229017496109), (11, 0.059249130077660084), (17, 0.06095684878528118), (0, 0.06300980877131224), (1, 0.06676734425127506), (8, 0.07467832136899233), (10, 0.08034484088420868), (16, 0.08408282790333033), (12, 0.09042049292474985), (5, 0.1066738748922944), (36, 0.3809508979320526), (18, 0.5108212977647781), (53, 2.046926349401474)]
computing accuracy for after removing block 22 . block score: 0.02341386047191918
removed block 22 current accuracy 0.6206 loss from initial  0.37939999999999996
since last training loss: 0.37939999999999996 threshold 999.0 training needed False
start iteration 25
[activation diff]: block to remove picked: 47, with score 0.024693. All blocks and scores: [(47, 0.024693051353096962), (38, 0.02493192022666335), (20, 0.026859007077291608), (39, 0.026928254403173923), (15, 0.03192339185625315), (51, 0.032181731425225735), (7, 0.03228544630110264), (19, 0.03262859536334872), (37, 0.03585043549537659), (9, 0.043401879258453846), (6, 0.04660903150215745), (4, 0.04749368503689766), (14, 0.047836633399128914), (52, 0.05143280699849129), (2, 0.05454846704378724), (3, 0.057224278803914785), (13, 0.058922904543578625), (11, 0.059249126352369785), (17, 0.060956848319619894), (0, 0.06300980737432837), (1, 0.06676734331995249), (8, 0.07467832323163748), (10, 0.08034484274685383), (16, 0.08408282976597548), (12, 0.090420494787395), (5, 0.10667386837303638), (36, 0.37775447964668274), (18, 0.5108213052153587), (53, 2.0304605066776276)]
computing accuracy for after removing block 47 . block score: 0.024693051353096962
removed block 47 current accuracy 0.5112 loss from initial  0.4888
since last training loss: 0.4888 threshold 999.0 training needed False
start iteration 26
[activation diff]: block to remove picked: 38, with score 0.024932. All blocks and scores: [(38, 0.024931919761002064), (20, 0.026859007077291608), (39, 0.026928254403173923), (15, 0.03192339139059186), (7, 0.0322854476980865), (19, 0.03262859536334872), (51, 0.03278361959382892), (37, 0.03585043642669916), (9, 0.04340187879279256), (6, 0.046609028708189726), (4, 0.04749368457123637), (14, 0.04783663572743535), (2, 0.05454846518114209), (3, 0.057224276941269636), (52, 0.05784878693521023), (13, 0.0589229017496109), (11, 0.05924912914633751), (17, 0.060956848319619894), (0, 0.06300980877131224), (1, 0.06676734238862991), (8, 0.0746783223003149), (10, 0.08034484274685383), (16, 0.08408282790333033), (12, 0.090420494787395), (5, 0.10667387209832668), (36, 0.37775447592139244), (18, 0.5108213052153587), (53, 2.1923640072345734)]
computing accuracy for after removing block 38 . block score: 0.024931919761002064
removed block 38 current accuracy 0.4864 loss from initial  0.5136000000000001
since last training loss: 0.5136000000000001 threshold 999.0 training needed False
start iteration 27
[activation diff]: block to remove picked: 20, with score 0.026859. All blocks and scores: [(20, 0.026859007077291608), (39, 0.0276474105194211), (15, 0.031923389993608), (7, 0.032285446766763926), (51, 0.03231552802026272), (19, 0.03262859582901001), (37, 0.0358504350297153), (9, 0.043401879258453846), (6, 0.04660903150215745), (4, 0.0474936836399138), (14, 0.04783663246780634), (2, 0.054548466112464666), (52, 0.05652459245175123), (3, 0.0572242783382535), (13, 0.058922902680933475), (11, 0.059249129611998796), (17, 0.06095685018226504), (0, 0.06300980923697352), (1, 0.06676734145730734), (8, 0.07467832509428263), (10, 0.08034484181553125), (16, 0.0840828288346529), (12, 0.09042049292474985), (5, 0.10667387209832668), (36, 0.37775448337197304), (18, 0.5108212977647781), (53, 2.2692441642284393)]
computing accuracy for after removing block 20 . block score: 0.026859007077291608
removed block 20 current accuracy 0.4572 loss from initial  0.5428
since last training loss: 0.5428 threshold 999.0 training needed False
start iteration 28
[activation diff]: block to remove picked: 39, with score 0.027516. All blocks and scores: [(39, 0.027515978552401066), (15, 0.03192339139059186), (51, 0.03214219957590103), (7, 0.032285446766763926), (19, 0.032628596760332584), (37, 0.037849188316613436), (9, 0.043401879258453846), (6, 0.04660903103649616), (4, 0.0474936836399138), (14, 0.0478366338647902), (2, 0.05454846518114209), (52, 0.05535036697983742), (3, 0.05722427647560835), (13, 0.05892290314659476), (11, 0.059249126352369785), (17, 0.06095684925094247), (0, 0.0630098101682961), (1, 0.06676734238862991), (8, 0.07467832416296005), (10, 0.08034484181553125), (16, 0.08408282697200775), (12, 0.09042049199342728), (5, 0.1066738748922944), (36, 0.3801221251487732), (18, 0.5108213052153587), (53, 2.2503695487976074)]
computing accuracy for after removing block 39 . block score: 0.027515978552401066
removed block 39 current accuracy 0.4052 loss from initial  0.5948
since last training loss: 0.5948 threshold 999.0 training needed False
start iteration 29
[activation diff]: block to remove picked: 51, with score 0.031507. All blocks and scores: [(51, 0.03150742733851075), (15, 0.03192339185625315), (7, 0.03228544630110264), (19, 0.03262859582901001), (37, 0.03784918738529086), (9, 0.043401881121098995), (6, 0.046609032433480024), (4, 0.047493684105575085), (14, 0.0478366338647902), (2, 0.05454846378415823), (52, 0.055606791749596596), (3, 0.05722427787259221), (13, 0.058922901283949614), (11, 0.05924912728369236), (17, 0.06095684785395861), (0, 0.0630098101682961), (1, 0.06676734145730734), (8, 0.0746783223003149), (10, 0.08034484460949898), (16, 0.08408282976597548), (12, 0.0904204910621047), (5, 0.10667387116700411), (36, 0.3801221251487732), (18, 0.5108212977647781), (53, 2.31274476647377)]
computing accuracy for after removing block 51 . block score: 0.03150742733851075
removed block 51 current accuracy 0.3552 loss from initial  0.6448
since last training loss: 0.6448 threshold 999.0 training needed False
start iteration 30
[activation diff]: block to remove picked: 15, with score 0.031923. All blocks and scores: [(15, 0.03192339139059186), (7, 0.03228544630110264), (19, 0.032628596760332584), (37, 0.03784918785095215), (9, 0.043401881121098995), (6, 0.04660903196781874), (4, 0.04749368503689766), (14, 0.047836634796112776), (2, 0.054548466112464666), (3, 0.05722427647560835), (13, 0.05892289895564318), (11, 0.0592491258867085), (17, 0.06095684692263603), (0, 0.06300980970263481), (52, 0.06306930910795927), (1, 0.06676734145730734), (8, 0.07467832323163748), (10, 0.08034484554082155), (16, 0.0840828288346529), (12, 0.09042049292474985), (5, 0.10667387302964926), (36, 0.3801221251487732), (18, 0.5108213126659393), (53, 2.1133328080177307)]
computing accuracy for after removing block 15 . block score: 0.03192339139059186
removed block 15 current accuracy 0.341 loss from initial  0.659
since last training loss: 0.659 threshold 999.0 training needed False
start iteration 31
[activation diff]: block to remove picked: 7, with score 0.032285. All blocks and scores: [(7, 0.03228544723242521), (19, 0.03289006371051073), (37, 0.03839392540976405), (9, 0.04340188018977642), (6, 0.04660903196781874), (4, 0.047493684105575085), (14, 0.04783663433045149), (2, 0.054548466578125954), (3, 0.05722427973523736), (13, 0.05892290221527219), (11, 0.05924912868067622), (52, 0.06275468459352851), (0, 0.06300980923697352), (17, 0.06476554181426764), (1, 0.06676734238862991), (8, 0.07467832043766975), (10, 0.08034484181553125), (12, 0.09042049385607243), (16, 0.09384128171950579), (5, 0.10667387023568153), (36, 0.37938394770026207), (18, 0.49817829579114914), (53, 2.1093852818012238)]
computing accuracy for after removing block 7 . block score: 0.03228544723242521
removed block 7 current accuracy 0.3074 loss from initial  0.6926
since last training loss: 0.6926 threshold 999.0 training needed False
start iteration 32
[activation diff]: block to remove picked: 19, with score 0.032648. All blocks and scores: [(19, 0.03264802973717451), (37, 0.03785328892990947), (9, 0.04323977045714855), (14, 0.044174234848469496), (6, 0.04660903103649616), (4, 0.04749368457123637), (13, 0.05111728189513087), (2, 0.05454846704378724), (17, 0.05493048718199134), (11, 0.05594676826149225), (3, 0.057224276941269636), (0, 0.06300980923697352), (52, 0.06325160665437579), (1, 0.06676734425127506), (8, 0.07245777267962694), (10, 0.08258579857647419), (12, 0.0844392403960228), (16, 0.08537900913506746), (5, 0.10667387302964926), (36, 0.3692890703678131), (18, 0.48171525821089745), (53, 2.1574577391147614)]
computing accuracy for after removing block 19 . block score: 0.03264802973717451
removed block 19 current accuracy 0.2838 loss from initial  0.7162
since last training loss: 0.7162 threshold 999.0 training needed False
start iteration 33
[activation diff]: block to remove picked: 37, with score 0.039909. All blocks and scores: [(37, 0.039909450337290764), (9, 0.04323976906016469), (14, 0.04417423298582435), (6, 0.04660903150215745), (4, 0.04749368457123637), (13, 0.0511172809638083), (2, 0.05454846890643239), (17, 0.054930489510297775), (11, 0.05594676826149225), (3, 0.057224276941269636), (52, 0.062408199068158865), (0, 0.06300980737432837), (1, 0.06676734052598476), (8, 0.07245777081698179), (10, 0.08258579857647419), (12, 0.08443923853337765), (16, 0.08537900820374489), (5, 0.10667387116700411), (36, 0.3678889647126198), (18, 0.48171526193618774), (53, 2.1203058063983917)]
computing accuracy for after removing block 37 . block score: 0.039909450337290764
removed block 37 current accuracy 0.2796 loss from initial  0.7203999999999999
since last training loss: 0.7203999999999999 threshold 999.0 training needed False
start iteration 34
[activation diff]: block to remove picked: 9, with score 0.043240. All blocks and scores: [(9, 0.04323977045714855), (14, 0.044174234848469496), (6, 0.04660903150215745), (4, 0.04749368317425251), (13, 0.05111728236079216), (2, 0.05454846564680338), (17, 0.054930487647652626), (11, 0.05594676733016968), (3, 0.057224276941269636), (52, 0.0614532632753253), (0, 0.06300980877131224), (1, 0.06676734238862991), (8, 0.07245777267962694), (10, 0.08258579950779676), (12, 0.08443923946470022), (16, 0.08537900820374489), (5, 0.10667387209832668), (36, 0.3678889572620392), (18, 0.48171526193618774), (53, 2.1730852127075195)]
computing accuracy for after removing block 9 . block score: 0.04323977045714855
removed block 9 current accuracy 0.2684 loss from initial  0.7316
since last training loss: 0.7316 threshold 999.0 training needed False
start iteration 35
[activation diff]: block to remove picked: 14, with score 0.040098. All blocks and scores: [(14, 0.04009782476350665), (6, 0.046609032433480024), (4, 0.04749368317425251), (13, 0.04848771635442972), (17, 0.049203913658857346), (11, 0.05179083300754428), (2, 0.05454846378415823), (3, 0.05722427740693092), (52, 0.0610812921077013), (0, 0.06300980877131224), (1, 0.06676734331995249), (16, 0.070338343270123), (8, 0.07245777174830437), (12, 0.07276354916393757), (10, 0.08102830406278372), (5, 0.10667387396097183), (36, 0.346349511295557), (18, 0.46365582197904587), (53, 2.2538903057575226)]
computing accuracy for after removing block 14 . block score: 0.04009782476350665
removed block 14 current accuracy 0.224 loss from initial  0.776
since last training loss: 0.776 threshold 999.0 training needed False
start iteration 36
[activation diff]: block to remove picked: 6, with score 0.046609. All blocks and scores: [(6, 0.04660903196781874), (4, 0.04749368503689766), (13, 0.04848771682009101), (17, 0.04876314476132393), (11, 0.05179083254188299), (2, 0.05454846564680338), (3, 0.057224278803914785), (52, 0.0604478670284152), (0, 0.06300980970263481), (1, 0.06676734238862991), (8, 0.07245777454227209), (12, 0.0727635445073247), (10, 0.08102830313146114), (16, 0.09262428432703018), (5, 0.10667387116700411), (36, 0.35410507023334503), (18, 0.46153905987739563), (53, 2.321132481098175)]
computing accuracy for after removing block 6 . block score: 0.04660903196781874
removed block 6 current accuracy 0.177 loss from initial  0.823
since last training loss: 0.823 threshold 999.0 training needed False
start iteration 37
[activation diff]: block to remove picked: 17, with score 0.045327. All blocks and scores: [(17, 0.04532731277868152), (13, 0.045690507628023624), (11, 0.047297772485762835), (4, 0.04749368457123637), (2, 0.054548466112464666), (3, 0.05722427926957607), (52, 0.06043575145304203), (0, 0.06300980830565095), (1, 0.06676734238862991), (12, 0.0676999157294631), (8, 0.0714182211086154), (16, 0.07619658205658197), (10, 0.08469914831221104), (5, 0.10667387023568153), (36, 0.3681607320904732), (18, 0.4568287879228592), (53, 2.395452320575714)]
computing accuracy for after removing block 17 . block score: 0.04532731277868152
removed block 17 current accuracy 0.187 loss from initial  0.813
since last training loss: 0.813 threshold 999.0 training needed False
start iteration 38
[activation diff]: block to remove picked: 13, with score 0.045691. All blocks and scores: [(13, 0.04569050716236234), (11, 0.047297773882746696), (4, 0.04749368457123637), (2, 0.05454846424981952), (3, 0.05722427740693092), (52, 0.05724678793922067), (0, 0.0630098101682961), (1, 0.06676734238862991), (12, 0.06769991759210825), (8, 0.07141822203993797), (16, 0.07619657926261425), (10, 0.08469915110617876), (5, 0.10667386930435896), (36, 0.34295323118567467), (18, 0.43099167197942734), (53, 2.4996899366378784)]
computing accuracy for after removing block 13 . block score: 0.04569050716236234
removed block 13 current accuracy 0.1444 loss from initial  0.8556
since last training loss: 0.8556 threshold 999.0 training needed False
start iteration 39
[activation diff]: block to remove picked: 11, with score 0.047298. All blocks and scores: [(11, 0.047297772485762835), (4, 0.04749368550255895), (2, 0.05454846378415823), (3, 0.057224278803914785), (52, 0.05743759172037244), (0, 0.06300981063395739), (1, 0.06676734052598476), (12, 0.06769991666078568), (8, 0.0714182211086154), (10, 0.08469914924353361), (16, 0.09214204922318459), (5, 0.10667386930435896), (36, 0.36266063526272774), (18, 0.4486052431166172), (53, 2.5943653285503387)]
computing accuracy for after removing block 11 . block score: 0.047297772485762835
removed block 11 current accuracy 0.1392 loss from initial  0.8608
since last training loss: 0.8608 threshold 999.0 training needed False
start iteration 40
[activation diff]: block to remove picked: 4, with score 0.047494. All blocks and scores: [(4, 0.04749368550255895), (2, 0.05454846564680338), (52, 0.0563006903976202), (3, 0.05722427740693092), (12, 0.05762896267697215), (0, 0.06300980970263481), (1, 0.06676734238862991), (16, 0.06730470340698957), (8, 0.07141821924597025), (10, 0.08469914924353361), (5, 0.10667387023568153), (36, 0.3720950223505497), (18, 0.4634442441165447), (53, 2.672854781150818)]
computing accuracy for after removing block 4 . block score: 0.04749368550255895
removed block 4 current accuracy 0.1184 loss from initial  0.8815999999999999
since last training loss: 0.8815999999999999 threshold 999.0 training needed False
start iteration 41
[activation diff]: block to remove picked: 2, with score 0.054548. All blocks and scores: [(2, 0.05454846704378724), (3, 0.057224278803914785), (12, 0.05863540107384324), (52, 0.05953893391415477), (16, 0.06066153757274151), (0, 0.06300980923697352), (1, 0.06676734238862991), (8, 0.07736566103994846), (10, 0.08648024313151836), (5, 0.11125743389129639), (36, 0.39885441586375237), (18, 0.4839479438960552), (53, 2.636992245912552)]
computing accuracy for after removing block 2 . block score: 0.05454846704378724
removed block 2 current accuracy 0.1222 loss from initial  0.8778
since last training loss: 0.8778 threshold 999.0 training needed False
start iteration 42
[activation diff]: block to remove picked: 3, with score 0.052245. All blocks and scores: [(3, 0.05224549910053611), (16, 0.05382554093375802), (12, 0.054412954952567816), (52, 0.05703970789909363), (0, 0.06300980970263481), (1, 0.06676734238862991), (8, 0.07832109276205301), (10, 0.08439413923770189), (5, 0.10767463594675064), (36, 0.3875095024704933), (18, 0.45733632892370224), (53, 2.6718545854091644)]
computing accuracy for after removing block 3 . block score: 0.05224549910053611
removed block 3 current accuracy 0.1056 loss from initial  0.8944
since last training loss: 0.8944 threshold 999.0 training needed False
start iteration 43
[activation diff]: block to remove picked: 16, with score 0.041897. All blocks and scores: [(16, 0.041896908078342676), (12, 0.055417926516383886), (0, 0.06300980783998966), (52, 0.06326292781159282), (1, 0.06676734238862991), (8, 0.07506978325545788), (10, 0.0883299745619297), (5, 0.11126189585775137), (36, 0.42770250514149666), (18, 0.4566463269293308), (53, 2.5204919278621674)]
computing accuracy for after removing block 16 . block score: 0.041896908078342676
removed block 16 current accuracy 0.1038 loss from initial  0.8962
since last training loss: 0.8962 threshold 999.0 training needed False
start iteration 44
[activation diff]: block to remove picked: 12, with score 0.055418. All blocks and scores: [(12, 0.05541792558506131), (0, 0.06300980970263481), (52, 0.0665358854457736), (1, 0.06676734425127506), (8, 0.07506978325545788), (10, 0.0883299745619297), (5, 0.1112618912011385), (36, 0.431771669536829), (18, 0.4783603213727474), (53, 2.505485236644745)]
computing accuracy for after removing block 12 . block score: 0.05541792558506131
removed block 12 current accuracy 0.0924 loss from initial  0.9076
training start
training epoch 0 val accuracy 0.6292 topk_dict {'top1': 0.6292} is_best True lr [0.1]
training epoch 1 val accuracy 0.7362 topk_dict {'top1': 0.7362} is_best True lr [0.1]
training epoch 2 val accuracy 0.7028 topk_dict {'top1': 0.7028} is_best False lr [0.1]
training epoch 3 val accuracy 0.7666 topk_dict {'top1': 0.7666} is_best True lr [0.1]
training epoch 4 val accuracy 0.7566 topk_dict {'top1': 0.7566} is_best False lr [0.1]
training epoch 5 val accuracy 0.8156 topk_dict {'top1': 0.8156} is_best True lr [0.1]
training epoch 6 val accuracy 0.8086 topk_dict {'top1': 0.8086} is_best False lr [0.1]
training epoch 7 val accuracy 0.8272 topk_dict {'top1': 0.8272} is_best True lr [0.1]
training epoch 8 val accuracy 0.8422 topk_dict {'top1': 0.8422} is_best True lr [0.1]
training epoch 9 val accuracy 0.7982 topk_dict {'top1': 0.7982} is_best False lr [0.1]
training epoch 10 val accuracy 0.8274 topk_dict {'top1': 0.8274} is_best False lr [0.1]
training epoch 11 val accuracy 0.8322 topk_dict {'top1': 0.8322} is_best False lr [0.1]
training epoch 12 val accuracy 0.8328 topk_dict {'top1': 0.8328} is_best False lr [0.1]
training epoch 13 val accuracy 0.8512 topk_dict {'top1': 0.8512} is_best True lr [0.1]
training epoch 14 val accuracy 0.827 topk_dict {'top1': 0.827} is_best False lr [0.1]
training epoch 15 val accuracy 0.8288 topk_dict {'top1': 0.8288} is_best False lr [0.1]
training epoch 16 val accuracy 0.8576 topk_dict {'top1': 0.8576} is_best True lr [0.1]
training epoch 17 val accuracy 0.8588 topk_dict {'top1': 0.8588} is_best True lr [0.1]
training epoch 18 val accuracy 0.8468 topk_dict {'top1': 0.8468} is_best False lr [0.1]
training epoch 19 val accuracy 0.8622 topk_dict {'top1': 0.8622} is_best True lr [0.1]
training epoch 20 val accuracy 0.8608 topk_dict {'top1': 0.8608} is_best False lr [0.1]
training epoch 21 val accuracy 0.8554 topk_dict {'top1': 0.8554} is_best False lr [0.1]
training epoch 22 val accuracy 0.8406 topk_dict {'top1': 0.8406} is_best False lr [0.1]
training epoch 23 val accuracy 0.8306 topk_dict {'top1': 0.8306} is_best False lr [0.1]
training epoch 24 val accuracy 0.8572 topk_dict {'top1': 0.8572} is_best False lr [0.1]
training epoch 25 val accuracy 0.8488 topk_dict {'top1': 0.8488} is_best False lr [0.1]
training epoch 26 val accuracy 0.8704 topk_dict {'top1': 0.8704} is_best True lr [0.1]
training epoch 27 val accuracy 0.8168 topk_dict {'top1': 0.8168} is_best False lr [0.1]
training epoch 28 val accuracy 0.8602 topk_dict {'top1': 0.8602} is_best False lr [0.1]
training epoch 29 val accuracy 0.866 topk_dict {'top1': 0.866} is_best False lr [0.1]
training epoch 30 val accuracy 0.8552 topk_dict {'top1': 0.8552} is_best False lr [0.1]
training epoch 31 val accuracy 0.8538 topk_dict {'top1': 0.8538} is_best False lr [0.1]
training epoch 32 val accuracy 0.8242 topk_dict {'top1': 0.8242} is_best False lr [0.1]
training epoch 33 val accuracy 0.8656 topk_dict {'top1': 0.8656} is_best False lr [0.1]
training epoch 34 val accuracy 0.8742 topk_dict {'top1': 0.8742} is_best True lr [0.1]
training epoch 35 val accuracy 0.8678 topk_dict {'top1': 0.8678} is_best False lr [0.1]
training epoch 36 val accuracy 0.8596 topk_dict {'top1': 0.8596} is_best False lr [0.1]
training epoch 37 val accuracy 0.8476 topk_dict {'top1': 0.8476} is_best False lr [0.1]
training epoch 38 val accuracy 0.8602 topk_dict {'top1': 0.8602} is_best False lr [0.1]
training epoch 39 val accuracy 0.8706 topk_dict {'top1': 0.8706} is_best False lr [0.1]
training epoch 40 val accuracy 0.8554 topk_dict {'top1': 0.8554} is_best False lr [0.1]
training epoch 41 val accuracy 0.8568 topk_dict {'top1': 0.8568} is_best False lr [0.1]
training epoch 42 val accuracy 0.85 topk_dict {'top1': 0.85} is_best False lr [0.1]
training epoch 43 val accuracy 0.8568 topk_dict {'top1': 0.8568} is_best False lr [0.1]
training epoch 44 val accuracy 0.8726 topk_dict {'top1': 0.8726} is_best False lr [0.1]
training epoch 45 val accuracy 0.8098 topk_dict {'top1': 0.8098} is_best False lr [0.1]
training epoch 46 val accuracy 0.8638 topk_dict {'top1': 0.8638} is_best False lr [0.1]
training epoch 47 val accuracy 0.8396 topk_dict {'top1': 0.8396} is_best False lr [0.1]
training epoch 48 val accuracy 0.8646 topk_dict {'top1': 0.8646} is_best False lr [0.1]
training epoch 49 val accuracy 0.8606 topk_dict {'top1': 0.8606} is_best False lr [0.1]
training epoch 50 val accuracy 0.8582 topk_dict {'top1': 0.8582} is_best False lr [0.1]
training epoch 51 val accuracy 0.84 topk_dict {'top1': 0.84} is_best False lr [0.1]
training epoch 52 val accuracy 0.8594 topk_dict {'top1': 0.8594} is_best False lr [0.1]
training epoch 53 val accuracy 0.8722 topk_dict {'top1': 0.8722} is_best False lr [0.1]
training epoch 54 val accuracy 0.8768 topk_dict {'top1': 0.8768} is_best True lr [0.1]
training epoch 55 val accuracy 0.8682 topk_dict {'top1': 0.8682} is_best False lr [0.1]
training epoch 56 val accuracy 0.8696 topk_dict {'top1': 0.8696} is_best False lr [0.1]
training epoch 57 val accuracy 0.8494 topk_dict {'top1': 0.8494} is_best False lr [0.1]
training epoch 58 val accuracy 0.8766 topk_dict {'top1': 0.8766} is_best False lr [0.1]
training epoch 59 val accuracy 0.861 topk_dict {'top1': 0.861} is_best False lr [0.1]
training epoch 60 val accuracy 0.8652 topk_dict {'top1': 0.8652} is_best False lr [0.1]
training epoch 61 val accuracy 0.8728 topk_dict {'top1': 0.8728} is_best False lr [0.1]
training epoch 62 val accuracy 0.8686 topk_dict {'top1': 0.8686} is_best False lr [0.1]
training epoch 63 val accuracy 0.8488 topk_dict {'top1': 0.8488} is_best False lr [0.1]
training epoch 64 val accuracy 0.853 topk_dict {'top1': 0.853} is_best False lr [0.1]
training epoch 65 val accuracy 0.8608 topk_dict {'top1': 0.8608} is_best False lr [0.1]
training epoch 66 val accuracy 0.8684 topk_dict {'top1': 0.8684} is_best False lr [0.1]
training epoch 67 val accuracy 0.8668 topk_dict {'top1': 0.8668} is_best False lr [0.1]
training epoch 68 val accuracy 0.8622 topk_dict {'top1': 0.8622} is_best False lr [0.1]
training epoch 69 val accuracy 0.8694 topk_dict {'top1': 0.8694} is_best False lr [0.1]
training epoch 70 val accuracy 0.8446 topk_dict {'top1': 0.8446} is_best False lr [0.1]
training epoch 71 val accuracy 0.8784 topk_dict {'top1': 0.8784} is_best True lr [0.1]
training epoch 72 val accuracy 0.8782 topk_dict {'top1': 0.8782} is_best False lr [0.1]
training epoch 73 val accuracy 0.8434 topk_dict {'top1': 0.8434} is_best False lr [0.1]
training epoch 74 val accuracy 0.8556 topk_dict {'top1': 0.8556} is_best False lr [0.1]
training epoch 75 val accuracy 0.8582 topk_dict {'top1': 0.8582} is_best False lr [0.1]
training epoch 76 val accuracy 0.8676 topk_dict {'top1': 0.8676} is_best False lr [0.1]
training epoch 77 val accuracy 0.8536 topk_dict {'top1': 0.8536} is_best False lr [0.1]
training epoch 78 val accuracy 0.847 topk_dict {'top1': 0.847} is_best False lr [0.1]
training epoch 79 val accuracy 0.8678 topk_dict {'top1': 0.8678} is_best False lr [0.1]
training epoch 80 val accuracy 0.8652 topk_dict {'top1': 0.8652} is_best False lr [0.1]
training epoch 81 val accuracy 0.8696 topk_dict {'top1': 0.8696} is_best False lr [0.1]
training epoch 82 val accuracy 0.8448 topk_dict {'top1': 0.8448} is_best False lr [0.1]
training epoch 83 val accuracy 0.8788 topk_dict {'top1': 0.8788} is_best True lr [0.1]
training epoch 84 val accuracy 0.8652 topk_dict {'top1': 0.8652} is_best False lr [0.1]
training epoch 85 val accuracy 0.8456 topk_dict {'top1': 0.8456} is_best False lr [0.1]
training epoch 86 val accuracy 0.87 topk_dict {'top1': 0.87} is_best False lr [0.1]
training epoch 87 val accuracy 0.858 topk_dict {'top1': 0.858} is_best False lr [0.1]
training epoch 88 val accuracy 0.8336 topk_dict {'top1': 0.8336} is_best False lr [0.1]
training epoch 89 val accuracy 0.865 topk_dict {'top1': 0.865} is_best False lr [0.1]
training epoch 90 val accuracy 0.8652 topk_dict {'top1': 0.8652} is_best False lr [0.1]
training epoch 91 val accuracy 0.8726 topk_dict {'top1': 0.8726} is_best False lr [0.1]
training epoch 92 val accuracy 0.8748 topk_dict {'top1': 0.8748} is_best False lr [0.1]
training epoch 93 val accuracy 0.8608 topk_dict {'top1': 0.8608} is_best False lr [0.1]
training epoch 94 val accuracy 0.865 topk_dict {'top1': 0.865} is_best False lr [0.1]
training epoch 95 val accuracy 0.8638 topk_dict {'top1': 0.8638} is_best False lr [0.1]
training epoch 96 val accuracy 0.8624 topk_dict {'top1': 0.8624} is_best False lr [0.1]
training epoch 97 val accuracy 0.8616 topk_dict {'top1': 0.8616} is_best False lr [0.1]
training epoch 98 val accuracy 0.8558 topk_dict {'top1': 0.8558} is_best False lr [0.1]
training epoch 99 val accuracy 0.843 topk_dict {'top1': 0.843} is_best False lr [0.1]
training epoch 100 val accuracy 0.8754 topk_dict {'top1': 0.8754} is_best False lr [0.1]
training epoch 101 val accuracy 0.8746 topk_dict {'top1': 0.8746} is_best False lr [0.1]
training epoch 102 val accuracy 0.8606 topk_dict {'top1': 0.8606} is_best False lr [0.1]
training epoch 103 val accuracy 0.8608 topk_dict {'top1': 0.8608} is_best False lr [0.1]
training epoch 104 val accuracy 0.86 topk_dict {'top1': 0.86} is_best False lr [0.1]
training epoch 105 val accuracy 0.866 topk_dict {'top1': 0.866} is_best False lr [0.1]
training epoch 106 val accuracy 0.867 topk_dict {'top1': 0.867} is_best False lr [0.1]
training epoch 107 val accuracy 0.8558 topk_dict {'top1': 0.8558} is_best False lr [0.1]
training epoch 108 val accuracy 0.8768 topk_dict {'top1': 0.8768} is_best False lr [0.1]
training epoch 109 val accuracy 0.861 topk_dict {'top1': 0.861} is_best False lr [0.1]
training epoch 110 val accuracy 0.8704 topk_dict {'top1': 0.8704} is_best False lr [0.1]
training epoch 111 val accuracy 0.8536 topk_dict {'top1': 0.8536} is_best False lr [0.1]
training epoch 112 val accuracy 0.867 topk_dict {'top1': 0.867} is_best False lr [0.1]
training epoch 113 val accuracy 0.8722 topk_dict {'top1': 0.8722} is_best False lr [0.1]
training epoch 114 val accuracy 0.8756 topk_dict {'top1': 0.8756} is_best False lr [0.1]
training epoch 115 val accuracy 0.8546 topk_dict {'top1': 0.8546} is_best False lr [0.1]
training epoch 116 val accuracy 0.864 topk_dict {'top1': 0.864} is_best False lr [0.1]
training epoch 117 val accuracy 0.8534 topk_dict {'top1': 0.8534} is_best False lr [0.1]
training epoch 118 val accuracy 0.8764 topk_dict {'top1': 0.8764} is_best False lr [0.1]
training epoch 119 val accuracy 0.87 topk_dict {'top1': 0.87} is_best False lr [0.1]
training epoch 120 val accuracy 0.8554 topk_dict {'top1': 0.8554} is_best False lr [0.1]
training epoch 121 val accuracy 0.8678 topk_dict {'top1': 0.8678} is_best False lr [0.1]
training epoch 122 val accuracy 0.8412 topk_dict {'top1': 0.8412} is_best False lr [0.1]
training epoch 123 val accuracy 0.836 topk_dict {'top1': 0.836} is_best False lr [0.1]
training epoch 124 val accuracy 0.8618 topk_dict {'top1': 0.8618} is_best False lr [0.1]
training epoch 125 val accuracy 0.8702 topk_dict {'top1': 0.8702} is_best False lr [0.1]
training epoch 126 val accuracy 0.8772 topk_dict {'top1': 0.8772} is_best False lr [0.1]
training epoch 127 val accuracy 0.8594 topk_dict {'top1': 0.8594} is_best False lr [0.1]
training epoch 128 val accuracy 0.8712 topk_dict {'top1': 0.8712} is_best False lr [0.1]
training epoch 129 val accuracy 0.8692 topk_dict {'top1': 0.8692} is_best False lr [0.1]
training epoch 130 val accuracy 0.8638 topk_dict {'top1': 0.8638} is_best False lr [0.1]
training epoch 131 val accuracy 0.858 topk_dict {'top1': 0.858} is_best False lr [0.1]
training epoch 132 val accuracy 0.8492 topk_dict {'top1': 0.8492} is_best False lr [0.1]
training epoch 133 val accuracy 0.8414 topk_dict {'top1': 0.8414} is_best False lr [0.1]
training epoch 134 val accuracy 0.8586 topk_dict {'top1': 0.8586} is_best False lr [0.1]
training epoch 135 val accuracy 0.8614 topk_dict {'top1': 0.8614} is_best False lr [0.1]
training epoch 136 val accuracy 0.8736 topk_dict {'top1': 0.8736} is_best False lr [0.1]
training epoch 137 val accuracy 0.8668 topk_dict {'top1': 0.8668} is_best False lr [0.1]
training epoch 138 val accuracy 0.8744 topk_dict {'top1': 0.8744} is_best False lr [0.1]
training epoch 139 val accuracy 0.8654 topk_dict {'top1': 0.8654} is_best False lr [0.1]
training epoch 140 val accuracy 0.8628 topk_dict {'top1': 0.8628} is_best False lr [0.1]
training epoch 141 val accuracy 0.8738 topk_dict {'top1': 0.8738} is_best False lr [0.1]
training epoch 142 val accuracy 0.8554 topk_dict {'top1': 0.8554} is_best False lr [0.1]
training epoch 143 val accuracy 0.874 topk_dict {'top1': 0.874} is_best False lr [0.1]
training epoch 144 val accuracy 0.8574 topk_dict {'top1': 0.8574} is_best False lr [0.1]
training epoch 145 val accuracy 0.8356 topk_dict {'top1': 0.8356} is_best False lr [0.1]
training epoch 146 val accuracy 0.867 topk_dict {'top1': 0.867} is_best False lr [0.1]
training epoch 147 val accuracy 0.8288 topk_dict {'top1': 0.8288} is_best False lr [0.1]
training epoch 148 val accuracy 0.854 topk_dict {'top1': 0.854} is_best False lr [0.1]
training epoch 149 val accuracy 0.8704 topk_dict {'top1': 0.8704} is_best False lr [0.1]
training epoch 150 val accuracy 0.8722 topk_dict {'top1': 0.8722} is_best False lr [0.1]
training epoch 151 val accuracy 0.8734 topk_dict {'top1': 0.8734} is_best False lr [0.1]
training epoch 152 val accuracy 0.8646 topk_dict {'top1': 0.8646} is_best False lr [0.1]
training epoch 153 val accuracy 0.8714 topk_dict {'top1': 0.8714} is_best False lr [0.1]
training epoch 154 val accuracy 0.8346 topk_dict {'top1': 0.8346} is_best False lr [0.1]
training epoch 155 val accuracy 0.8494 topk_dict {'top1': 0.8494} is_best False lr [0.1]
training epoch 156 val accuracy 0.847 topk_dict {'top1': 0.847} is_best False lr [0.1]
training epoch 157 val accuracy 0.8572 topk_dict {'top1': 0.8572} is_best False lr [0.1]
training epoch 158 val accuracy 0.8456 topk_dict {'top1': 0.8456} is_best False lr [0.1]
training epoch 159 val accuracy 0.87 topk_dict {'top1': 0.87} is_best False lr [0.1]
training epoch 160 val accuracy 0.8798 topk_dict {'top1': 0.8798} is_best True lr [0.1]
training epoch 161 val accuracy 0.8736 topk_dict {'top1': 0.8736} is_best False lr [0.1]
training epoch 162 val accuracy 0.8652 topk_dict {'top1': 0.8652} is_best False lr [0.1]
training epoch 163 val accuracy 0.8544 topk_dict {'top1': 0.8544} is_best False lr [0.1]
training epoch 164 val accuracy 0.865 topk_dict {'top1': 0.865} is_best False lr [0.1]
training epoch 165 val accuracy 0.869 topk_dict {'top1': 0.869} is_best False lr [0.1]
training epoch 166 val accuracy 0.8506 topk_dict {'top1': 0.8506} is_best False lr [0.1]
training epoch 167 val accuracy 0.8622 topk_dict {'top1': 0.8622} is_best False lr [0.1]
training epoch 168 val accuracy 0.8776 topk_dict {'top1': 0.8776} is_best False lr [0.1]
training epoch 169 val accuracy 0.875 topk_dict {'top1': 0.875} is_best False lr [0.1]
training epoch 170 val accuracy 0.8676 topk_dict {'top1': 0.8676} is_best False lr [0.1]
training epoch 171 val accuracy 0.8672 topk_dict {'top1': 0.8672} is_best False lr [0.1]
training epoch 172 val accuracy 0.8714 topk_dict {'top1': 0.8714} is_best False lr [0.1]
training epoch 173 val accuracy 0.8692 topk_dict {'top1': 0.8692} is_best False lr [0.1]
training epoch 174 val accuracy 0.865 topk_dict {'top1': 0.865} is_best False lr [0.1]
training epoch 175 val accuracy 0.8716 topk_dict {'top1': 0.8716} is_best False lr [0.1]
training epoch 176 val accuracy 0.8814 topk_dict {'top1': 0.8814} is_best True lr [0.1]
training epoch 177 val accuracy 0.8374 topk_dict {'top1': 0.8374} is_best False lr [0.1]
training epoch 178 val accuracy 0.8764 topk_dict {'top1': 0.8764} is_best False lr [0.1]
training epoch 179 val accuracy 0.8688 topk_dict {'top1': 0.8688} is_best False lr [0.1]
training epoch 180 val accuracy 0.8586 topk_dict {'top1': 0.8586} is_best False lr [0.1]
training epoch 181 val accuracy 0.862 topk_dict {'top1': 0.862} is_best False lr [0.1]
training epoch 182 val accuracy 0.8698 topk_dict {'top1': 0.8698} is_best False lr [0.1]
training epoch 183 val accuracy 0.854 topk_dict {'top1': 0.854} is_best False lr [0.1]
training epoch 184 val accuracy 0.8448 topk_dict {'top1': 0.8448} is_best False lr [0.1]
training epoch 185 val accuracy 0.868 topk_dict {'top1': 0.868} is_best False lr [0.1]
training epoch 186 val accuracy 0.8762 topk_dict {'top1': 0.8762} is_best False lr [0.1]
training epoch 187 val accuracy 0.8744 topk_dict {'top1': 0.8744} is_best False lr [0.1]
training epoch 188 val accuracy 0.8724 topk_dict {'top1': 0.8724} is_best False lr [0.1]
training epoch 189 val accuracy 0.8444 topk_dict {'top1': 0.8444} is_best False lr [0.1]
training epoch 190 val accuracy 0.8714 topk_dict {'top1': 0.8714} is_best False lr [0.1]
training epoch 191 val accuracy 0.881 topk_dict {'top1': 0.881} is_best False lr [0.1]
training epoch 192 val accuracy 0.8656 topk_dict {'top1': 0.8656} is_best False lr [0.1]
training epoch 193 val accuracy 0.8556 topk_dict {'top1': 0.8556} is_best False lr [0.1]
training epoch 194 val accuracy 0.8514 topk_dict {'top1': 0.8514} is_best False lr [0.1]
training epoch 195 val accuracy 0.8772 topk_dict {'top1': 0.8772} is_best False lr [0.1]
training epoch 196 val accuracy 0.8474 topk_dict {'top1': 0.8474} is_best False lr [0.1]
training epoch 197 val accuracy 0.8496 topk_dict {'top1': 0.8496} is_best False lr [0.1]
training epoch 198 val accuracy 0.8806 topk_dict {'top1': 0.8806} is_best False lr [0.1]
training epoch 199 val accuracy 0.8658 topk_dict {'top1': 0.8658} is_best False lr [0.1]
training epoch 200 val accuracy 0.8336 topk_dict {'top1': 0.8336} is_best False lr [0.1]
training epoch 201 val accuracy 0.859 topk_dict {'top1': 0.859} is_best False lr [0.1]
training epoch 202 val accuracy 0.8668 topk_dict {'top1': 0.8668} is_best False lr [0.1]
training epoch 203 val accuracy 0.8546 topk_dict {'top1': 0.8546} is_best False lr [0.1]
training epoch 204 val accuracy 0.8634 topk_dict {'top1': 0.8634} is_best False lr [0.1]
training epoch 205 val accuracy 0.8456 topk_dict {'top1': 0.8456} is_best False lr [0.1]
training epoch 206 val accuracy 0.8778 topk_dict {'top1': 0.8778} is_best False lr [0.1]
training epoch 207 val accuracy 0.862 topk_dict {'top1': 0.862} is_best False lr [0.1]
training epoch 208 val accuracy 0.8658 topk_dict {'top1': 0.8658} is_best False lr [0.1]
training epoch 209 val accuracy 0.867 topk_dict {'top1': 0.867} is_best False lr [0.1]
training epoch 210 val accuracy 0.8626 topk_dict {'top1': 0.8626} is_best False lr [0.1]
training epoch 211 val accuracy 0.8762 topk_dict {'top1': 0.8762} is_best False lr [0.1]
training epoch 212 val accuracy 0.8622 topk_dict {'top1': 0.8622} is_best False lr [0.1]
training epoch 213 val accuracy 0.8744 topk_dict {'top1': 0.8744} is_best False lr [0.1]
training epoch 214 val accuracy 0.8742 topk_dict {'top1': 0.8742} is_best False lr [0.1]
training epoch 215 val accuracy 0.8716 topk_dict {'top1': 0.8716} is_best False lr [0.1]
training epoch 216 val accuracy 0.868 topk_dict {'top1': 0.868} is_best False lr [0.1]
training epoch 217 val accuracy 0.875 topk_dict {'top1': 0.875} is_best False lr [0.1]
training epoch 218 val accuracy 0.8618 topk_dict {'top1': 0.8618} is_best False lr [0.1]
training epoch 219 val accuracy 0.853 topk_dict {'top1': 0.853} is_best False lr [0.1]
training epoch 220 val accuracy 0.8848 topk_dict {'top1': 0.8848} is_best True lr [0.1]
training epoch 221 val accuracy 0.862 topk_dict {'top1': 0.862} is_best False lr [0.1]
training epoch 222 val accuracy 0.8802 topk_dict {'top1': 0.8802} is_best False lr [0.1]
training epoch 223 val accuracy 0.8752 topk_dict {'top1': 0.8752} is_best False lr [0.1]
training epoch 224 val accuracy 0.8746 topk_dict {'top1': 0.8746} is_best False lr [0.1]
training epoch 225 val accuracy 0.8606 topk_dict {'top1': 0.8606} is_best False lr [0.1]
training epoch 226 val accuracy 0.8582 topk_dict {'top1': 0.8582} is_best False lr [0.1]
training epoch 227 val accuracy 0.8552 topk_dict {'top1': 0.8552} is_best False lr [0.1]
training epoch 228 val accuracy 0.876 topk_dict {'top1': 0.876} is_best False lr [0.1]
training epoch 229 val accuracy 0.8612 topk_dict {'top1': 0.8612} is_best False lr [0.1]
training epoch 230 val accuracy 0.8662 topk_dict {'top1': 0.8662} is_best False lr [0.1]
training epoch 231 val accuracy 0.8706 topk_dict {'top1': 0.8706} is_best False lr [0.1]
training epoch 232 val accuracy 0.848 topk_dict {'top1': 0.848} is_best False lr [0.1]
training epoch 233 val accuracy 0.8486 topk_dict {'top1': 0.8486} is_best False lr [0.1]
training epoch 234 val accuracy 0.8564 topk_dict {'top1': 0.8564} is_best False lr [0.1]
training epoch 235 val accuracy 0.8814 topk_dict {'top1': 0.8814} is_best False lr [0.1]
training epoch 236 val accuracy 0.8664 topk_dict {'top1': 0.8664} is_best False lr [0.1]
training epoch 237 val accuracy 0.86 topk_dict {'top1': 0.86} is_best False lr [0.1]
training epoch 238 val accuracy 0.8784 topk_dict {'top1': 0.8784} is_best False lr [0.1]
training epoch 239 val accuracy 0.8538 topk_dict {'top1': 0.8538} is_best False lr [0.1]
training epoch 240 val accuracy 0.8662 topk_dict {'top1': 0.8662} is_best False lr [0.1]
training epoch 241 val accuracy 0.853 topk_dict {'top1': 0.853} is_best False lr [0.1]
training epoch 242 val accuracy 0.8756 topk_dict {'top1': 0.8756} is_best False lr [0.1]
training epoch 243 val accuracy 0.871 topk_dict {'top1': 0.871} is_best False lr [0.1]
training epoch 244 val accuracy 0.8644 topk_dict {'top1': 0.8644} is_best False lr [0.1]
training epoch 245 val accuracy 0.8602 topk_dict {'top1': 0.8602} is_best False lr [0.1]
training epoch 246 val accuracy 0.8616 topk_dict {'top1': 0.8616} is_best False lr [0.1]
training epoch 247 val accuracy 0.8668 topk_dict {'top1': 0.8668} is_best False lr [0.1]
training epoch 248 val accuracy 0.8606 topk_dict {'top1': 0.8606} is_best False lr [0.1]
training epoch 249 val accuracy 0.8748 topk_dict {'top1': 0.8748} is_best False lr [0.1]
training epoch 250 val accuracy 0.909 topk_dict {'top1': 0.909} is_best True lr [0.010000000000000002]
training epoch 251 val accuracy 0.9124 topk_dict {'top1': 0.9124} is_best True lr [0.010000000000000002]
training epoch 252 val accuracy 0.9116 topk_dict {'top1': 0.9116} is_best False lr [0.010000000000000002]
training epoch 253 val accuracy 0.9128 topk_dict {'top1': 0.9128} is_best True lr [0.010000000000000002]
training epoch 254 val accuracy 0.9118 topk_dict {'top1': 0.9118} is_best False lr [0.010000000000000002]
training epoch 255 val accuracy 0.9126 topk_dict {'top1': 0.9126} is_best False lr [0.010000000000000002]
training epoch 256 val accuracy 0.9122 topk_dict {'top1': 0.9122} is_best False lr [0.010000000000000002]
training epoch 257 val accuracy 0.9138 topk_dict {'top1': 0.9138} is_best True lr [0.010000000000000002]
training epoch 258 val accuracy 0.913 topk_dict {'top1': 0.913} is_best False lr [0.010000000000000002]
training epoch 259 val accuracy 0.914 topk_dict {'top1': 0.914} is_best True lr [0.010000000000000002]
training epoch 260 val accuracy 0.914 topk_dict {'top1': 0.914} is_best False lr [0.010000000000000002]
training epoch 261 val accuracy 0.913 topk_dict {'top1': 0.913} is_best False lr [0.010000000000000002]
training epoch 262 val accuracy 0.9144 topk_dict {'top1': 0.9144} is_best True lr [0.010000000000000002]
training epoch 263 val accuracy 0.9124 topk_dict {'top1': 0.9124} is_best False lr [0.010000000000000002]
training epoch 264 val accuracy 0.9176 topk_dict {'top1': 0.9176} is_best True lr [0.010000000000000002]
training epoch 265 val accuracy 0.9126 topk_dict {'top1': 0.9126} is_best False lr [0.010000000000000002]
training epoch 266 val accuracy 0.9138 topk_dict {'top1': 0.9138} is_best False lr [0.010000000000000002]
training epoch 267 val accuracy 0.9134 topk_dict {'top1': 0.9134} is_best False lr [0.010000000000000002]
training epoch 268 val accuracy 0.9094 topk_dict {'top1': 0.9094} is_best False lr [0.010000000000000002]
training epoch 269 val accuracy 0.913 topk_dict {'top1': 0.913} is_best False lr [0.010000000000000002]
training epoch 270 val accuracy 0.9116 topk_dict {'top1': 0.9116} is_best False lr [0.010000000000000002]
training epoch 271 val accuracy 0.912 topk_dict {'top1': 0.912} is_best False lr [0.010000000000000002]
training epoch 272 val accuracy 0.9124 topk_dict {'top1': 0.9124} is_best False lr [0.010000000000000002]
training epoch 273 val accuracy 0.9126 topk_dict {'top1': 0.9126} is_best False lr [0.010000000000000002]
training epoch 274 val accuracy 0.9114 topk_dict {'top1': 0.9114} is_best False lr [0.010000000000000002]
training epoch 275 val accuracy 0.9116 topk_dict {'top1': 0.9116} is_best False lr [0.010000000000000002]
training epoch 276 val accuracy 0.9104 topk_dict {'top1': 0.9104} is_best False lr [0.010000000000000002]
training epoch 277 val accuracy 0.9104 topk_dict {'top1': 0.9104} is_best False lr [0.010000000000000002]
training epoch 278 val accuracy 0.9112 topk_dict {'top1': 0.9112} is_best False lr [0.010000000000000002]
training epoch 279 val accuracy 0.9132 topk_dict {'top1': 0.9132} is_best False lr [0.010000000000000002]
training epoch 280 val accuracy 0.9128 topk_dict {'top1': 0.9128} is_best False lr [0.010000000000000002]
training epoch 281 val accuracy 0.9124 topk_dict {'top1': 0.9124} is_best False lr [0.010000000000000002]
training epoch 282 val accuracy 0.9112 topk_dict {'top1': 0.9112} is_best False lr [0.010000000000000002]
training epoch 283 val accuracy 0.9118 topk_dict {'top1': 0.9118} is_best False lr [0.010000000000000002]
training epoch 284 val accuracy 0.9114 topk_dict {'top1': 0.9114} is_best False lr [0.010000000000000002]
training epoch 285 val accuracy 0.9126 topk_dict {'top1': 0.9126} is_best False lr [0.010000000000000002]
training epoch 286 val accuracy 0.9156 topk_dict {'top1': 0.9156} is_best False lr [0.010000000000000002]
training epoch 287 val accuracy 0.9116 topk_dict {'top1': 0.9116} is_best False lr [0.010000000000000002]
training epoch 288 val accuracy 0.9142 topk_dict {'top1': 0.9142} is_best False lr [0.010000000000000002]
training epoch 289 val accuracy 0.915 topk_dict {'top1': 0.915} is_best False lr [0.010000000000000002]
training epoch 290 val accuracy 0.9128 topk_dict {'top1': 0.9128} is_best False lr [0.010000000000000002]
training epoch 291 val accuracy 0.9124 topk_dict {'top1': 0.9124} is_best False lr [0.010000000000000002]
training epoch 292 val accuracy 0.9118 topk_dict {'top1': 0.9118} is_best False lr [0.010000000000000002]
training epoch 293 val accuracy 0.915 topk_dict {'top1': 0.915} is_best False lr [0.010000000000000002]
training epoch 294 val accuracy 0.9118 topk_dict {'top1': 0.9118} is_best False lr [0.010000000000000002]
training epoch 295 val accuracy 0.9156 topk_dict {'top1': 0.9156} is_best False lr [0.010000000000000002]
training epoch 296 val accuracy 0.913 topk_dict {'top1': 0.913} is_best False lr [0.010000000000000002]
training epoch 297 val accuracy 0.9122 topk_dict {'top1': 0.9122} is_best False lr [0.010000000000000002]
training epoch 298 val accuracy 0.9134 topk_dict {'top1': 0.9134} is_best False lr [0.010000000000000002]
training epoch 299 val accuracy 0.9108 topk_dict {'top1': 0.9108} is_best False lr [0.010000000000000002]
training epoch 300 val accuracy 0.9118 topk_dict {'top1': 0.9118} is_best False lr [0.010000000000000002]
training epoch 301 val accuracy 0.911 topk_dict {'top1': 0.911} is_best False lr [0.010000000000000002]
training epoch 302 val accuracy 0.9144 topk_dict {'top1': 0.9144} is_best False lr [0.010000000000000002]
training epoch 303 val accuracy 0.9156 topk_dict {'top1': 0.9156} is_best False lr [0.010000000000000002]
training epoch 304 val accuracy 0.9122 topk_dict {'top1': 0.9122} is_best False lr [0.010000000000000002]
training epoch 305 val accuracy 0.914 topk_dict {'top1': 0.914} is_best False lr [0.010000000000000002]
training epoch 306 val accuracy 0.9132 topk_dict {'top1': 0.9132} is_best False lr [0.010000000000000002]
training epoch 307 val accuracy 0.9128 topk_dict {'top1': 0.9128} is_best False lr [0.010000000000000002]
training epoch 308 val accuracy 0.9096 topk_dict {'top1': 0.9096} is_best False lr [0.010000000000000002]
training epoch 309 val accuracy 0.9098 topk_dict {'top1': 0.9098} is_best False lr [0.010000000000000002]
training epoch 310 val accuracy 0.9098 topk_dict {'top1': 0.9098} is_best False lr [0.010000000000000002]
training epoch 311 val accuracy 0.911 topk_dict {'top1': 0.911} is_best False lr [0.010000000000000002]
training epoch 312 val accuracy 0.9116 topk_dict {'top1': 0.9116} is_best False lr [0.010000000000000002]
training epoch 313 val accuracy 0.9104 topk_dict {'top1': 0.9104} is_best False lr [0.010000000000000002]
training epoch 314 val accuracy 0.9108 topk_dict {'top1': 0.9108} is_best False lr [0.010000000000000002]
training epoch 315 val accuracy 0.911 topk_dict {'top1': 0.911} is_best False lr [0.010000000000000002]
training epoch 316 val accuracy 0.9108 topk_dict {'top1': 0.9108} is_best False lr [0.010000000000000002]
training epoch 317 val accuracy 0.9098 topk_dict {'top1': 0.9098} is_best False lr [0.010000000000000002]
training epoch 318 val accuracy 0.9108 topk_dict {'top1': 0.9108} is_best False lr [0.010000000000000002]
training epoch 319 val accuracy 0.9108 topk_dict {'top1': 0.9108} is_best False lr [0.010000000000000002]
training epoch 320 val accuracy 0.9108 topk_dict {'top1': 0.9108} is_best False lr [0.010000000000000002]
training epoch 321 val accuracy 0.9112 topk_dict {'top1': 0.9112} is_best False lr [0.010000000000000002]
training epoch 322 val accuracy 0.9112 topk_dict {'top1': 0.9112} is_best False lr [0.010000000000000002]
training epoch 323 val accuracy 0.9098 topk_dict {'top1': 0.9098} is_best False lr [0.010000000000000002]
training epoch 324 val accuracy 0.912 topk_dict {'top1': 0.912} is_best False lr [0.010000000000000002]
training epoch 325 val accuracy 0.9114 topk_dict {'top1': 0.9114} is_best False lr [0.010000000000000002]
training epoch 326 val accuracy 0.9106 topk_dict {'top1': 0.9106} is_best False lr [0.010000000000000002]
training epoch 327 val accuracy 0.9126 topk_dict {'top1': 0.9126} is_best False lr [0.010000000000000002]
training epoch 328 val accuracy 0.9118 topk_dict {'top1': 0.9118} is_best False lr [0.010000000000000002]
training epoch 329 val accuracy 0.9112 topk_dict {'top1': 0.9112} is_best False lr [0.010000000000000002]
training epoch 330 val accuracy 0.9156 topk_dict {'top1': 0.9156} is_best False lr [0.010000000000000002]
training epoch 331 val accuracy 0.9116 topk_dict {'top1': 0.9116} is_best False lr [0.010000000000000002]
training epoch 332 val accuracy 0.9128 topk_dict {'top1': 0.9128} is_best False lr [0.010000000000000002]
training epoch 333 val accuracy 0.9134 topk_dict {'top1': 0.9134} is_best False lr [0.010000000000000002]
training epoch 334 val accuracy 0.9118 topk_dict {'top1': 0.9118} is_best False lr [0.010000000000000002]
training epoch 335 val accuracy 0.913 topk_dict {'top1': 0.913} is_best False lr [0.010000000000000002]
training epoch 336 val accuracy 0.9124 topk_dict {'top1': 0.9124} is_best False lr [0.010000000000000002]
training epoch 337 val accuracy 0.9112 topk_dict {'top1': 0.9112} is_best False lr [0.010000000000000002]
training epoch 338 val accuracy 0.9158 topk_dict {'top1': 0.9158} is_best False lr [0.010000000000000002]
training epoch 339 val accuracy 0.91 topk_dict {'top1': 0.91} is_best False lr [0.010000000000000002]
training epoch 340 val accuracy 0.9124 topk_dict {'top1': 0.9124} is_best False lr [0.010000000000000002]
training epoch 341 val accuracy 0.9122 topk_dict {'top1': 0.9122} is_best False lr [0.010000000000000002]
training epoch 342 val accuracy 0.9116 topk_dict {'top1': 0.9116} is_best False lr [0.010000000000000002]
training epoch 343 val accuracy 0.9094 topk_dict {'top1': 0.9094} is_best False lr [0.010000000000000002]
training epoch 344 val accuracy 0.9102 topk_dict {'top1': 0.9102} is_best False lr [0.010000000000000002]
training epoch 345 val accuracy 0.9138 topk_dict {'top1': 0.9138} is_best False lr [0.010000000000000002]
training epoch 346 val accuracy 0.9114 topk_dict {'top1': 0.9114} is_best False lr [0.010000000000000002]
training epoch 347 val accuracy 0.9084 topk_dict {'top1': 0.9084} is_best False lr [0.010000000000000002]
training epoch 348 val accuracy 0.9108 topk_dict {'top1': 0.9108} is_best False lr [0.010000000000000002]
training epoch 349 val accuracy 0.9128 topk_dict {'top1': 0.9128} is_best False lr [0.010000000000000002]
training epoch 350 val accuracy 0.9126 topk_dict {'top1': 0.9126} is_best False lr [0.010000000000000002]
training epoch 351 val accuracy 0.9148 topk_dict {'top1': 0.9148} is_best False lr [0.010000000000000002]
training epoch 352 val accuracy 0.9134 topk_dict {'top1': 0.9134} is_best False lr [0.010000000000000002]
training epoch 353 val accuracy 0.9138 topk_dict {'top1': 0.9138} is_best False lr [0.010000000000000002]
training epoch 354 val accuracy 0.911 topk_dict {'top1': 0.911} is_best False lr [0.010000000000000002]
training epoch 355 val accuracy 0.912 topk_dict {'top1': 0.912} is_best False lr [0.010000000000000002]
training epoch 356 val accuracy 0.9102 topk_dict {'top1': 0.9102} is_best False lr [0.010000000000000002]
training epoch 357 val accuracy 0.914 topk_dict {'top1': 0.914} is_best False lr [0.010000000000000002]
training epoch 358 val accuracy 0.9142 topk_dict {'top1': 0.9142} is_best False lr [0.010000000000000002]
training epoch 359 val accuracy 0.9142 topk_dict {'top1': 0.9142} is_best False lr [0.010000000000000002]
training epoch 360 val accuracy 0.9128 topk_dict {'top1': 0.9128} is_best False lr [0.010000000000000002]
training epoch 361 val accuracy 0.9146 topk_dict {'top1': 0.9146} is_best False lr [0.010000000000000002]
training epoch 362 val accuracy 0.9134 topk_dict {'top1': 0.9134} is_best False lr [0.010000000000000002]
training epoch 363 val accuracy 0.9112 topk_dict {'top1': 0.9112} is_best False lr [0.010000000000000002]
training epoch 364 val accuracy 0.9138 topk_dict {'top1': 0.9138} is_best False lr [0.010000000000000002]
training epoch 365 val accuracy 0.9092 topk_dict {'top1': 0.9092} is_best False lr [0.010000000000000002]
training epoch 366 val accuracy 0.9108 topk_dict {'top1': 0.9108} is_best False lr [0.010000000000000002]
training epoch 367 val accuracy 0.9128 topk_dict {'top1': 0.9128} is_best False lr [0.010000000000000002]
training epoch 368 val accuracy 0.91 topk_dict {'top1': 0.91} is_best False lr [0.010000000000000002]
training epoch 369 val accuracy 0.911 topk_dict {'top1': 0.911} is_best False lr [0.010000000000000002]
training epoch 370 val accuracy 0.9098 topk_dict {'top1': 0.9098} is_best False lr [0.010000000000000002]
training epoch 371 val accuracy 0.9152 topk_dict {'top1': 0.9152} is_best False lr [0.010000000000000002]
training epoch 372 val accuracy 0.9104 topk_dict {'top1': 0.9104} is_best False lr [0.010000000000000002]
training epoch 373 val accuracy 0.911 topk_dict {'top1': 0.911} is_best False lr [0.010000000000000002]
training epoch 374 val accuracy 0.915 topk_dict {'top1': 0.915} is_best False lr [0.010000000000000002]
training epoch 375 val accuracy 0.9168 topk_dict {'top1': 0.9168} is_best False lr [0.0010000000000000002]
training epoch 376 val accuracy 0.9156 topk_dict {'top1': 0.9156} is_best False lr [0.0010000000000000002]
training epoch 377 val accuracy 0.9132 topk_dict {'top1': 0.9132} is_best False lr [0.0010000000000000002]
training epoch 378 val accuracy 0.914 topk_dict {'top1': 0.914} is_best False lr [0.0010000000000000002]
training epoch 379 val accuracy 0.9158 topk_dict {'top1': 0.9158} is_best False lr [0.0010000000000000002]
training epoch 380 val accuracy 0.9158 topk_dict {'top1': 0.9158} is_best False lr [0.0010000000000000002]
training epoch 381 val accuracy 0.9168 topk_dict {'top1': 0.9168} is_best False lr [0.0010000000000000002]
training epoch 382 val accuracy 0.917 topk_dict {'top1': 0.917} is_best False lr [0.0010000000000000002]
training epoch 383 val accuracy 0.9156 topk_dict {'top1': 0.9156} is_best False lr [0.0010000000000000002]
training epoch 384 val accuracy 0.9152 topk_dict {'top1': 0.9152} is_best False lr [0.0010000000000000002]
training epoch 385 val accuracy 0.917 topk_dict {'top1': 0.917} is_best False lr [0.0010000000000000002]
training epoch 386 val accuracy 0.916 topk_dict {'top1': 0.916} is_best False lr [0.0010000000000000002]
training epoch 387 val accuracy 0.917 topk_dict {'top1': 0.917} is_best False lr [0.0010000000000000002]
training epoch 388 val accuracy 0.9162 topk_dict {'top1': 0.9162} is_best False lr [0.0010000000000000002]
training epoch 389 val accuracy 0.9152 topk_dict {'top1': 0.9152} is_best False lr [0.0010000000000000002]
training epoch 390 val accuracy 0.9162 topk_dict {'top1': 0.9162} is_best False lr [0.0010000000000000002]
training epoch 391 val accuracy 0.9164 topk_dict {'top1': 0.9164} is_best False lr [0.0010000000000000002]
training epoch 392 val accuracy 0.9158 topk_dict {'top1': 0.9158} is_best False lr [0.0010000000000000002]
training epoch 393 val accuracy 0.9164 topk_dict {'top1': 0.9164} is_best False lr [0.0010000000000000002]
training epoch 394 val accuracy 0.9142 topk_dict {'top1': 0.9142} is_best False lr [0.0010000000000000002]
training epoch 395 val accuracy 0.9154 topk_dict {'top1': 0.9154} is_best False lr [0.0010000000000000002]
training epoch 396 val accuracy 0.9162 topk_dict {'top1': 0.9162} is_best False lr [0.0010000000000000002]
training epoch 397 val accuracy 0.9148 topk_dict {'top1': 0.9148} is_best False lr [0.0010000000000000002]
training epoch 398 val accuracy 0.915 topk_dict {'top1': 0.915} is_best False lr [0.0010000000000000002]
training epoch 399 val accuracy 0.9144 topk_dict {'top1': 0.9144} is_best False lr [0.0010000000000000002]
training epoch 400 val accuracy 0.9152 topk_dict {'top1': 0.9152} is_best False lr [0.0010000000000000002]
training epoch 401 val accuracy 0.9162 topk_dict {'top1': 0.9162} is_best False lr [0.0010000000000000002]
training epoch 402 val accuracy 0.9168 topk_dict {'top1': 0.9168} is_best False lr [0.0010000000000000002]
training epoch 403 val accuracy 0.916 topk_dict {'top1': 0.916} is_best False lr [0.0010000000000000002]
training epoch 404 val accuracy 0.9164 topk_dict {'top1': 0.9164} is_best False lr [0.0010000000000000002]
training epoch 405 val accuracy 0.9154 topk_dict {'top1': 0.9154} is_best False lr [0.0010000000000000002]
training epoch 406 val accuracy 0.917 topk_dict {'top1': 0.917} is_best False lr [0.0010000000000000002]
training epoch 407 val accuracy 0.9166 topk_dict {'top1': 0.9166} is_best False lr [0.0010000000000000002]
training epoch 408 val accuracy 0.9148 topk_dict {'top1': 0.9148} is_best False lr [0.0010000000000000002]
training epoch 409 val accuracy 0.9168 topk_dict {'top1': 0.9168} is_best False lr [0.0010000000000000002]
training epoch 410 val accuracy 0.9162 topk_dict {'top1': 0.9162} is_best False lr [0.0010000000000000002]
training epoch 411 val accuracy 0.915 topk_dict {'top1': 0.915} is_best False lr [0.0010000000000000002]
training epoch 412 val accuracy 0.9164 topk_dict {'top1': 0.9164} is_best False lr [0.0010000000000000002]
training epoch 413 val accuracy 0.9166 topk_dict {'top1': 0.9166} is_best False lr [0.0010000000000000002]
training epoch 414 val accuracy 0.917 topk_dict {'top1': 0.917} is_best False lr [0.0010000000000000002]
training epoch 415 val accuracy 0.9188 topk_dict {'top1': 0.9188} is_best True lr [0.0010000000000000002]
training epoch 416 val accuracy 0.9172 topk_dict {'top1': 0.9172} is_best False lr [0.0010000000000000002]
training epoch 417 val accuracy 0.9176 topk_dict {'top1': 0.9176} is_best False lr [0.0010000000000000002]
training epoch 418 val accuracy 0.9172 topk_dict {'top1': 0.9172} is_best False lr [0.0010000000000000002]
training epoch 419 val accuracy 0.9176 topk_dict {'top1': 0.9176} is_best False lr [0.0010000000000000002]
training epoch 420 val accuracy 0.915 topk_dict {'top1': 0.915} is_best False lr [0.0010000000000000002]
training epoch 421 val accuracy 0.9152 topk_dict {'top1': 0.9152} is_best False lr [0.0010000000000000002]
training epoch 422 val accuracy 0.9176 topk_dict {'top1': 0.9176} is_best False lr [0.0010000000000000002]
training epoch 423 val accuracy 0.9172 topk_dict {'top1': 0.9172} is_best False lr [0.0010000000000000002]
training epoch 424 val accuracy 0.9162 topk_dict {'top1': 0.9162} is_best False lr [0.0010000000000000002]
training epoch 425 val accuracy 0.9172 topk_dict {'top1': 0.9172} is_best False lr [0.0010000000000000002]
training epoch 426 val accuracy 0.9168 topk_dict {'top1': 0.9168} is_best False lr [0.0010000000000000002]
training epoch 427 val accuracy 0.9164 topk_dict {'top1': 0.9164} is_best False lr [0.0010000000000000002]
training epoch 428 val accuracy 0.9158 topk_dict {'top1': 0.9158} is_best False lr [0.0010000000000000002]
training epoch 429 val accuracy 0.9176 topk_dict {'top1': 0.9176} is_best False lr [0.0010000000000000002]
training epoch 430 val accuracy 0.9158 topk_dict {'top1': 0.9158} is_best False lr [0.0010000000000000002]
training epoch 431 val accuracy 0.9168 topk_dict {'top1': 0.9168} is_best False lr [0.0010000000000000002]
training epoch 432 val accuracy 0.9176 topk_dict {'top1': 0.9176} is_best False lr [0.0010000000000000002]
training epoch 433 val accuracy 0.9172 topk_dict {'top1': 0.9172} is_best False lr [0.0010000000000000002]
training epoch 434 val accuracy 0.917 topk_dict {'top1': 0.917} is_best False lr [0.0010000000000000002]
training epoch 435 val accuracy 0.9164 topk_dict {'top1': 0.9164} is_best False lr [0.0010000000000000002]
training epoch 436 val accuracy 0.9164 topk_dict {'top1': 0.9164} is_best False lr [0.0010000000000000002]
training epoch 437 val accuracy 0.9168 topk_dict {'top1': 0.9168} is_best False lr [0.0010000000000000002]
training epoch 438 val accuracy 0.917 topk_dict {'top1': 0.917} is_best False lr [0.0010000000000000002]
training epoch 439 val accuracy 0.9144 topk_dict {'top1': 0.9144} is_best False lr [0.0010000000000000002]
training epoch 440 val accuracy 0.917 topk_dict {'top1': 0.917} is_best False lr [0.0010000000000000002]
training epoch 441 val accuracy 0.9158 topk_dict {'top1': 0.9158} is_best False lr [0.0010000000000000002]
training epoch 442 val accuracy 0.9166 topk_dict {'top1': 0.9166} is_best False lr [0.0010000000000000002]
training epoch 443 val accuracy 0.9176 topk_dict {'top1': 0.9176} is_best False lr [0.0010000000000000002]
training epoch 444 val accuracy 0.9176 topk_dict {'top1': 0.9176} is_best False lr [0.0010000000000000002]
training epoch 445 val accuracy 0.915 topk_dict {'top1': 0.915} is_best False lr [0.0010000000000000002]
training epoch 446 val accuracy 0.916 topk_dict {'top1': 0.916} is_best False lr [0.0010000000000000002]
training epoch 447 val accuracy 0.9162 topk_dict {'top1': 0.9162} is_best False lr [0.0010000000000000002]
training epoch 448 val accuracy 0.916 topk_dict {'top1': 0.916} is_best False lr [0.0010000000000000002]
training epoch 449 val accuracy 0.9164 topk_dict {'top1': 0.9164} is_best False lr [0.0010000000000000002]
training epoch 450 val accuracy 0.9178 topk_dict {'top1': 0.9178} is_best False lr [0.0010000000000000002]
training epoch 451 val accuracy 0.9176 topk_dict {'top1': 0.9176} is_best False lr [0.0010000000000000002]
training epoch 452 val accuracy 0.9168 topk_dict {'top1': 0.9168} is_best False lr [0.0010000000000000002]
training epoch 453 val accuracy 0.9154 topk_dict {'top1': 0.9154} is_best False lr [0.0010000000000000002]
training epoch 454 val accuracy 0.9152 topk_dict {'top1': 0.9152} is_best False lr [0.0010000000000000002]
training epoch 455 val accuracy 0.9152 topk_dict {'top1': 0.9152} is_best False lr [0.0010000000000000002]
training epoch 456 val accuracy 0.9162 topk_dict {'top1': 0.9162} is_best False lr [0.0010000000000000002]
training epoch 457 val accuracy 0.9176 topk_dict {'top1': 0.9176} is_best False lr [0.0010000000000000002]
training epoch 458 val accuracy 0.9178 topk_dict {'top1': 0.9178} is_best False lr [0.0010000000000000002]
training epoch 459 val accuracy 0.9166 topk_dict {'top1': 0.9166} is_best False lr [0.0010000000000000002]
training epoch 460 val accuracy 0.9172 topk_dict {'top1': 0.9172} is_best False lr [0.0010000000000000002]
training epoch 461 val accuracy 0.9162 topk_dict {'top1': 0.9162} is_best False lr [0.0010000000000000002]
training epoch 462 val accuracy 0.9176 topk_dict {'top1': 0.9176} is_best False lr [0.0010000000000000002]
training epoch 463 val accuracy 0.9154 topk_dict {'top1': 0.9154} is_best False lr [0.0010000000000000002]
training epoch 464 val accuracy 0.9166 topk_dict {'top1': 0.9166} is_best False lr [0.0010000000000000002]
training epoch 465 val accuracy 0.916 topk_dict {'top1': 0.916} is_best False lr [0.0010000000000000002]
training epoch 466 val accuracy 0.9178 topk_dict {'top1': 0.9178} is_best False lr [0.0010000000000000002]
training epoch 467 val accuracy 0.9172 topk_dict {'top1': 0.9172} is_best False lr [0.0010000000000000002]
training epoch 468 val accuracy 0.9158 topk_dict {'top1': 0.9158} is_best False lr [0.0010000000000000002]
training epoch 469 val accuracy 0.9166 topk_dict {'top1': 0.9166} is_best False lr [0.0010000000000000002]
training epoch 470 val accuracy 0.9164 topk_dict {'top1': 0.9164} is_best False lr [0.0010000000000000002]
training epoch 471 val accuracy 0.917 topk_dict {'top1': 0.917} is_best False lr [0.0010000000000000002]
training epoch 472 val accuracy 0.9174 topk_dict {'top1': 0.9174} is_best False lr [0.0010000000000000002]
training epoch 473 val accuracy 0.916 topk_dict {'top1': 0.916} is_best False lr [0.0010000000000000002]
training epoch 474 val accuracy 0.9174 topk_dict {'top1': 0.9174} is_best False lr [0.0010000000000000002]
training epoch 475 val accuracy 0.9168 topk_dict {'top1': 0.9168} is_best False lr [0.0010000000000000002]
training epoch 476 val accuracy 0.917 topk_dict {'top1': 0.917} is_best False lr [0.0010000000000000002]
training epoch 477 val accuracy 0.9178 topk_dict {'top1': 0.9178} is_best False lr [0.0010000000000000002]
training epoch 478 val accuracy 0.9166 topk_dict {'top1': 0.9166} is_best False lr [0.0010000000000000002]
training epoch 479 val accuracy 0.917 topk_dict {'top1': 0.917} is_best False lr [0.0010000000000000002]
training epoch 480 val accuracy 0.9182 topk_dict {'top1': 0.9182} is_best False lr [0.0010000000000000002]
training epoch 481 val accuracy 0.9174 topk_dict {'top1': 0.9174} is_best False lr [0.0010000000000000002]
training epoch 482 val accuracy 0.9164 topk_dict {'top1': 0.9164} is_best False lr [0.0010000000000000002]
training epoch 483 val accuracy 0.9176 topk_dict {'top1': 0.9176} is_best False lr [0.0010000000000000002]
training epoch 484 val accuracy 0.917 topk_dict {'top1': 0.917} is_best False lr [0.0010000000000000002]
training epoch 485 val accuracy 0.916 topk_dict {'top1': 0.916} is_best False lr [0.0010000000000000002]
training epoch 486 val accuracy 0.9162 topk_dict {'top1': 0.9162} is_best False lr [0.0010000000000000002]
training epoch 487 val accuracy 0.9162 topk_dict {'top1': 0.9162} is_best False lr [0.0010000000000000002]
training epoch 488 val accuracy 0.9154 topk_dict {'top1': 0.9154} is_best False lr [0.0010000000000000002]
training epoch 489 val accuracy 0.9144 topk_dict {'top1': 0.9144} is_best False lr [0.0010000000000000002]
training epoch 490 val accuracy 0.9158 topk_dict {'top1': 0.9158} is_best False lr [0.0010000000000000002]
training epoch 491 val accuracy 0.9166 topk_dict {'top1': 0.9166} is_best False lr [0.0010000000000000002]
training epoch 492 val accuracy 0.9168 topk_dict {'top1': 0.9168} is_best False lr [0.0010000000000000002]
training epoch 493 val accuracy 0.9166 topk_dict {'top1': 0.9166} is_best False lr [0.0010000000000000002]
training epoch 494 val accuracy 0.9172 topk_dict {'top1': 0.9172} is_best False lr [0.0010000000000000002]
training epoch 495 val accuracy 0.917 topk_dict {'top1': 0.917} is_best False lr [0.0010000000000000002]
training epoch 496 val accuracy 0.9172 topk_dict {'top1': 0.9172} is_best False lr [0.0010000000000000002]
training epoch 497 val accuracy 0.9158 topk_dict {'top1': 0.9158} is_best False lr [0.0010000000000000002]
training epoch 498 val accuracy 0.9154 topk_dict {'top1': 0.9154} is_best False lr [0.0010000000000000002]
training epoch 499 val accuracy 0.9152 topk_dict {'top1': 0.9152} is_best False lr [0.0010000000000000002]
loading model_best from epoch 415 (acc 0.918800)
finished training. finished 500 epochs. accuracy 0.9188 topk_dict {'top1': 0.9188}
