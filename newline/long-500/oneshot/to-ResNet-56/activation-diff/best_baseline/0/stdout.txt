start iteration 0
[activation diff]: block to remove picked: 33, with score 0.007069. All blocks and scores: [(33, 0.007068843638990074), (32, 0.009399589733220637), (30, 0.010011187638156116), (31, 0.01023258175700903), (34, 0.013294661417603493), (29, 0.01342111686244607), (35, 0.01595768961124122), (26, 0.01607214123941958), (28, 0.017636860953643918), (27, 0.019022798165678978), (43, 0.019996492192149162), (46, 0.020590225234627724), (25, 0.022078295005485415), (23, 0.022228716174140573), (41, 0.022336415946483612), (44, 0.023145999060943723), (40, 0.02374959085136652), (45, 0.02397549501620233), (21, 0.024941090028733015), (48, 0.024957706686109304), (22, 0.025151390582323074), (50, 0.02528717485256493), (24, 0.025880583096295595), (49, 0.02591664856299758), (42, 0.02623223210684955), (20, 0.026848892215639353), (47, 0.028632947709411383), (38, 0.03134434390813112), (39, 0.0314412962179631), (15, 0.032058384735137224), (7, 0.032445503398776054), (19, 0.03254077862948179), (37, 0.037918031215667725), (51, 0.0417875861749053), (9, 0.04337632842361927), (6, 0.04682369530200958), (14, 0.047897722106426954), (4, 0.04852241184562445), (2, 0.054577404633164406), (3, 0.05784992780536413), (13, 0.059144286438822746), (11, 0.05970003316178918), (17, 0.06132525531575084), (0, 0.06337464926764369), (1, 0.06593216210603714), (52, 0.06606104224920273), (8, 0.074663613922894), (10, 0.08082299493253231), (16, 0.08527506235986948), (12, 0.0903953779488802), (5, 0.10671143885701895), (36, 0.4361986480653286), (18, 0.5117432847619057), (53, 0.8053385093808174)]
computing accuracy for after removing block 33 . block score: 0.007068843638990074
removed block 33 current accuracy 0.949 loss from initial  0.0024000000000000687
since last training loss: 0.0024000000000000687 threshold 999.0 training needed False
start iteration 1
[activation diff]: block to remove picked: 32, with score 0.009400. All blocks and scores: [(32, 0.009399589616805315), (30, 0.01001118787098676), (31, 0.010232581640593708), (34, 0.013119243900291622), (29, 0.013421116746030748), (26, 0.016072141472250223), (35, 0.01609392766840756), (28, 0.017636861419305205), (27, 0.01902279770001769), (43, 0.01985268760472536), (46, 0.02030070568434894), (41, 0.02186027471907437), (25, 0.022078294306993484), (23, 0.022228715009987354), (44, 0.022977192886173725), (40, 0.023573831422254443), (45, 0.023648238508030772), (48, 0.02454021736048162), (50, 0.02477082284167409), (21, 0.024941089330241084), (22, 0.025151389883831143), (49, 0.025575740728527308), (24, 0.025880582630634308), (42, 0.025893412297591567), (20, 0.02684889081865549), (47, 0.028072760673239827), (38, 0.0310911878477782), (39, 0.031191361136734486), (15, 0.03205838426947594), (7, 0.03244550293311477), (19, 0.032540778163820505), (37, 0.037973211612552404), (51, 0.04127101367339492), (9, 0.04337632795795798), (6, 0.046823696698993444), (14, 0.04789772117510438), (4, 0.04852241324260831), (2, 0.05457740230485797), (3, 0.057849929202347994), (13, 0.05914428876712918), (11, 0.05970003269612789), (17, 0.06132525531575084), (0, 0.06337464740499854), (52, 0.0649335184134543), (1, 0.06593216303735971), (8, 0.07466361485421658), (10, 0.08082299493253231), (16, 0.08527506422251463), (12, 0.09039537515491247), (5, 0.1067114369943738), (36, 0.4339805990457535), (18, 0.5117433071136475), (53, 0.806397058069706)]
computing accuracy for after removing block 32 . block score: 0.009399589616805315
removed block 32 current accuracy 0.9482 loss from initial  0.0031999999999999806
since last training loss: 0.0031999999999999806 threshold 999.0 training needed False
start iteration 2
[activation diff]: block to remove picked: 30, with score 0.010011. All blocks and scores: [(30, 0.010011187405325472), (31, 0.010232581407763064), (34, 0.01275888190139085), (29, 0.013421116746030748), (35, 0.015918420627713203), (26, 0.016072141472250223), (28, 0.017636860953643918), (27, 0.019022797932848334), (43, 0.01985046500340104), (46, 0.020411915378645062), (41, 0.021827629301697016), (25, 0.022078294306993484), (23, 0.022228715708479285), (44, 0.02289147791452706), (40, 0.0236025785561651), (45, 0.023770849220454693), (48, 0.024519873084500432), (50, 0.024639350594952703), (21, 0.024941089330241084), (22, 0.025151390116661787), (49, 0.025392549578100443), (42, 0.025712220929563046), (24, 0.025880582630634308), (20, 0.026848892215639353), (47, 0.028052504640072584), (38, 0.030935873743146658), (39, 0.031173035968095064), (15, 0.0320583856664598), (7, 0.03244550293311477), (19, 0.03254077862948179), (37, 0.0383431906811893), (51, 0.04113080771639943), (9, 0.043376327492296696), (6, 0.04682369623333216), (14, 0.04789772070944309), (4, 0.048522412311285734), (2, 0.05457740509882569), (3, 0.05784992687404156), (13, 0.05914428737014532), (11, 0.05970003316178918), (17, 0.061325255781412125), (0, 0.06337464554235339), (52, 0.06441722810268402), (1, 0.06593216117471457), (8, 0.07466361485421658), (10, 0.08082299400120974), (16, 0.08527506049722433), (12, 0.09039537701755762), (5, 0.10671143792569637), (36, 0.435020312666893), (18, 0.5117433071136475), (53, 0.8136166632175446)]
computing accuracy for after removing block 30 . block score: 0.010011187405325472
removed block 30 current accuracy 0.9466 loss from initial  0.0048000000000000265
since last training loss: 0.0048000000000000265 threshold 999.0 training needed False
start iteration 3
[activation diff]: block to remove picked: 31, with score 0.010245. All blocks and scores: [(31, 0.010244824341498315), (34, 0.012400159728713334), (29, 0.01342111686244607), (35, 0.01591864973306656), (26, 0.016072141472250223), (28, 0.01763686118647456), (27, 0.019022797932848334), (43, 0.019867350114509463), (46, 0.02027974440716207), (41, 0.02175602037459612), (25, 0.022078294772654772), (23, 0.02222871547564864), (44, 0.023001376073807478), (40, 0.02373992674984038), (45, 0.02379016811028123), (48, 0.02435004524886608), (50, 0.02446310454979539), (21, 0.02494108979590237), (22, 0.02515139034949243), (49, 0.025246929842978716), (42, 0.025273551931604743), (24, 0.025880582397803664), (20, 0.026848891749978065), (47, 0.02772757550701499), (38, 0.030746274394914508), (39, 0.03128179511986673), (15, 0.032058384735137224), (7, 0.03244550293311477), (19, 0.03254077909514308), (37, 0.03895266819745302), (51, 0.04082479886710644), (9, 0.04337632795795798), (6, 0.04682369623333216), (14, 0.04789772164076567), (4, 0.04852241277694702), (2, 0.05457740370184183), (3, 0.05784992873668671), (13, 0.05914428597316146), (11, 0.05970003409311175), (17, 0.06132525531575084), (0, 0.06337464554235339), (52, 0.0635675610974431), (1, 0.06593216210603714), (8, 0.07466361578553915), (10, 0.08082299493253231), (16, 0.0852750614285469), (12, 0.09039537701755762), (5, 0.10671143606305122), (36, 0.4377693012356758), (18, 0.5117432996630669), (53, 0.8228829503059387)]
computing accuracy for after removing block 31 . block score: 0.010244824341498315
removed block 31 current accuracy 0.9462 loss from initial  0.005199999999999982
since last training loss: 0.005199999999999982 threshold 999.0 training needed False
start iteration 4
[activation diff]: block to remove picked: 34, with score 0.012506. All blocks and scores: [(34, 0.012506232247687876), (29, 0.013421116746030748), (35, 0.015968912048265338), (26, 0.016072141006588936), (28, 0.01763686165213585), (27, 0.019022798165678978), (43, 0.01983700809068978), (46, 0.02013718755915761), (41, 0.021584055619314313), (25, 0.022078295005485415), (23, 0.022228715708479285), (44, 0.022687325486913323), (40, 0.023569098440930247), (45, 0.023840721230953932), (48, 0.02410835912451148), (50, 0.02411420946009457), (49, 0.024870117660611868), (21, 0.024941088631749153), (42, 0.02504557534120977), (22, 0.025151390116661787), (24, 0.025880582630634308), (20, 0.026848891051486135), (47, 0.027423853054642677), (38, 0.03073564893566072), (39, 0.031410424038767815), (15, 0.03205838520079851), (7, 0.032445503398776054), (19, 0.03254077862948179), (37, 0.039083510637283325), (51, 0.04034593980759382), (9, 0.04337632842361927), (6, 0.046823696698993444), (14, 0.04789772164076567), (4, 0.048522413708269596), (2, 0.054577406495809555), (3, 0.05784992687404156), (13, 0.059144288301467896), (11, 0.05970003176480532), (17, 0.06132525438442826), (52, 0.0627010790631175), (0, 0.06337464833632112), (1, 0.06593216117471457), (8, 0.07466361578553915), (10, 0.08082299586385489), (16, 0.0852750614285469), (12, 0.09039537608623505), (5, 0.10671143885701895), (36, 0.43692686036229134), (18, 0.5117432922124863), (53, 0.8283701166510582)]
computing accuracy for after removing block 34 . block score: 0.012506232247687876
removed block 34 current accuracy 0.946 loss from initial  0.005400000000000071
since last training loss: 0.005400000000000071 threshold 999.0 training needed False
start iteration 5
[activation diff]: block to remove picked: 29, with score 0.013421. All blocks and scores: [(29, 0.01342111686244607), (26, 0.01607214123941958), (35, 0.016558772418648005), (28, 0.017636860720813274), (27, 0.019022797932848334), (43, 0.020302684511989355), (46, 0.020324197597801685), (41, 0.021962702507153153), (25, 0.02207829407416284), (23, 0.02222871547564864), (44, 0.023045077919960022), (48, 0.024024546844884753), (50, 0.024096972309052944), (40, 0.024156817002221942), (45, 0.02416840917430818), (49, 0.024922372307628393), (21, 0.024941089563071728), (22, 0.02515139034949243), (42, 0.02581605943851173), (24, 0.02588058216497302), (20, 0.026848890585824847), (47, 0.027568295365199447), (38, 0.03178726346231997), (15, 0.03205838426947594), (39, 0.032257913146167994), (7, 0.032445503398776054), (19, 0.03254077769815922), (51, 0.04008621256798506), (37, 0.04069073125720024), (9, 0.04337632656097412), (6, 0.046823696698993444), (14, 0.04789771977812052), (4, 0.04852241184562445), (2, 0.05457740603014827), (3, 0.05784992687404156), (13, 0.059144288301467896), (11, 0.059700033627450466), (17, 0.06132525485008955), (52, 0.06221094820648432), (0, 0.06337464833632112), (1, 0.06593216117471457), (8, 0.07466361671686172), (10, 0.08082299679517746), (16, 0.08527506235986948), (12, 0.0903953779488802), (5, 0.10671143606305122), (36, 0.44933701306581497), (18, 0.5117433145642281), (53, 0.8277030661702156)]
computing accuracy for after removing block 29 . block score: 0.01342111686244607
removed block 29 current accuracy 0.9406 loss from initial  0.010800000000000032
since last training loss: 0.010800000000000032 threshold 999.0 training needed False
start iteration 6
[activation diff]: block to remove picked: 26, with score 0.016072. All blocks and scores: [(26, 0.016072140773758292), (35, 0.016370511148124933), (28, 0.017636860953643918), (27, 0.01902279770001769), (43, 0.019856703002005816), (46, 0.01998897618614137), (41, 0.021256204694509506), (25, 0.022078294772654772), (23, 0.02222871594130993), (44, 0.022692033322528005), (48, 0.023521370720118284), (50, 0.023533890023827553), (40, 0.023616240127012134), (45, 0.023933292599394917), (49, 0.024449915857985616), (42, 0.0248383276630193), (21, 0.024941089563071728), (22, 0.02515139104798436), (24, 0.02588058286346495), (47, 0.026813456788659096), (20, 0.026848890585824847), (38, 0.031083731446415186), (39, 0.03205688903108239), (15, 0.03205838520079851), (7, 0.03244550293311477), (19, 0.032540778163820505), (51, 0.039079748559743166), (37, 0.0401521441526711), (9, 0.043376327492296696), (6, 0.04682369530200958), (14, 0.04789772164076567), (4, 0.048522412311285734), (2, 0.05457740509882569), (3, 0.05784992780536413), (13, 0.05914428969845176), (11, 0.05970003316178918), (52, 0.060369076672941446), (17, 0.061325253918766975), (0, 0.06337464740499854), (1, 0.06593216303735971), (8, 0.07466361485421658), (10, 0.08082299400120974), (16, 0.08527506049722433), (12, 0.09039537608623505), (5, 0.1067114369943738), (36, 0.4432784430682659), (18, 0.5117432922124863), (53, 0.8375032693147659)]
computing accuracy for after removing block 26 . block score: 0.016072140773758292
removed block 26 current accuracy 0.9362 loss from initial  0.015199999999999991
since last training loss: 0.015199999999999991 threshold 999.0 training needed False
start iteration 7
[activation diff]: block to remove picked: 35, with score 0.015504. All blocks and scores: [(35, 0.015504144015721977), (28, 0.01698602200485766), (27, 0.018769708927720785), (43, 0.019405571511015296), (46, 0.01970007666386664), (41, 0.020515799522399902), (25, 0.022078295703977346), (23, 0.022228714544326067), (44, 0.02250757277943194), (48, 0.022899369010701776), (50, 0.022937727626413107), (40, 0.023057402577251196), (42, 0.023520407965406775), (45, 0.023633699165657163), (49, 0.024081918876618147), (21, 0.02494108979590237), (22, 0.025151390116661787), (24, 0.025880581932142377), (47, 0.026322792284190655), (20, 0.02684889198280871), (38, 0.030149149475619197), (39, 0.031466697342693806), (15, 0.03205838520079851), (7, 0.032445503398776054), (19, 0.032540778163820505), (51, 0.03785192873328924), (37, 0.039268902968615294), (9, 0.04337632656097412), (6, 0.046823696698993444), (14, 0.04789772117510438), (4, 0.04852241184562445), (2, 0.05457740556448698), (3, 0.05784992687404156), (52, 0.05846811970695853), (13, 0.05914428737014532), (11, 0.059700035490095615), (17, 0.06132525485008955), (0, 0.06337464367970824), (1, 0.06593216117471457), (8, 0.07466361578553915), (10, 0.08082299493253231), (16, 0.08527506329119205), (12, 0.0903953779488802), (5, 0.1067114369943738), (36, 0.43490004166960716), (18, 0.5117432996630669), (53, 0.8595060929656029)]
computing accuracy for after removing block 35 . block score: 0.015504144015721977
removed block 35 current accuracy 0.9314 loss from initial  0.020000000000000018
since last training loss: 0.020000000000000018 threshold 999.0 training needed False
start iteration 8
[activation diff]: block to remove picked: 28, with score 0.016986. All blocks and scores: [(28, 0.016986021539196372), (43, 0.018381991423666477), (27, 0.018769709393382072), (46, 0.018842301098629832), (41, 0.01901637064293027), (48, 0.021309157367795706), (50, 0.021624521352350712), (44, 0.021748854778707027), (40, 0.02191696735098958), (42, 0.021930374205112457), (25, 0.02207829523831606), (23, 0.02222871547564864), (45, 0.022736448561772704), (49, 0.02297006407752633), (21, 0.024941089330241084), (22, 0.025151391280815005), (47, 0.025355831952765584), (24, 0.025880583096295595), (20, 0.026848891749978065), (38, 0.028691886691376567), (39, 0.029624431626871228), (15, 0.0320583856664598), (7, 0.03244550293311477), (19, 0.03254077909514308), (51, 0.036016357596963644), (37, 0.036430368199944496), (9, 0.04337632702663541), (6, 0.04682369716465473), (14, 0.047897722106426954), (4, 0.04852241184562445), (2, 0.05457740370184183), (52, 0.05466857831925154), (3, 0.05784992640838027), (13, 0.059144286904484034), (11, 0.05970003455877304), (17, 0.06132525485008955), (0, 0.06337464833632112), (1, 0.06593216210603714), (8, 0.07466361485421658), (10, 0.08082299772650003), (16, 0.08527506049722433), (12, 0.09039537608623505), (5, 0.10671143606305122), (36, 0.41641608625650406), (18, 0.5117433071136475), (53, 0.8948249146342278)]
computing accuracy for after removing block 28 . block score: 0.016986021539196372
removed block 28 current accuracy 0.9286 loss from initial  0.022800000000000042
since last training loss: 0.022800000000000042 threshold 999.0 training needed False
start iteration 9
[activation diff]: block to remove picked: 43, with score 0.017988. All blocks and scores: [(43, 0.017987634986639023), (46, 0.01835862430743873), (41, 0.018467807210981846), (27, 0.01876970869489014), (48, 0.020775508601218462), (42, 0.021206470439210534), (50, 0.021302447421476245), (44, 0.021586896618828177), (40, 0.021592722507193685), (25, 0.022078294772654772), (23, 0.022228715708479285), (45, 0.022315293550491333), (49, 0.02240756689570844), (47, 0.02460939739830792), (21, 0.02494108979590237), (22, 0.02515139034949243), (24, 0.025880582630634308), (20, 0.026848892448469996), (38, 0.027890325291082263), (39, 0.029191895155236125), (15, 0.03205838520079851), (7, 0.032445503398776054), (19, 0.03254077769815922), (51, 0.03550667641684413), (37, 0.035919226706027985), (9, 0.04337632842361927), (6, 0.04682369623333216), (14, 0.04789771931245923), (4, 0.048522413708269596), (52, 0.053374080918729305), (2, 0.054577404633164406), (3, 0.057849929202347994), (13, 0.05914428783580661), (11, 0.05970003455877304), (17, 0.061325253918766975), (0, 0.06337464833632112), (1, 0.06593216210603714), (8, 0.07466361485421658), (10, 0.08082299306988716), (16, 0.08527506329119205), (12, 0.0903953742235899), (5, 0.10671143792569637), (36, 0.4126182049512863), (18, 0.5117432996630669), (53, 0.9067213237285614)]
computing accuracy for after removing block 43 . block score: 0.017987634986639023
removed block 43 current accuracy 0.9278 loss from initial  0.023600000000000065
since last training loss: 0.023600000000000065 threshold 999.0 training needed False
start iteration 10
[activation diff]: block to remove picked: 41, with score 0.018468. All blocks and scores: [(41, 0.018467806978151202), (27, 0.018769708927720785), (46, 0.018994681537151337), (42, 0.021206470439210534), (48, 0.02141889533959329), (50, 0.021441322984173894), (40, 0.02159272227436304), (25, 0.022078295005485415), (23, 0.022228715708479285), (49, 0.0223390175960958), (44, 0.02278283378109336), (45, 0.023323106579482555), (21, 0.024941089330241084), (22, 0.025151390116661787), (47, 0.02538607711903751), (24, 0.025880583561956882), (20, 0.026848892448469996), (38, 0.027890325291082263), (39, 0.02919189492240548), (15, 0.032058386132121086), (7, 0.03244550293311477), (19, 0.03254077909514308), (51, 0.035217716824263334), (37, 0.03591922763735056), (9, 0.04337632702663541), (6, 0.046823696698993444), (14, 0.04789772117510438), (4, 0.04852241324260831), (52, 0.05213337251916528), (2, 0.05457740509882569), (3, 0.05784992454573512), (13, 0.05914428597316146), (11, 0.0597000359557569), (17, 0.06132525438442826), (0, 0.06337464647367597), (1, 0.06593216024339199), (8, 0.07466361578553915), (10, 0.08082299679517746), (16, 0.08527506422251463), (12, 0.0903953742235899), (5, 0.10671143792569637), (36, 0.4126181975007057), (18, 0.5117432922124863), (53, 0.9521220847964287)]
computing accuracy for after removing block 41 . block score: 0.018467806978151202
removed block 41 current accuracy 0.9244 loss from initial  0.027000000000000024
since last training loss: 0.027000000000000024 threshold 999.0 training needed False
start iteration 11
[activation diff]: block to remove picked: 27, with score 0.018770. All blocks and scores: [(27, 0.01876970799639821), (46, 0.018828539410606027), (48, 0.020589639898389578), (50, 0.021004352252930403), (40, 0.021592722041532397), (42, 0.021820761496201158), (49, 0.021985649596899748), (25, 0.022078295005485415), (23, 0.02222871547564864), (44, 0.023674041032791138), (45, 0.023752038599923253), (21, 0.02494108979590237), (22, 0.02515139034949243), (47, 0.02563072764314711), (24, 0.025880583096295595), (20, 0.02684889198280871), (38, 0.02789032575674355), (39, 0.02919189492240548), (15, 0.03205838380381465), (7, 0.03244550246745348), (19, 0.03254077862948179), (51, 0.033954931888729334), (37, 0.03591922717168927), (9, 0.04337632702663541), (6, 0.04682369576767087), (14, 0.04789772257208824), (4, 0.04852241324260831), (52, 0.04963196162134409), (2, 0.05457740509882569), (3, 0.05784992640838027), (13, 0.05914428783580661), (11, 0.05970003409311175), (17, 0.06132525438442826), (0, 0.06337464833632112), (1, 0.06593216117471457), (8, 0.07466361671686172), (10, 0.08082299400120974), (16, 0.08527506235986948), (12, 0.09039537701755762), (5, 0.10671143885701895), (36, 0.4126181975007057), (18, 0.5117432922124863), (53, 1.0119272097945213)]
computing accuracy for after removing block 27 . block score: 0.01876970799639821
removed block 27 current accuracy 0.9184 loss from initial  0.03300000000000003
since last training loss: 0.03300000000000003 threshold 999.0 training needed False
start iteration 12
[activation diff]: block to remove picked: 46, with score 0.018469. All blocks and scores: [(46, 0.018469110364094377), (48, 0.019927537068724632), (50, 0.02050375984981656), (40, 0.020875946385785937), (42, 0.021248552249744534), (49, 0.021396144526079297), (25, 0.02207829523831606), (23, 0.022228715009987354), (44, 0.022923258366063237), (45, 0.023436837131157517), (47, 0.02469770284369588), (21, 0.02494108909741044), (22, 0.025151390116661787), (24, 0.02588058286346495), (20, 0.026848892215639353), (38, 0.026989459292963147), (39, 0.028602139558643103), (15, 0.03205838520079851), (7, 0.03244550293311477), (19, 0.032540778163820505), (51, 0.03302581747993827), (37, 0.03541383473202586), (9, 0.04337632702663541), (6, 0.04682369576767087), (52, 0.04775991849601269), (14, 0.04789772070944309), (4, 0.04852241417393088), (2, 0.05457740509882569), (3, 0.05784992687404156), (13, 0.05914428550750017), (11, 0.059700032230466604), (17, 0.06132525485008955), (0, 0.06337464554235339), (1, 0.06593216210603714), (8, 0.0746636176481843), (10, 0.08082299679517746), (16, 0.08527505863457918), (12, 0.0903953779488802), (5, 0.10671143606305122), (36, 0.40587934851646423), (18, 0.5117433071136475), (53, 1.0234627947211266)]
computing accuracy for after removing block 46 . block score: 0.018469110364094377
removed block 46 current accuracy 0.9132 loss from initial  0.03820000000000001
since last training loss: 0.03820000000000001 threshold 999.0 training needed False
start iteration 13
[activation diff]: block to remove picked: 48, with score 0.020277. All blocks and scores: [(48, 0.02027660864405334), (50, 0.020639732480049133), (40, 0.02087594592012465), (42, 0.021248551551252604), (25, 0.022078295005485415), (49, 0.02211672835983336), (23, 0.02222871547564864), (44, 0.022923258831724524), (45, 0.02343683782964945), (21, 0.024941089563071728), (22, 0.025151389883831143), (24, 0.025880582397803664), (47, 0.026193964295089245), (20, 0.026848891284316778), (38, 0.026989459292963147), (39, 0.028602140257135034), (15, 0.03205838520079851), (7, 0.032445503398776054), (19, 0.03254077909514308), (51, 0.0331102623604238), (37, 0.03541383473202586), (9, 0.04337632888928056), (6, 0.04682369576767087), (52, 0.047355798073112965), (14, 0.047897722106426954), (4, 0.048522413708269596), (2, 0.054577404633164406), (3, 0.05784992780536413), (13, 0.059144286904484034), (11, 0.05970003409311175), (17, 0.061325252521783113), (0, 0.06337464740499854), (1, 0.06593216117471457), (8, 0.07466361671686172), (10, 0.08082299400120974), (16, 0.08527506049722433), (12, 0.09039537515491247), (5, 0.10671143792569637), (36, 0.405879370868206), (18, 0.5117432847619057), (53, 1.1398278772830963)]
computing accuracy for after removing block 48 . block score: 0.02027660864405334
removed block 48 current accuracy 0.9042 loss from initial  0.04720000000000002
since last training loss: 0.04720000000000002 threshold 999.0 training needed False
start iteration 14
[activation diff]: block to remove picked: 40, with score 0.020876. All blocks and scores: [(40, 0.020875946385785937), (42, 0.021248552715405822), (25, 0.02207829523831606), (50, 0.022216575918719172), (23, 0.022228715242817998), (44, 0.022923258831724524), (45, 0.023436836898326874), (49, 0.024761620676144958), (21, 0.02494108909741044), (22, 0.025151390116661787), (24, 0.025880583096295595), (47, 0.02619396452791989), (20, 0.026848891284316778), (38, 0.026989459292963147), (39, 0.028602140489965677), (15, 0.0320583856664598), (7, 0.03244550293311477), (19, 0.03254077862948179), (51, 0.033145139925181866), (37, 0.035413835663348436), (9, 0.043376327492296696), (6, 0.04682369716465473), (14, 0.04789772070944309), (4, 0.04852241277694702), (52, 0.049928945023566484), (2, 0.054577404633164406), (3, 0.057849927339702845), (13, 0.05914428783580661), (11, 0.05970003409311175), (17, 0.061325253918766975), (0, 0.06337464740499854), (1, 0.06593216024339199), (8, 0.074663613922894), (10, 0.08082299493253231), (16, 0.08527506422251463), (12, 0.0903953779488802), (5, 0.10671143606305122), (36, 0.40587935596704483), (18, 0.5117432922124863), (53, 1.252875655889511)]
computing accuracy for after removing block 40 . block score: 0.020875946385785937
removed block 40 current accuracy 0.896 loss from initial  0.055400000000000005
since last training loss: 0.055400000000000005 threshold 999.0 training needed False
start iteration 15
[activation diff]: block to remove picked: 42, with score 0.020879. All blocks and scores: [(42, 0.020879492396488786), (50, 0.02111514238640666), (25, 0.022078295471146703), (23, 0.022228715009987354), (45, 0.022996684536337852), (44, 0.02390008559450507), (49, 0.02406831830739975), (21, 0.02494108909741044), (22, 0.02515139034949243), (24, 0.02588058286346495), (47, 0.026127796387299895), (20, 0.026848891517147422), (38, 0.026989458594471216), (39, 0.028602140257135034), (15, 0.032058386132121086), (51, 0.03239615494385362), (7, 0.03244550293311477), (19, 0.03254077909514308), (37, 0.03541383380070329), (9, 0.04337632795795798), (6, 0.046823694836348295), (14, 0.04789772070944309), (52, 0.04809587262570858), (4, 0.04852241277694702), (2, 0.054577406495809555), (3, 0.05784992827102542), (13, 0.05914428737014532), (11, 0.05970003502443433), (17, 0.061325253918766975), (0, 0.06337464554235339), (1, 0.06593216210603714), (8, 0.07466361485421658), (10, 0.08082299493253231), (16, 0.08527506235986948), (12, 0.09039537888020277), (5, 0.1067114369943738), (36, 0.40587935969233513), (18, 0.5117432922124863), (53, 1.353750929236412)]
computing accuracy for after removing block 42 . block score: 0.020879492396488786
removed block 42 current accuracy 0.8888 loss from initial  0.06259999999999999
since last training loss: 0.06259999999999999 threshold 999.0 training needed False
start iteration 16
[activation diff]: block to remove picked: 50, with score 0.021091. All blocks and scores: [(50, 0.021091153379529715), (25, 0.02207829593680799), (23, 0.022228715242817998), (45, 0.023672133684158325), (49, 0.02420335542410612), (44, 0.024335477268323302), (21, 0.024941089330241084), (22, 0.025151390116661787), (47, 0.02587820403277874), (24, 0.02588058286346495), (20, 0.026848891051486135), (38, 0.026989459292963147), (39, 0.02860214072279632), (51, 0.03148272796534002), (15, 0.032058384735137224), (7, 0.03244550293311477), (19, 0.03254077909514308), (37, 0.03541383473202586), (9, 0.043376327492296696), (52, 0.04569368530064821), (6, 0.046823694836348295), (14, 0.04789772070944309), (4, 0.04852241277694702), (2, 0.054577406495809555), (3, 0.05784992454573512), (13, 0.059144288301467896), (11, 0.05970003502443433), (17, 0.06132525345310569), (0, 0.06337464647367597), (1, 0.06593216117471457), (8, 0.07466361671686172), (10, 0.08082299213856459), (16, 0.08527506049722433), (12, 0.09039537515491247), (5, 0.10671143420040607), (36, 0.40587934851646423), (18, 0.5117432996630669), (53, 1.4009628891944885)]
computing accuracy for after removing block 50 . block score: 0.021091153379529715
removed block 50 current accuracy 0.876 loss from initial  0.07540000000000002
since last training loss: 0.07540000000000002 threshold 999.0 training needed False
start iteration 17
[activation diff]: block to remove picked: 25, with score 0.022078. All blocks and scores: [(25, 0.022078295471146703), (23, 0.02222871477715671), (45, 0.023672134149819613), (49, 0.024203354958444834), (44, 0.024335477501153946), (21, 0.02494108909741044), (22, 0.025151389883831143), (47, 0.02587820403277874), (24, 0.025880582397803664), (20, 0.026848891749978065), (38, 0.02698945952579379), (39, 0.02860214002430439), (15, 0.032058384735137224), (7, 0.032445503398776054), (19, 0.03254077862948179), (51, 0.0335880839265883), (37, 0.03541383473202586), (9, 0.043376327492296696), (6, 0.04682369576767087), (14, 0.04789772117510438), (4, 0.048522412311285734), (52, 0.05229589343070984), (2, 0.05457740509882569), (3, 0.05784992873668671), (13, 0.05914428876712918), (11, 0.059700035490095615), (17, 0.06132525345310569), (0, 0.06337464554235339), (1, 0.06593216117471457), (8, 0.07466361578553915), (10, 0.08082299679517746), (16, 0.08527506235986948), (12, 0.09039537329226732), (5, 0.1067114369943738), (36, 0.40587934851646423), (18, 0.5117432996630669), (53, 1.6140173226594925)]
computing accuracy for after removing block 25 . block score: 0.022078295471146703
removed block 25 current accuracy 0.8662 loss from initial  0.08520000000000005
since last training loss: 0.08520000000000005 threshold 999.0 training needed False
start iteration 18
[activation diff]: block to remove picked: 23, with score 0.022229. All blocks and scores: [(23, 0.022228715009987354), (45, 0.023334302473813295), (49, 0.023444467457011342), (44, 0.02356359618715942), (21, 0.02494108909741044), (47, 0.025034846737980843), (22, 0.025151389883831143), (24, 0.02588058286346495), (38, 0.026316353818401694), (20, 0.02684889198280871), (39, 0.028504946967586875), (15, 0.0320583856664598), (7, 0.03244550246745348), (19, 0.03254077723249793), (51, 0.03260140819475055), (37, 0.03481204807758331), (9, 0.04337632702663541), (6, 0.04682369716465473), (14, 0.047897722106426954), (4, 0.04852241324260831), (52, 0.05003444943577051), (2, 0.054577403236180544), (3, 0.05784992687404156), (13, 0.05914428969845176), (11, 0.05970003455877304), (17, 0.06132525438442826), (0, 0.06337464833632112), (1, 0.06593216396868229), (8, 0.07466361578553915), (10, 0.08082299493253231), (16, 0.08527506235986948), (12, 0.0903953779488802), (5, 0.10671143885701895), (36, 0.39897945150732994), (18, 0.5117433071136475), (53, 1.616637796163559)]
computing accuracy for after removing block 23 . block score: 0.022228715009987354
removed block 23 current accuracy 0.8484 loss from initial  0.10299999999999998
since last training loss: 0.10299999999999998 threshold 999.0 training needed False
start iteration 19
[activation diff]: block to remove picked: 44, with score 0.023170. All blocks and scores: [(44, 0.02317032776772976), (49, 0.02331229578703642), (45, 0.023541997419670224), (47, 0.024369530845433474), (24, 0.024534435709938407), (21, 0.024941088864579797), (22, 0.02515139034949243), (38, 0.02618582732975483), (20, 0.02684889198280871), (39, 0.02844515908509493), (15, 0.032058386132121086), (7, 0.03244550293311477), (51, 0.03250818932428956), (19, 0.032540778163820505), (37, 0.03589514223858714), (9, 0.04337632795795798), (6, 0.04682369530200958), (14, 0.047897720243781805), (52, 0.04851162200793624), (4, 0.048522412311285734), (2, 0.05457740556448698), (3, 0.05784992827102542), (13, 0.059144286904484034), (11, 0.05970003409311175), (17, 0.06132525485008955), (0, 0.06337464740499854), (1, 0.06593216117471457), (8, 0.0746636176481843), (10, 0.08082299400120974), (16, 0.08527506329119205), (12, 0.09039537701755762), (5, 0.10671143420040607), (36, 0.40171103179454803), (18, 0.5117433071136475), (53, 1.603790894150734)]
computing accuracy for after removing block 44 . block score: 0.02317032776772976
removed block 44 current accuracy 0.8256 loss from initial  0.12580000000000002
since last training loss: 0.12580000000000002 threshold 999.0 training needed False
start iteration 20
[activation diff]: block to remove picked: 45, with score 0.023154. All blocks and scores: [(45, 0.02315433113835752), (49, 0.0232395778875798), (24, 0.02453443594276905), (21, 0.02494108979590237), (22, 0.025151389883831143), (47, 0.025605065282434225), (38, 0.026185827096924186), (20, 0.026848892914131284), (39, 0.028445158852264285), (15, 0.032058386132121086), (51, 0.03214721102267504), (7, 0.03244550293311477), (19, 0.03254077956080437), (37, 0.03589514223858714), (9, 0.043376327492296696), (6, 0.046823696698993444), (52, 0.04758123401552439), (14, 0.04789772164076567), (4, 0.048522413708269596), (2, 0.05457740509882569), (3, 0.05784992780536413), (13, 0.05914428737014532), (11, 0.05970003455877304), (17, 0.06132525345310569), (0, 0.06337464740499854), (1, 0.06593216117471457), (8, 0.07466361671686172), (10, 0.08082299586385489), (16, 0.08527506049722433), (12, 0.0903953779488802), (5, 0.1067114369943738), (36, 0.40171102806925774), (18, 0.5117433071136475), (53, 1.7372965663671494)]
computing accuracy for after removing block 45 . block score: 0.02315433113835752
removed block 45 current accuracy 0.7878 loss from initial  0.16360000000000008
since last training loss: 0.16360000000000008 threshold 999.0 training needed False
start iteration 21
[activation diff]: block to remove picked: 49, with score 0.023890. All blocks and scores: [(49, 0.023889959091320634), (24, 0.024534435709938407), (21, 0.02494108979590237), (22, 0.02515139104798436), (38, 0.02618582732975483), (20, 0.026848891284316778), (47, 0.026992416009306908), (39, 0.028445159317925572), (51, 0.03199382405728102), (15, 0.03205838520079851), (7, 0.03244550246745348), (19, 0.03254077862948179), (37, 0.035895141772925854), (9, 0.043376327492296696), (6, 0.04682369576767087), (14, 0.047897722106426954), (4, 0.04852241277694702), (52, 0.04859335953369737), (2, 0.054577403236180544), (3, 0.05784992687404156), (13, 0.05914428737014532), (11, 0.05970003502443433), (17, 0.061325255781412125), (0, 0.06337464740499854), (1, 0.06593216024339199), (8, 0.074663613922894), (10, 0.08082299493253231), (16, 0.08527506235986948), (12, 0.0903953742235899), (5, 0.10671143606305122), (36, 0.40171103924512863), (18, 0.5117433071136475), (53, 1.8795002847909927)]
computing accuracy for after removing block 49 . block score: 0.023889959091320634
removed block 49 current accuracy 0.7182 loss from initial  0.23320000000000007
since last training loss: 0.23320000000000007 threshold 999.0 training needed False
start iteration 22
[activation diff]: block to remove picked: 24, with score 0.024534. All blocks and scores: [(24, 0.02453443524427712), (21, 0.024941089563071728), (22, 0.02515139034949243), (38, 0.0261858266312629), (20, 0.026848892215639353), (47, 0.026992415310814977), (39, 0.02844515978358686), (15, 0.0320583856664598), (7, 0.03244550293311477), (19, 0.03254077909514308), (51, 0.033395109698176384), (37, 0.035895141772925854), (9, 0.04337632702663541), (6, 0.046823696698993444), (14, 0.047897722106426954), (4, 0.048522412311285734), (2, 0.05457740509882569), (52, 0.05507979914546013), (3, 0.057849927339702845), (13, 0.05914428737014532), (11, 0.059700033627450466), (17, 0.06132525345310569), (0, 0.06337464647367597), (1, 0.06593216117471457), (8, 0.07466361578553915), (10, 0.08082299679517746), (16, 0.08527505956590176), (12, 0.0903953779488802), (5, 0.10671143606305122), (36, 0.40171102434396744), (18, 0.5117432847619057), (53, 2.0280015021562576)]
computing accuracy for after removing block 24 . block score: 0.02453443524427712
removed block 24 current accuracy 0.676 loss from initial  0.2754
since last training loss: 0.2754 threshold 999.0 training needed False
start iteration 23
[activation diff]: block to remove picked: 21, with score 0.024941. All blocks and scores: [(21, 0.024941089563071728), (22, 0.02515139104798436), (38, 0.025702511426061392), (47, 0.02601254591718316), (20, 0.026848891749978065), (39, 0.027984512969851494), (15, 0.032058386132121086), (7, 0.03244550246745348), (19, 0.03254077862948179), (51, 0.0326524181291461), (37, 0.035636335611343384), (9, 0.04337632656097412), (6, 0.04682369623333216), (14, 0.04789771977812052), (4, 0.048522412311285734), (52, 0.05336605478078127), (2, 0.05457740603014827), (3, 0.05784992594271898), (13, 0.059144286904484034), (11, 0.059700033627450466), (17, 0.0613252529874444), (0, 0.06337464647367597), (1, 0.06593216210603714), (8, 0.07466361671686172), (10, 0.08082299586385489), (16, 0.0852750651538372), (12, 0.0903953779488802), (5, 0.10671143140643835), (36, 0.393228929489851), (18, 0.5117432922124863), (53, 2.031999111175537)]
computing accuracy for after removing block 21 . block score: 0.024941089563071728
removed block 21 current accuracy 0.651 loss from initial  0.3004
since last training loss: 0.3004 threshold 999.0 training needed False
start iteration 24
[activation diff]: block to remove picked: 22, with score 0.023398. All blocks and scores: [(22, 0.023398424964398146), (38, 0.024896334623917937), (47, 0.0253498419187963), (20, 0.026848892448469996), (39, 0.027679561404511333), (15, 0.0320583856664598), (51, 0.0322577990591526), (7, 0.03244550246745348), (19, 0.03254077769815922), (37, 0.03518892405554652), (9, 0.04337632888928056), (6, 0.04682369576767087), (14, 0.047897722106426954), (4, 0.04852241324260831), (52, 0.05205858265981078), (2, 0.05457740556448698), (3, 0.05784992780536413), (13, 0.059144288301467896), (11, 0.0597000359557569), (17, 0.06132525438442826), (0, 0.06337464554235339), (1, 0.06593216210603714), (8, 0.07466361485421658), (10, 0.08082299679517746), (16, 0.08527506235986948), (12, 0.09039537608623505), (5, 0.10671143885701895), (36, 0.3806836009025574), (18, 0.5117432996630669), (53, 2.035163015127182)]
computing accuracy for after removing block 22 . block score: 0.023398424964398146
removed block 22 current accuracy 0.6 loss from initial  0.35140000000000005
since last training loss: 0.35140000000000005 threshold 999.0 training needed False
start iteration 25
[activation diff]: block to remove picked: 47, with score 0.024421. All blocks and scores: [(47, 0.024420716566964984), (38, 0.024780795676633716), (20, 0.02684889198280871), (39, 0.027085775043815374), (15, 0.03205838520079851), (51, 0.03225559554994106), (7, 0.03244550246745348), (19, 0.03254077862948179), (37, 0.035836359951645136), (9, 0.043376326095312834), (6, 0.04682369530200958), (14, 0.047897722106426954), (4, 0.04852241324260831), (52, 0.051067161839455366), (2, 0.05457740416750312), (3, 0.05784992780536413), (13, 0.05914428923279047), (11, 0.05970003269612789), (17, 0.06132525159046054), (0, 0.06337464833632112), (1, 0.06593216117471457), (8, 0.07466361485421658), (10, 0.08082299493253231), (16, 0.08527506235986948), (12, 0.09039537701755762), (5, 0.1067114369943738), (36, 0.3776939883828163), (18, 0.5117432847619057), (53, 2.016892448067665)]
computing accuracy for after removing block 47 . block score: 0.024420716566964984
removed block 47 current accuracy 0.4938 loss from initial  0.4576
since last training loss: 0.4576 threshold 999.0 training needed False
start iteration 26
[activation diff]: block to remove picked: 38, with score 0.024781. All blocks and scores: [(38, 0.02478079590946436), (20, 0.026848891517147422), (39, 0.0270857741124928), (15, 0.03205838520079851), (7, 0.03244550293311477), (19, 0.03254077862948179), (51, 0.03279675683006644), (37, 0.035836359951645136), (9, 0.04337632702663541), (6, 0.046823694836348295), (14, 0.04789772164076567), (4, 0.04852241184562445), (2, 0.054577403236180544), (52, 0.05761870555579662), (3, 0.05784992640838027), (13, 0.05914428737014532), (11, 0.05970003409311175), (17, 0.06132525345310569), (0, 0.06337464740499854), (1, 0.06593215931206942), (8, 0.07466361671686172), (10, 0.08082299586385489), (16, 0.08527506329119205), (12, 0.09039537515491247), (5, 0.10671143885701895), (36, 0.3776939623057842), (18, 0.5117432922124863), (53, 2.1844322979450226)]
computing accuracy for after removing block 38 . block score: 0.02478079590946436
removed block 38 current accuracy 0.4658 loss from initial  0.48560000000000003
training start
training epoch 0 val accuracy 0.826 topk_dict {'top1': 0.826} is_best True lr [0.1]
training epoch 1 val accuracy 0.8562 topk_dict {'top1': 0.8562} is_best True lr [0.1]
training epoch 2 val accuracy 0.8782 topk_dict {'top1': 0.8782} is_best True lr [0.1]
training epoch 3 val accuracy 0.87 topk_dict {'top1': 0.87} is_best False lr [0.1]
training epoch 4 val accuracy 0.861 topk_dict {'top1': 0.861} is_best False lr [0.1]
training epoch 5 val accuracy 0.8822 topk_dict {'top1': 0.8822} is_best True lr [0.1]
training epoch 6 val accuracy 0.8838 topk_dict {'top1': 0.8838} is_best True lr [0.1]
training epoch 7 val accuracy 0.8968 topk_dict {'top1': 0.8968} is_best True lr [0.1]
training epoch 8 val accuracy 0.8886 topk_dict {'top1': 0.8886} is_best False lr [0.1]
training epoch 9 val accuracy 0.8748 topk_dict {'top1': 0.8748} is_best False lr [0.1]
training epoch 10 val accuracy 0.8832 topk_dict {'top1': 0.8832} is_best False lr [0.1]
training epoch 11 val accuracy 0.8978 topk_dict {'top1': 0.8978} is_best True lr [0.1]
training epoch 12 val accuracy 0.8716 topk_dict {'top1': 0.8716} is_best False lr [0.1]
training epoch 13 val accuracy 0.875 topk_dict {'top1': 0.875} is_best False lr [0.1]
training epoch 14 val accuracy 0.9054 topk_dict {'top1': 0.9054} is_best True lr [0.1]
training epoch 15 val accuracy 0.8756 topk_dict {'top1': 0.8756} is_best False lr [0.1]
training epoch 16 val accuracy 0.8854 topk_dict {'top1': 0.8854} is_best False lr [0.1]
training epoch 17 val accuracy 0.8262 topk_dict {'top1': 0.8262} is_best False lr [0.1]
training epoch 18 val accuracy 0.8998 topk_dict {'top1': 0.8998} is_best False lr [0.1]
training epoch 19 val accuracy 0.88 topk_dict {'top1': 0.88} is_best False lr [0.1]
training epoch 20 val accuracy 0.9008 topk_dict {'top1': 0.9008} is_best False lr [0.1]
training epoch 21 val accuracy 0.871 topk_dict {'top1': 0.871} is_best False lr [0.1]
training epoch 22 val accuracy 0.8826 topk_dict {'top1': 0.8826} is_best False lr [0.1]
training epoch 23 val accuracy 0.9028 topk_dict {'top1': 0.9028} is_best False lr [0.1]
training epoch 24 val accuracy 0.8958 topk_dict {'top1': 0.8958} is_best False lr [0.1]
training epoch 25 val accuracy 0.8976 topk_dict {'top1': 0.8976} is_best False lr [0.1]
training epoch 26 val accuracy 0.896 topk_dict {'top1': 0.896} is_best False lr [0.1]
training epoch 27 val accuracy 0.9042 topk_dict {'top1': 0.9042} is_best False lr [0.1]
training epoch 28 val accuracy 0.889 topk_dict {'top1': 0.889} is_best False lr [0.1]
training epoch 29 val accuracy 0.9026 topk_dict {'top1': 0.9026} is_best False lr [0.1]
training epoch 30 val accuracy 0.8652 topk_dict {'top1': 0.8652} is_best False lr [0.1]
training epoch 31 val accuracy 0.8582 topk_dict {'top1': 0.8582} is_best False lr [0.1]
training epoch 32 val accuracy 0.8936 topk_dict {'top1': 0.8936} is_best False lr [0.1]
training epoch 33 val accuracy 0.9084 topk_dict {'top1': 0.9084} is_best True lr [0.1]
training epoch 34 val accuracy 0.9062 topk_dict {'top1': 0.9062} is_best False lr [0.1]
training epoch 35 val accuracy 0.9054 topk_dict {'top1': 0.9054} is_best False lr [0.1]
training epoch 36 val accuracy 0.9032 topk_dict {'top1': 0.9032} is_best False lr [0.1]
training epoch 37 val accuracy 0.8966 topk_dict {'top1': 0.8966} is_best False lr [0.1]
training epoch 38 val accuracy 0.9014 topk_dict {'top1': 0.9014} is_best False lr [0.1]
training epoch 39 val accuracy 0.8876 topk_dict {'top1': 0.8876} is_best False lr [0.1]
training epoch 40 val accuracy 0.903 topk_dict {'top1': 0.903} is_best False lr [0.1]
training epoch 41 val accuracy 0.9148 topk_dict {'top1': 0.9148} is_best True lr [0.1]
training epoch 42 val accuracy 0.8918 topk_dict {'top1': 0.8918} is_best False lr [0.1]
training epoch 43 val accuracy 0.8972 topk_dict {'top1': 0.8972} is_best False lr [0.1]
training epoch 44 val accuracy 0.8696 topk_dict {'top1': 0.8696} is_best False lr [0.1]
training epoch 45 val accuracy 0.9042 topk_dict {'top1': 0.9042} is_best False lr [0.1]
training epoch 46 val accuracy 0.9078 topk_dict {'top1': 0.9078} is_best False lr [0.1]
training epoch 47 val accuracy 0.8838 topk_dict {'top1': 0.8838} is_best False lr [0.1]
training epoch 48 val accuracy 0.8978 topk_dict {'top1': 0.8978} is_best False lr [0.1]
training epoch 49 val accuracy 0.8956 topk_dict {'top1': 0.8956} is_best False lr [0.1]
training epoch 50 val accuracy 0.897 topk_dict {'top1': 0.897} is_best False lr [0.1]
training epoch 51 val accuracy 0.9054 topk_dict {'top1': 0.9054} is_best False lr [0.1]
training epoch 52 val accuracy 0.8832 topk_dict {'top1': 0.8832} is_best False lr [0.1]
training epoch 53 val accuracy 0.883 topk_dict {'top1': 0.883} is_best False lr [0.1]
training epoch 54 val accuracy 0.885 topk_dict {'top1': 0.885} is_best False lr [0.1]
training epoch 55 val accuracy 0.8844 topk_dict {'top1': 0.8844} is_best False lr [0.1]
training epoch 56 val accuracy 0.8914 topk_dict {'top1': 0.8914} is_best False lr [0.1]
training epoch 57 val accuracy 0.851 topk_dict {'top1': 0.851} is_best False lr [0.1]
training epoch 58 val accuracy 0.897 topk_dict {'top1': 0.897} is_best False lr [0.1]
training epoch 59 val accuracy 0.8886 topk_dict {'top1': 0.8886} is_best False lr [0.1]
training epoch 60 val accuracy 0.8576 topk_dict {'top1': 0.8576} is_best False lr [0.1]
training epoch 61 val accuracy 0.8932 topk_dict {'top1': 0.8932} is_best False lr [0.1]
training epoch 62 val accuracy 0.8954 topk_dict {'top1': 0.8954} is_best False lr [0.1]
training epoch 63 val accuracy 0.8976 topk_dict {'top1': 0.8976} is_best False lr [0.1]
training epoch 64 val accuracy 0.8808 topk_dict {'top1': 0.8808} is_best False lr [0.1]
training epoch 65 val accuracy 0.9088 topk_dict {'top1': 0.9088} is_best False lr [0.1]
training epoch 66 val accuracy 0.8812 topk_dict {'top1': 0.8812} is_best False lr [0.1]
training epoch 67 val accuracy 0.896 topk_dict {'top1': 0.896} is_best False lr [0.1]
training epoch 68 val accuracy 0.8896 topk_dict {'top1': 0.8896} is_best False lr [0.1]
training epoch 69 val accuracy 0.8958 topk_dict {'top1': 0.8958} is_best False lr [0.1]
training epoch 70 val accuracy 0.8884 topk_dict {'top1': 0.8884} is_best False lr [0.1]
training epoch 71 val accuracy 0.9028 topk_dict {'top1': 0.9028} is_best False lr [0.1]
training epoch 72 val accuracy 0.893 topk_dict {'top1': 0.893} is_best False lr [0.1]
training epoch 73 val accuracy 0.8748 topk_dict {'top1': 0.8748} is_best False lr [0.1]
training epoch 74 val accuracy 0.893 topk_dict {'top1': 0.893} is_best False lr [0.1]
training epoch 75 val accuracy 0.9012 topk_dict {'top1': 0.9012} is_best False lr [0.1]
training epoch 76 val accuracy 0.905 topk_dict {'top1': 0.905} is_best False lr [0.1]
training epoch 77 val accuracy 0.9068 topk_dict {'top1': 0.9068} is_best False lr [0.1]
training epoch 78 val accuracy 0.9048 topk_dict {'top1': 0.9048} is_best False lr [0.1]
training epoch 79 val accuracy 0.8748 topk_dict {'top1': 0.8748} is_best False lr [0.1]
training epoch 80 val accuracy 0.8922 topk_dict {'top1': 0.8922} is_best False lr [0.1]
training epoch 81 val accuracy 0.8796 topk_dict {'top1': 0.8796} is_best False lr [0.1]
training epoch 82 val accuracy 0.8942 topk_dict {'top1': 0.8942} is_best False lr [0.1]
training epoch 83 val accuracy 0.8922 topk_dict {'top1': 0.8922} is_best False lr [0.1]
training epoch 84 val accuracy 0.8844 topk_dict {'top1': 0.8844} is_best False lr [0.1]
training epoch 85 val accuracy 0.9086 topk_dict {'top1': 0.9086} is_best False lr [0.1]
training epoch 86 val accuracy 0.905 topk_dict {'top1': 0.905} is_best False lr [0.1]
training epoch 87 val accuracy 0.9056 topk_dict {'top1': 0.9056} is_best False lr [0.1]
training epoch 88 val accuracy 0.8812 topk_dict {'top1': 0.8812} is_best False lr [0.1]
training epoch 89 val accuracy 0.8772 topk_dict {'top1': 0.8772} is_best False lr [0.1]
training epoch 90 val accuracy 0.8606 topk_dict {'top1': 0.8606} is_best False lr [0.1]
training epoch 91 val accuracy 0.888 topk_dict {'top1': 0.888} is_best False lr [0.1]
training epoch 92 val accuracy 0.8758 topk_dict {'top1': 0.8758} is_best False lr [0.1]
training epoch 93 val accuracy 0.8896 topk_dict {'top1': 0.8896} is_best False lr [0.1]
training epoch 94 val accuracy 0.8972 topk_dict {'top1': 0.8972} is_best False lr [0.1]
training epoch 95 val accuracy 0.9056 topk_dict {'top1': 0.9056} is_best False lr [0.1]
training epoch 96 val accuracy 0.898 topk_dict {'top1': 0.898} is_best False lr [0.1]
training epoch 97 val accuracy 0.8766 topk_dict {'top1': 0.8766} is_best False lr [0.1]
training epoch 98 val accuracy 0.9052 topk_dict {'top1': 0.9052} is_best False lr [0.1]
training epoch 99 val accuracy 0.9002 topk_dict {'top1': 0.9002} is_best False lr [0.1]
training epoch 100 val accuracy 0.8934 topk_dict {'top1': 0.8934} is_best False lr [0.1]
training epoch 101 val accuracy 0.9 topk_dict {'top1': 0.9} is_best False lr [0.1]
training epoch 102 val accuracy 0.9008 topk_dict {'top1': 0.9008} is_best False lr [0.1]
training epoch 103 val accuracy 0.9028 topk_dict {'top1': 0.9028} is_best False lr [0.1]
training epoch 104 val accuracy 0.8722 topk_dict {'top1': 0.8722} is_best False lr [0.1]
training epoch 105 val accuracy 0.8996 topk_dict {'top1': 0.8996} is_best False lr [0.1]
training epoch 106 val accuracy 0.9104 topk_dict {'top1': 0.9104} is_best False lr [0.1]
training epoch 107 val accuracy 0.8894 topk_dict {'top1': 0.8894} is_best False lr [0.1]
training epoch 108 val accuracy 0.8864 topk_dict {'top1': 0.8864} is_best False lr [0.1]
training epoch 109 val accuracy 0.878 topk_dict {'top1': 0.878} is_best False lr [0.1]
training epoch 110 val accuracy 0.897 topk_dict {'top1': 0.897} is_best False lr [0.1]
training epoch 111 val accuracy 0.8952 topk_dict {'top1': 0.8952} is_best False lr [0.1]
training epoch 112 val accuracy 0.8666 topk_dict {'top1': 0.8666} is_best False lr [0.1]
training epoch 113 val accuracy 0.866 topk_dict {'top1': 0.866} is_best False lr [0.1]
training epoch 114 val accuracy 0.8896 topk_dict {'top1': 0.8896} is_best False lr [0.1]
training epoch 115 val accuracy 0.8948 topk_dict {'top1': 0.8948} is_best False lr [0.1]
training epoch 116 val accuracy 0.903 topk_dict {'top1': 0.903} is_best False lr [0.1]
training epoch 117 val accuracy 0.8904 topk_dict {'top1': 0.8904} is_best False lr [0.1]
training epoch 118 val accuracy 0.8928 topk_dict {'top1': 0.8928} is_best False lr [0.1]
training epoch 119 val accuracy 0.901 topk_dict {'top1': 0.901} is_best False lr [0.1]
training epoch 120 val accuracy 0.8896 topk_dict {'top1': 0.8896} is_best False lr [0.1]
training epoch 121 val accuracy 0.8978 topk_dict {'top1': 0.8978} is_best False lr [0.1]
training epoch 122 val accuracy 0.8774 topk_dict {'top1': 0.8774} is_best False lr [0.1]
training epoch 123 val accuracy 0.8952 topk_dict {'top1': 0.8952} is_best False lr [0.1]
training epoch 124 val accuracy 0.9118 topk_dict {'top1': 0.9118} is_best False lr [0.1]
training epoch 125 val accuracy 0.898 topk_dict {'top1': 0.898} is_best False lr [0.1]
training epoch 126 val accuracy 0.8886 topk_dict {'top1': 0.8886} is_best False lr [0.1]
training epoch 127 val accuracy 0.8918 topk_dict {'top1': 0.8918} is_best False lr [0.1]
training epoch 128 val accuracy 0.894 topk_dict {'top1': 0.894} is_best False lr [0.1]
training epoch 129 val accuracy 0.903 topk_dict {'top1': 0.903} is_best False lr [0.1]
training epoch 130 val accuracy 0.8826 topk_dict {'top1': 0.8826} is_best False lr [0.1]
training epoch 131 val accuracy 0.9044 topk_dict {'top1': 0.9044} is_best False lr [0.1]
training epoch 132 val accuracy 0.8942 topk_dict {'top1': 0.8942} is_best False lr [0.1]
training epoch 133 val accuracy 0.8936 topk_dict {'top1': 0.8936} is_best False lr [0.1]
training epoch 134 val accuracy 0.9034 topk_dict {'top1': 0.9034} is_best False lr [0.1]
training epoch 135 val accuracy 0.9 topk_dict {'top1': 0.9} is_best False lr [0.1]
training epoch 136 val accuracy 0.8886 topk_dict {'top1': 0.8886} is_best False lr [0.1]
training epoch 137 val accuracy 0.8816 topk_dict {'top1': 0.8816} is_best False lr [0.1]
training epoch 138 val accuracy 0.8884 topk_dict {'top1': 0.8884} is_best False lr [0.1]
training epoch 139 val accuracy 0.9146 topk_dict {'top1': 0.9146} is_best False lr [0.1]
training epoch 140 val accuracy 0.891 topk_dict {'top1': 0.891} is_best False lr [0.1]
training epoch 141 val accuracy 0.898 topk_dict {'top1': 0.898} is_best False lr [0.1]
training epoch 142 val accuracy 0.9024 topk_dict {'top1': 0.9024} is_best False lr [0.1]
training epoch 143 val accuracy 0.8952 topk_dict {'top1': 0.8952} is_best False lr [0.1]
training epoch 144 val accuracy 0.898 topk_dict {'top1': 0.898} is_best False lr [0.1]
training epoch 145 val accuracy 0.8898 topk_dict {'top1': 0.8898} is_best False lr [0.1]
training epoch 146 val accuracy 0.8662 topk_dict {'top1': 0.8662} is_best False lr [0.1]
training epoch 147 val accuracy 0.8964 topk_dict {'top1': 0.8964} is_best False lr [0.1]
training epoch 148 val accuracy 0.903 topk_dict {'top1': 0.903} is_best False lr [0.1]
training epoch 149 val accuracy 0.9028 topk_dict {'top1': 0.9028} is_best False lr [0.1]
training epoch 150 val accuracy 0.896 topk_dict {'top1': 0.896} is_best False lr [0.1]
training epoch 151 val accuracy 0.8844 topk_dict {'top1': 0.8844} is_best False lr [0.1]
training epoch 152 val accuracy 0.8734 topk_dict {'top1': 0.8734} is_best False lr [0.1]
training epoch 153 val accuracy 0.9096 topk_dict {'top1': 0.9096} is_best False lr [0.1]
training epoch 154 val accuracy 0.8902 topk_dict {'top1': 0.8902} is_best False lr [0.1]
training epoch 155 val accuracy 0.8992 topk_dict {'top1': 0.8992} is_best False lr [0.1]
training epoch 156 val accuracy 0.897 topk_dict {'top1': 0.897} is_best False lr [0.1]
training epoch 157 val accuracy 0.8866 topk_dict {'top1': 0.8866} is_best False lr [0.1]
training epoch 158 val accuracy 0.9002 topk_dict {'top1': 0.9002} is_best False lr [0.1]
training epoch 159 val accuracy 0.895 topk_dict {'top1': 0.895} is_best False lr [0.1]
training epoch 160 val accuracy 0.9028 topk_dict {'top1': 0.9028} is_best False lr [0.1]
training epoch 161 val accuracy 0.8916 topk_dict {'top1': 0.8916} is_best False lr [0.1]
training epoch 162 val accuracy 0.8926 topk_dict {'top1': 0.8926} is_best False lr [0.1]
training epoch 163 val accuracy 0.9092 topk_dict {'top1': 0.9092} is_best False lr [0.1]
training epoch 164 val accuracy 0.9008 topk_dict {'top1': 0.9008} is_best False lr [0.1]
training epoch 165 val accuracy 0.8978 topk_dict {'top1': 0.8978} is_best False lr [0.1]
training epoch 166 val accuracy 0.876 topk_dict {'top1': 0.876} is_best False lr [0.1]
training epoch 167 val accuracy 0.9012 topk_dict {'top1': 0.9012} is_best False lr [0.1]
training epoch 168 val accuracy 0.9042 topk_dict {'top1': 0.9042} is_best False lr [0.1]
training epoch 169 val accuracy 0.9034 topk_dict {'top1': 0.9034} is_best False lr [0.1]
training epoch 170 val accuracy 0.9068 topk_dict {'top1': 0.9068} is_best False lr [0.1]
training epoch 171 val accuracy 0.8838 topk_dict {'top1': 0.8838} is_best False lr [0.1]
training epoch 172 val accuracy 0.8942 topk_dict {'top1': 0.8942} is_best False lr [0.1]
training epoch 173 val accuracy 0.907 topk_dict {'top1': 0.907} is_best False lr [0.1]
training epoch 174 val accuracy 0.9028 topk_dict {'top1': 0.9028} is_best False lr [0.1]
training epoch 175 val accuracy 0.9086 topk_dict {'top1': 0.9086} is_best False lr [0.1]
training epoch 176 val accuracy 0.8976 topk_dict {'top1': 0.8976} is_best False lr [0.1]
training epoch 177 val accuracy 0.8948 topk_dict {'top1': 0.8948} is_best False lr [0.1]
training epoch 178 val accuracy 0.898 topk_dict {'top1': 0.898} is_best False lr [0.1]
training epoch 179 val accuracy 0.8928 topk_dict {'top1': 0.8928} is_best False lr [0.1]
training epoch 180 val accuracy 0.8984 topk_dict {'top1': 0.8984} is_best False lr [0.1]
training epoch 181 val accuracy 0.9114 topk_dict {'top1': 0.9114} is_best False lr [0.1]
training epoch 182 val accuracy 0.894 topk_dict {'top1': 0.894} is_best False lr [0.1]
training epoch 183 val accuracy 0.8994 topk_dict {'top1': 0.8994} is_best False lr [0.1]
training epoch 184 val accuracy 0.8864 topk_dict {'top1': 0.8864} is_best False lr [0.1]
training epoch 185 val accuracy 0.8916 topk_dict {'top1': 0.8916} is_best False lr [0.1]
training epoch 186 val accuracy 0.9036 topk_dict {'top1': 0.9036} is_best False lr [0.1]
training epoch 187 val accuracy 0.881 topk_dict {'top1': 0.881} is_best False lr [0.1]
training epoch 188 val accuracy 0.874 topk_dict {'top1': 0.874} is_best False lr [0.1]
training epoch 189 val accuracy 0.8982 topk_dict {'top1': 0.8982} is_best False lr [0.1]
training epoch 190 val accuracy 0.8862 topk_dict {'top1': 0.8862} is_best False lr [0.1]
training epoch 191 val accuracy 0.8976 topk_dict {'top1': 0.8976} is_best False lr [0.1]
training epoch 192 val accuracy 0.8918 topk_dict {'top1': 0.8918} is_best False lr [0.1]
training epoch 193 val accuracy 0.8932 topk_dict {'top1': 0.8932} is_best False lr [0.1]
training epoch 194 val accuracy 0.907 topk_dict {'top1': 0.907} is_best False lr [0.1]
training epoch 195 val accuracy 0.8982 topk_dict {'top1': 0.8982} is_best False lr [0.1]
training epoch 196 val accuracy 0.8984 topk_dict {'top1': 0.8984} is_best False lr [0.1]
training epoch 197 val accuracy 0.9048 topk_dict {'top1': 0.9048} is_best False lr [0.1]
training epoch 198 val accuracy 0.9048 topk_dict {'top1': 0.9048} is_best False lr [0.1]
training epoch 199 val accuracy 0.9034 topk_dict {'top1': 0.9034} is_best False lr [0.1]
training epoch 200 val accuracy 0.9018 topk_dict {'top1': 0.9018} is_best False lr [0.1]
training epoch 201 val accuracy 0.8998 topk_dict {'top1': 0.8998} is_best False lr [0.1]
training epoch 202 val accuracy 0.8822 topk_dict {'top1': 0.8822} is_best False lr [0.1]
training epoch 203 val accuracy 0.9094 topk_dict {'top1': 0.9094} is_best False lr [0.1]
training epoch 204 val accuracy 0.9098 topk_dict {'top1': 0.9098} is_best False lr [0.1]
training epoch 205 val accuracy 0.9042 topk_dict {'top1': 0.9042} is_best False lr [0.1]
training epoch 206 val accuracy 0.9064 topk_dict {'top1': 0.9064} is_best False lr [0.1]
training epoch 207 val accuracy 0.8968 topk_dict {'top1': 0.8968} is_best False lr [0.1]
training epoch 208 val accuracy 0.8888 topk_dict {'top1': 0.8888} is_best False lr [0.1]
training epoch 209 val accuracy 0.8986 topk_dict {'top1': 0.8986} is_best False lr [0.1]
training epoch 210 val accuracy 0.8922 topk_dict {'top1': 0.8922} is_best False lr [0.1]
training epoch 211 val accuracy 0.9036 topk_dict {'top1': 0.9036} is_best False lr [0.1]
training epoch 212 val accuracy 0.89 topk_dict {'top1': 0.89} is_best False lr [0.1]
training epoch 213 val accuracy 0.8908 topk_dict {'top1': 0.8908} is_best False lr [0.1]
training epoch 214 val accuracy 0.8996 topk_dict {'top1': 0.8996} is_best False lr [0.1]
training epoch 215 val accuracy 0.8846 topk_dict {'top1': 0.8846} is_best False lr [0.1]
training epoch 216 val accuracy 0.8806 topk_dict {'top1': 0.8806} is_best False lr [0.1]
training epoch 217 val accuracy 0.8788 topk_dict {'top1': 0.8788} is_best False lr [0.1]
training epoch 218 val accuracy 0.9032 topk_dict {'top1': 0.9032} is_best False lr [0.1]
training epoch 219 val accuracy 0.8928 topk_dict {'top1': 0.8928} is_best False lr [0.1]
training epoch 220 val accuracy 0.8628 topk_dict {'top1': 0.8628} is_best False lr [0.1]
training epoch 221 val accuracy 0.8942 topk_dict {'top1': 0.8942} is_best False lr [0.1]
training epoch 222 val accuracy 0.8958 topk_dict {'top1': 0.8958} is_best False lr [0.1]
training epoch 223 val accuracy 0.8996 topk_dict {'top1': 0.8996} is_best False lr [0.1]
training epoch 224 val accuracy 0.8854 topk_dict {'top1': 0.8854} is_best False lr [0.1]
training epoch 225 val accuracy 0.8862 topk_dict {'top1': 0.8862} is_best False lr [0.1]
training epoch 226 val accuracy 0.8858 topk_dict {'top1': 0.8858} is_best False lr [0.1]
training epoch 227 val accuracy 0.8936 topk_dict {'top1': 0.8936} is_best False lr [0.1]
training epoch 228 val accuracy 0.8918 topk_dict {'top1': 0.8918} is_best False lr [0.1]
training epoch 229 val accuracy 0.8894 topk_dict {'top1': 0.8894} is_best False lr [0.1]
training epoch 230 val accuracy 0.8852 topk_dict {'top1': 0.8852} is_best False lr [0.1]
training epoch 231 val accuracy 0.9026 topk_dict {'top1': 0.9026} is_best False lr [0.1]
training epoch 232 val accuracy 0.912 topk_dict {'top1': 0.912} is_best False lr [0.1]
training epoch 233 val accuracy 0.888 topk_dict {'top1': 0.888} is_best False lr [0.1]
training epoch 234 val accuracy 0.8866 topk_dict {'top1': 0.8866} is_best False lr [0.1]
training epoch 235 val accuracy 0.9012 topk_dict {'top1': 0.9012} is_best False lr [0.1]
training epoch 236 val accuracy 0.8832 topk_dict {'top1': 0.8832} is_best False lr [0.1]
training epoch 237 val accuracy 0.902 topk_dict {'top1': 0.902} is_best False lr [0.1]
training epoch 238 val accuracy 0.9024 topk_dict {'top1': 0.9024} is_best False lr [0.1]
training epoch 239 val accuracy 0.9012 topk_dict {'top1': 0.9012} is_best False lr [0.1]
training epoch 240 val accuracy 0.8992 topk_dict {'top1': 0.8992} is_best False lr [0.1]
training epoch 241 val accuracy 0.9062 topk_dict {'top1': 0.9062} is_best False lr [0.1]
training epoch 242 val accuracy 0.8952 topk_dict {'top1': 0.8952} is_best False lr [0.1]
training epoch 243 val accuracy 0.8818 topk_dict {'top1': 0.8818} is_best False lr [0.1]
training epoch 244 val accuracy 0.9002 topk_dict {'top1': 0.9002} is_best False lr [0.1]
training epoch 245 val accuracy 0.8932 topk_dict {'top1': 0.8932} is_best False lr [0.1]
training epoch 246 val accuracy 0.9002 topk_dict {'top1': 0.9002} is_best False lr [0.1]
training epoch 247 val accuracy 0.9002 topk_dict {'top1': 0.9002} is_best False lr [0.1]
training epoch 248 val accuracy 0.8954 topk_dict {'top1': 0.8954} is_best False lr [0.1]
training epoch 249 val accuracy 0.8814 topk_dict {'top1': 0.8814} is_best False lr [0.1]
training epoch 250 val accuracy 0.9372 topk_dict {'top1': 0.9372} is_best True lr [0.010000000000000002]
training epoch 251 val accuracy 0.9418 topk_dict {'top1': 0.9418} is_best True lr [0.010000000000000002]
training epoch 252 val accuracy 0.9422 topk_dict {'top1': 0.9422} is_best True lr [0.010000000000000002]
training epoch 253 val accuracy 0.943 topk_dict {'top1': 0.943} is_best True lr [0.010000000000000002]
training epoch 254 val accuracy 0.9424 topk_dict {'top1': 0.9424} is_best False lr [0.010000000000000002]
training epoch 255 val accuracy 0.9442 topk_dict {'top1': 0.9442} is_best True lr [0.010000000000000002]
training epoch 256 val accuracy 0.9434 topk_dict {'top1': 0.9434} is_best False lr [0.010000000000000002]
training epoch 257 val accuracy 0.9436 topk_dict {'top1': 0.9436} is_best False lr [0.010000000000000002]
training epoch 258 val accuracy 0.9434 topk_dict {'top1': 0.9434} is_best False lr [0.010000000000000002]
training epoch 259 val accuracy 0.9434 topk_dict {'top1': 0.9434} is_best False lr [0.010000000000000002]
training epoch 260 val accuracy 0.943 topk_dict {'top1': 0.943} is_best False lr [0.010000000000000002]
training epoch 261 val accuracy 0.943 topk_dict {'top1': 0.943} is_best False lr [0.010000000000000002]
training epoch 262 val accuracy 0.941 topk_dict {'top1': 0.941} is_best False lr [0.010000000000000002]
training epoch 263 val accuracy 0.9428 topk_dict {'top1': 0.9428} is_best False lr [0.010000000000000002]
training epoch 264 val accuracy 0.9406 topk_dict {'top1': 0.9406} is_best False lr [0.010000000000000002]
training epoch 265 val accuracy 0.9408 topk_dict {'top1': 0.9408} is_best False lr [0.010000000000000002]
training epoch 266 val accuracy 0.9418 topk_dict {'top1': 0.9418} is_best False lr [0.010000000000000002]
training epoch 267 val accuracy 0.9424 topk_dict {'top1': 0.9424} is_best False lr [0.010000000000000002]
training epoch 268 val accuracy 0.9422 topk_dict {'top1': 0.9422} is_best False lr [0.010000000000000002]
training epoch 269 val accuracy 0.9428 topk_dict {'top1': 0.9428} is_best False lr [0.010000000000000002]
training epoch 270 val accuracy 0.9422 topk_dict {'top1': 0.9422} is_best False lr [0.010000000000000002]
training epoch 271 val accuracy 0.9392 topk_dict {'top1': 0.9392} is_best False lr [0.010000000000000002]
training epoch 272 val accuracy 0.9418 topk_dict {'top1': 0.9418} is_best False lr [0.010000000000000002]
training epoch 273 val accuracy 0.9408 topk_dict {'top1': 0.9408} is_best False lr [0.010000000000000002]
training epoch 274 val accuracy 0.9422 topk_dict {'top1': 0.9422} is_best False lr [0.010000000000000002]
training epoch 275 val accuracy 0.9414 topk_dict {'top1': 0.9414} is_best False lr [0.010000000000000002]
training epoch 276 val accuracy 0.9402 topk_dict {'top1': 0.9402} is_best False lr [0.010000000000000002]
training epoch 277 val accuracy 0.9422 topk_dict {'top1': 0.9422} is_best False lr [0.010000000000000002]
training epoch 278 val accuracy 0.9424 topk_dict {'top1': 0.9424} is_best False lr [0.010000000000000002]
training epoch 279 val accuracy 0.9426 topk_dict {'top1': 0.9426} is_best False lr [0.010000000000000002]
training epoch 280 val accuracy 0.9404 topk_dict {'top1': 0.9404} is_best False lr [0.010000000000000002]
training epoch 281 val accuracy 0.9436 topk_dict {'top1': 0.9436} is_best False lr [0.010000000000000002]
training epoch 282 val accuracy 0.945 topk_dict {'top1': 0.945} is_best True lr [0.010000000000000002]
training epoch 283 val accuracy 0.9434 topk_dict {'top1': 0.9434} is_best False lr [0.010000000000000002]
training epoch 284 val accuracy 0.9426 topk_dict {'top1': 0.9426} is_best False lr [0.010000000000000002]
training epoch 285 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.010000000000000002]
training epoch 286 val accuracy 0.9422 topk_dict {'top1': 0.9422} is_best False lr [0.010000000000000002]
training epoch 287 val accuracy 0.9402 topk_dict {'top1': 0.9402} is_best False lr [0.010000000000000002]
training epoch 288 val accuracy 0.9442 topk_dict {'top1': 0.9442} is_best False lr [0.010000000000000002]
training epoch 289 val accuracy 0.944 topk_dict {'top1': 0.944} is_best False lr [0.010000000000000002]
training epoch 290 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.010000000000000002]
training epoch 291 val accuracy 0.9436 topk_dict {'top1': 0.9436} is_best False lr [0.010000000000000002]
training epoch 292 val accuracy 0.943 topk_dict {'top1': 0.943} is_best False lr [0.010000000000000002]
training epoch 293 val accuracy 0.9426 topk_dict {'top1': 0.9426} is_best False lr [0.010000000000000002]
training epoch 294 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.010000000000000002]
training epoch 295 val accuracy 0.9434 topk_dict {'top1': 0.9434} is_best False lr [0.010000000000000002]
training epoch 296 val accuracy 0.9434 topk_dict {'top1': 0.9434} is_best False lr [0.010000000000000002]
training epoch 297 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.010000000000000002]
training epoch 298 val accuracy 0.9438 topk_dict {'top1': 0.9438} is_best False lr [0.010000000000000002]
training epoch 299 val accuracy 0.9446 topk_dict {'top1': 0.9446} is_best False lr [0.010000000000000002]
training epoch 300 val accuracy 0.9426 topk_dict {'top1': 0.9426} is_best False lr [0.010000000000000002]
training epoch 301 val accuracy 0.9446 topk_dict {'top1': 0.9446} is_best False lr [0.010000000000000002]
training epoch 302 val accuracy 0.9446 topk_dict {'top1': 0.9446} is_best False lr [0.010000000000000002]
training epoch 303 val accuracy 0.9416 topk_dict {'top1': 0.9416} is_best False lr [0.010000000000000002]
training epoch 304 val accuracy 0.943 topk_dict {'top1': 0.943} is_best False lr [0.010000000000000002]
training epoch 305 val accuracy 0.9438 topk_dict {'top1': 0.9438} is_best False lr [0.010000000000000002]
training epoch 306 val accuracy 0.9426 topk_dict {'top1': 0.9426} is_best False lr [0.010000000000000002]
training epoch 307 val accuracy 0.9428 topk_dict {'top1': 0.9428} is_best False lr [0.010000000000000002]
training epoch 308 val accuracy 0.9434 topk_dict {'top1': 0.9434} is_best False lr [0.010000000000000002]
training epoch 309 val accuracy 0.944 topk_dict {'top1': 0.944} is_best False lr [0.010000000000000002]
training epoch 310 val accuracy 0.9436 topk_dict {'top1': 0.9436} is_best False lr [0.010000000000000002]
training epoch 311 val accuracy 0.9414 topk_dict {'top1': 0.9414} is_best False lr [0.010000000000000002]
training epoch 312 val accuracy 0.9436 topk_dict {'top1': 0.9436} is_best False lr [0.010000000000000002]
training epoch 313 val accuracy 0.9436 topk_dict {'top1': 0.9436} is_best False lr [0.010000000000000002]
training epoch 314 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.010000000000000002]
training epoch 315 val accuracy 0.9424 topk_dict {'top1': 0.9424} is_best False lr [0.010000000000000002]
training epoch 316 val accuracy 0.9412 topk_dict {'top1': 0.9412} is_best False lr [0.010000000000000002]
training epoch 317 val accuracy 0.9424 topk_dict {'top1': 0.9424} is_best False lr [0.010000000000000002]
training epoch 318 val accuracy 0.9428 topk_dict {'top1': 0.9428} is_best False lr [0.010000000000000002]
training epoch 319 val accuracy 0.9424 topk_dict {'top1': 0.9424} is_best False lr [0.010000000000000002]
training epoch 320 val accuracy 0.9426 topk_dict {'top1': 0.9426} is_best False lr [0.010000000000000002]
training epoch 321 val accuracy 0.9428 topk_dict {'top1': 0.9428} is_best False lr [0.010000000000000002]
training epoch 322 val accuracy 0.9432 topk_dict {'top1': 0.9432} is_best False lr [0.010000000000000002]
training epoch 323 val accuracy 0.9424 topk_dict {'top1': 0.9424} is_best False lr [0.010000000000000002]
training epoch 324 val accuracy 0.943 topk_dict {'top1': 0.943} is_best False lr [0.010000000000000002]
training epoch 325 val accuracy 0.9432 topk_dict {'top1': 0.9432} is_best False lr [0.010000000000000002]
training epoch 326 val accuracy 0.942 topk_dict {'top1': 0.942} is_best False lr [0.010000000000000002]
training epoch 327 val accuracy 0.944 topk_dict {'top1': 0.944} is_best False lr [0.010000000000000002]
training epoch 328 val accuracy 0.9428 topk_dict {'top1': 0.9428} is_best False lr [0.010000000000000002]
training epoch 329 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best True lr [0.010000000000000002]
training epoch 330 val accuracy 0.9424 topk_dict {'top1': 0.9424} is_best False lr [0.010000000000000002]
training epoch 331 val accuracy 0.9426 topk_dict {'top1': 0.9426} is_best False lr [0.010000000000000002]
training epoch 332 val accuracy 0.9414 topk_dict {'top1': 0.9414} is_best False lr [0.010000000000000002]
training epoch 333 val accuracy 0.9436 topk_dict {'top1': 0.9436} is_best False lr [0.010000000000000002]
training epoch 334 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.010000000000000002]
training epoch 335 val accuracy 0.943 topk_dict {'top1': 0.943} is_best False lr [0.010000000000000002]
training epoch 336 val accuracy 0.942 topk_dict {'top1': 0.942} is_best False lr [0.010000000000000002]
training epoch 337 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.010000000000000002]
training epoch 338 val accuracy 0.9442 topk_dict {'top1': 0.9442} is_best False lr [0.010000000000000002]
training epoch 339 val accuracy 0.9416 topk_dict {'top1': 0.9416} is_best False lr [0.010000000000000002]
training epoch 340 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.010000000000000002]
training epoch 341 val accuracy 0.9432 topk_dict {'top1': 0.9432} is_best False lr [0.010000000000000002]
training epoch 342 val accuracy 0.9438 topk_dict {'top1': 0.9438} is_best False lr [0.010000000000000002]
training epoch 343 val accuracy 0.9422 topk_dict {'top1': 0.9422} is_best False lr [0.010000000000000002]
training epoch 344 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.010000000000000002]
training epoch 345 val accuracy 0.9416 topk_dict {'top1': 0.9416} is_best False lr [0.010000000000000002]
training epoch 346 val accuracy 0.9434 topk_dict {'top1': 0.9434} is_best False lr [0.010000000000000002]
training epoch 347 val accuracy 0.9424 topk_dict {'top1': 0.9424} is_best False lr [0.010000000000000002]
training epoch 348 val accuracy 0.9422 topk_dict {'top1': 0.9422} is_best False lr [0.010000000000000002]
training epoch 349 val accuracy 0.942 topk_dict {'top1': 0.942} is_best False lr [0.010000000000000002]
training epoch 350 val accuracy 0.9418 topk_dict {'top1': 0.9418} is_best False lr [0.010000000000000002]
training epoch 351 val accuracy 0.9422 topk_dict {'top1': 0.9422} is_best False lr [0.010000000000000002]
training epoch 352 val accuracy 0.941 topk_dict {'top1': 0.941} is_best False lr [0.010000000000000002]
training epoch 353 val accuracy 0.9416 topk_dict {'top1': 0.9416} is_best False lr [0.010000000000000002]
training epoch 354 val accuracy 0.9424 topk_dict {'top1': 0.9424} is_best False lr [0.010000000000000002]
training epoch 355 val accuracy 0.944 topk_dict {'top1': 0.944} is_best False lr [0.010000000000000002]
training epoch 356 val accuracy 0.9404 topk_dict {'top1': 0.9404} is_best False lr [0.010000000000000002]
training epoch 357 val accuracy 0.9416 topk_dict {'top1': 0.9416} is_best False lr [0.010000000000000002]
training epoch 358 val accuracy 0.9404 topk_dict {'top1': 0.9404} is_best False lr [0.010000000000000002]
training epoch 359 val accuracy 0.9422 topk_dict {'top1': 0.9422} is_best False lr [0.010000000000000002]
training epoch 360 val accuracy 0.943 topk_dict {'top1': 0.943} is_best False lr [0.010000000000000002]
training epoch 361 val accuracy 0.9442 topk_dict {'top1': 0.9442} is_best False lr [0.010000000000000002]
training epoch 362 val accuracy 0.942 topk_dict {'top1': 0.942} is_best False lr [0.010000000000000002]
training epoch 363 val accuracy 0.9418 topk_dict {'top1': 0.9418} is_best False lr [0.010000000000000002]
training epoch 364 val accuracy 0.9434 topk_dict {'top1': 0.9434} is_best False lr [0.010000000000000002]
training epoch 365 val accuracy 0.943 topk_dict {'top1': 0.943} is_best False lr [0.010000000000000002]
training epoch 366 val accuracy 0.9432 topk_dict {'top1': 0.9432} is_best False lr [0.010000000000000002]
training epoch 367 val accuracy 0.9428 topk_dict {'top1': 0.9428} is_best False lr [0.010000000000000002]
training epoch 368 val accuracy 0.9424 topk_dict {'top1': 0.9424} is_best False lr [0.010000000000000002]
training epoch 369 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.010000000000000002]
training epoch 370 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.010000000000000002]
training epoch 371 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.010000000000000002]
training epoch 372 val accuracy 0.942 topk_dict {'top1': 0.942} is_best False lr [0.010000000000000002]
training epoch 373 val accuracy 0.943 topk_dict {'top1': 0.943} is_best False lr [0.010000000000000002]
training epoch 374 val accuracy 0.9438 topk_dict {'top1': 0.9438} is_best False lr [0.010000000000000002]
training epoch 375 val accuracy 0.9442 topk_dict {'top1': 0.9442} is_best False lr [0.0010000000000000002]
training epoch 376 val accuracy 0.9438 topk_dict {'top1': 0.9438} is_best False lr [0.0010000000000000002]
training epoch 377 val accuracy 0.9452 topk_dict {'top1': 0.9452} is_best False lr [0.0010000000000000002]
training epoch 378 val accuracy 0.9452 topk_dict {'top1': 0.9452} is_best False lr [0.0010000000000000002]
training epoch 379 val accuracy 0.945 topk_dict {'top1': 0.945} is_best False lr [0.0010000000000000002]
training epoch 380 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 381 val accuracy 0.9454 topk_dict {'top1': 0.9454} is_best False lr [0.0010000000000000002]
training epoch 382 val accuracy 0.946 topk_dict {'top1': 0.946} is_best True lr [0.0010000000000000002]
training epoch 383 val accuracy 0.9454 topk_dict {'top1': 0.9454} is_best False lr [0.0010000000000000002]
training epoch 384 val accuracy 0.946 topk_dict {'top1': 0.946} is_best False lr [0.0010000000000000002]
training epoch 385 val accuracy 0.9462 topk_dict {'top1': 0.9462} is_best True lr [0.0010000000000000002]
training epoch 386 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 387 val accuracy 0.9462 topk_dict {'top1': 0.9462} is_best False lr [0.0010000000000000002]
training epoch 388 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 389 val accuracy 0.946 topk_dict {'top1': 0.946} is_best False lr [0.0010000000000000002]
training epoch 390 val accuracy 0.9462 topk_dict {'top1': 0.9462} is_best False lr [0.0010000000000000002]
training epoch 391 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 392 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.0010000000000000002]
training epoch 393 val accuracy 0.9436 topk_dict {'top1': 0.9436} is_best False lr [0.0010000000000000002]
training epoch 394 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 395 val accuracy 0.945 topk_dict {'top1': 0.945} is_best False lr [0.0010000000000000002]
training epoch 396 val accuracy 0.9454 topk_dict {'top1': 0.9454} is_best False lr [0.0010000000000000002]
training epoch 397 val accuracy 0.9464 topk_dict {'top1': 0.9464} is_best True lr [0.0010000000000000002]
training epoch 398 val accuracy 0.9464 topk_dict {'top1': 0.9464} is_best False lr [0.0010000000000000002]
training epoch 399 val accuracy 0.9464 topk_dict {'top1': 0.9464} is_best False lr [0.0010000000000000002]
training epoch 400 val accuracy 0.9452 topk_dict {'top1': 0.9452} is_best False lr [0.0010000000000000002]
training epoch 401 val accuracy 0.9446 topk_dict {'top1': 0.9446} is_best False lr [0.0010000000000000002]
training epoch 402 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.0010000000000000002]
training epoch 403 val accuracy 0.9462 topk_dict {'top1': 0.9462} is_best False lr [0.0010000000000000002]
training epoch 404 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 405 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 406 val accuracy 0.946 topk_dict {'top1': 0.946} is_best False lr [0.0010000000000000002]
training epoch 407 val accuracy 0.9452 topk_dict {'top1': 0.9452} is_best False lr [0.0010000000000000002]
training epoch 408 val accuracy 0.9464 topk_dict {'top1': 0.9464} is_best False lr [0.0010000000000000002]
training epoch 409 val accuracy 0.9464 topk_dict {'top1': 0.9464} is_best False lr [0.0010000000000000002]
training epoch 410 val accuracy 0.9464 topk_dict {'top1': 0.9464} is_best False lr [0.0010000000000000002]
training epoch 411 val accuracy 0.9462 topk_dict {'top1': 0.9462} is_best False lr [0.0010000000000000002]
training epoch 412 val accuracy 0.9454 topk_dict {'top1': 0.9454} is_best False lr [0.0010000000000000002]
training epoch 413 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.0010000000000000002]
training epoch 414 val accuracy 0.945 topk_dict {'top1': 0.945} is_best False lr [0.0010000000000000002]
training epoch 415 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.0010000000000000002]
training epoch 416 val accuracy 0.945 topk_dict {'top1': 0.945} is_best False lr [0.0010000000000000002]
training epoch 417 val accuracy 0.946 topk_dict {'top1': 0.946} is_best False lr [0.0010000000000000002]
training epoch 418 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.0010000000000000002]
training epoch 419 val accuracy 0.9452 topk_dict {'top1': 0.9452} is_best False lr [0.0010000000000000002]
training epoch 420 val accuracy 0.9452 topk_dict {'top1': 0.9452} is_best False lr [0.0010000000000000002]
training epoch 421 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 422 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.0010000000000000002]
training epoch 423 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.0010000000000000002]
training epoch 424 val accuracy 0.945 topk_dict {'top1': 0.945} is_best False lr [0.0010000000000000002]
training epoch 425 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.0010000000000000002]
training epoch 426 val accuracy 0.9446 topk_dict {'top1': 0.9446} is_best False lr [0.0010000000000000002]
training epoch 427 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 428 val accuracy 0.946 topk_dict {'top1': 0.946} is_best False lr [0.0010000000000000002]
training epoch 429 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 430 val accuracy 0.9438 topk_dict {'top1': 0.9438} is_best False lr [0.0010000000000000002]
training epoch 431 val accuracy 0.944 topk_dict {'top1': 0.944} is_best False lr [0.0010000000000000002]
training epoch 432 val accuracy 0.9454 topk_dict {'top1': 0.9454} is_best False lr [0.0010000000000000002]
training epoch 433 val accuracy 0.945 topk_dict {'top1': 0.945} is_best False lr [0.0010000000000000002]
training epoch 434 val accuracy 0.9438 topk_dict {'top1': 0.9438} is_best False lr [0.0010000000000000002]
training epoch 435 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.0010000000000000002]
training epoch 436 val accuracy 0.9466 topk_dict {'top1': 0.9466} is_best True lr [0.0010000000000000002]
training epoch 437 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 438 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 439 val accuracy 0.946 topk_dict {'top1': 0.946} is_best False lr [0.0010000000000000002]
training epoch 440 val accuracy 0.9454 topk_dict {'top1': 0.9454} is_best False lr [0.0010000000000000002]
training epoch 441 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 442 val accuracy 0.946 topk_dict {'top1': 0.946} is_best False lr [0.0010000000000000002]
training epoch 443 val accuracy 0.9454 topk_dict {'top1': 0.9454} is_best False lr [0.0010000000000000002]
training epoch 444 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 445 val accuracy 0.9454 topk_dict {'top1': 0.9454} is_best False lr [0.0010000000000000002]
training epoch 446 val accuracy 0.945 topk_dict {'top1': 0.945} is_best False lr [0.0010000000000000002]
training epoch 447 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 448 val accuracy 0.9462 topk_dict {'top1': 0.9462} is_best False lr [0.0010000000000000002]
training epoch 449 val accuracy 0.9452 topk_dict {'top1': 0.9452} is_best False lr [0.0010000000000000002]
training epoch 450 val accuracy 0.9464 topk_dict {'top1': 0.9464} is_best False lr [0.0010000000000000002]
training epoch 451 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 452 val accuracy 0.9464 topk_dict {'top1': 0.9464} is_best False lr [0.0010000000000000002]
training epoch 453 val accuracy 0.9452 topk_dict {'top1': 0.9452} is_best False lr [0.0010000000000000002]
training epoch 454 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 455 val accuracy 0.9468 topk_dict {'top1': 0.9468} is_best True lr [0.0010000000000000002]
training epoch 456 val accuracy 0.945 topk_dict {'top1': 0.945} is_best False lr [0.0010000000000000002]
training epoch 457 val accuracy 0.946 topk_dict {'top1': 0.946} is_best False lr [0.0010000000000000002]
training epoch 458 val accuracy 0.9462 topk_dict {'top1': 0.9462} is_best False lr [0.0010000000000000002]
training epoch 459 val accuracy 0.9454 topk_dict {'top1': 0.9454} is_best False lr [0.0010000000000000002]
training epoch 460 val accuracy 0.945 topk_dict {'top1': 0.945} is_best False lr [0.0010000000000000002]
training epoch 461 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 462 val accuracy 0.9468 topk_dict {'top1': 0.9468} is_best False lr [0.0010000000000000002]
training epoch 463 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 464 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.0010000000000000002]
training epoch 465 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 466 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.0010000000000000002]
training epoch 467 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.0010000000000000002]
training epoch 468 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 469 val accuracy 0.945 topk_dict {'top1': 0.945} is_best False lr [0.0010000000000000002]
training epoch 470 val accuracy 0.9446 topk_dict {'top1': 0.9446} is_best False lr [0.0010000000000000002]
training epoch 471 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.0010000000000000002]
training epoch 472 val accuracy 0.9464 topk_dict {'top1': 0.9464} is_best False lr [0.0010000000000000002]
training epoch 473 val accuracy 0.9452 topk_dict {'top1': 0.9452} is_best False lr [0.0010000000000000002]
training epoch 474 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.0010000000000000002]
training epoch 475 val accuracy 0.9452 topk_dict {'top1': 0.9452} is_best False lr [0.0010000000000000002]
training epoch 476 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 477 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 478 val accuracy 0.946 topk_dict {'top1': 0.946} is_best False lr [0.0010000000000000002]
training epoch 479 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 480 val accuracy 0.9452 topk_dict {'top1': 0.9452} is_best False lr [0.0010000000000000002]
training epoch 481 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 482 val accuracy 0.9454 topk_dict {'top1': 0.9454} is_best False lr [0.0010000000000000002]
training epoch 483 val accuracy 0.9446 topk_dict {'top1': 0.9446} is_best False lr [0.0010000000000000002]
training epoch 484 val accuracy 0.9444 topk_dict {'top1': 0.9444} is_best False lr [0.0010000000000000002]
training epoch 485 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 486 val accuracy 0.9462 topk_dict {'top1': 0.9462} is_best False lr [0.0010000000000000002]
training epoch 487 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.0010000000000000002]
training epoch 488 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 489 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.0010000000000000002]
training epoch 490 val accuracy 0.9462 topk_dict {'top1': 0.9462} is_best False lr [0.0010000000000000002]
training epoch 491 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 492 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.0010000000000000002]
training epoch 493 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 494 val accuracy 0.9448 topk_dict {'top1': 0.9448} is_best False lr [0.0010000000000000002]
training epoch 495 val accuracy 0.9456 topk_dict {'top1': 0.9456} is_best False lr [0.0010000000000000002]
training epoch 496 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 497 val accuracy 0.9458 topk_dict {'top1': 0.9458} is_best False lr [0.0010000000000000002]
training epoch 498 val accuracy 0.945 topk_dict {'top1': 0.945} is_best False lr [0.0010000000000000002]
training epoch 499 val accuracy 0.9454 topk_dict {'top1': 0.9454} is_best False lr [0.0010000000000000002]
loading model_best from epoch 455 (acc 0.946800)
finished training. finished 500 epochs. accuracy 0.9468 topk_dict {'top1': 0.9468}
