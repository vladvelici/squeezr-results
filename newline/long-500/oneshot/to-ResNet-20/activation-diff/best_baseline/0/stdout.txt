start iteration 0
[activation diff]: block to remove picked: 33, with score 0.007069. All blocks and scores: [(33, 0.007068843755405396), (32, 0.009399589500389993), (30, 0.010011187754571438), (31, 0.010232581524178386), (34, 0.013294661184772849), (29, 0.013421116629615426), (35, 0.01595768961124122), (26, 0.01607214123941958), (28, 0.01763686118647456), (27, 0.019022797467187047), (43, 0.01999649195931852), (46, 0.020590225234627724), (25, 0.02207829523831606), (23, 0.022228714544326067), (41, 0.022336416644975543), (44, 0.02314599882811308), (40, 0.023749589920043945), (45, 0.02397549501620233), (21, 0.024941088864579797), (48, 0.024957707384601235), (22, 0.025151390582323074), (50, 0.02528717485256493), (24, 0.025880583096295595), (49, 0.025916649727150798), (42, 0.02623223257251084), (20, 0.02684889198280871), (47, 0.028632949106395245), (38, 0.03134434390813112), (39, 0.03144129505380988), (15, 0.03205838426947594), (7, 0.03244550246745348), (19, 0.03254077862948179), (37, 0.0379180321469903), (51, 0.04178758803755045), (9, 0.04337632795795798), (6, 0.04682369623333216), (14, 0.04789772117510438), (4, 0.048522413708269596), (2, 0.05457740509882569), (3, 0.057849927339702845), (13, 0.059144286904484034), (11, 0.059700033627450466), (17, 0.06132525485008955), (0, 0.06337464554235339), (1, 0.06593216210603714), (52, 0.06606104411184788), (8, 0.074663613922894), (10, 0.08082299493253231), (16, 0.0852750651538372), (12, 0.09039537608623505), (5, 0.10671143513172865), (36, 0.4361986443400383), (18, 0.5117432847619057), (53, 0.805338516831398)]
computing accuracy for after removing block 33 . block score: 0.007068843755405396
removed block 33 current accuracy 0.949 loss from initial  0.0024000000000000687
since last training loss: 0.0024000000000000687 threshold 999.0 training needed False
start iteration 1
[activation diff]: block to remove picked: 32, with score 0.009400. All blocks and scores: [(32, 0.009399589500389993), (30, 0.010011187405325472), (31, 0.010232581524178386), (34, 0.013119243551045656), (29, 0.013421116746030748), (26, 0.01607214123941958), (35, 0.016093927435576916), (28, 0.01763686118647456), (27, 0.019022797467187047), (43, 0.019852687837556005), (46, 0.020300705451518297), (41, 0.0218602754175663), (25, 0.02207829523831606), (23, 0.022228715708479285), (44, 0.022977192886173725), (40, 0.0235738311894238), (45, 0.023648238042369485), (48, 0.024540217127650976), (50, 0.024770821910351515), (21, 0.02494108979590237), (22, 0.02515139034949243), (49, 0.025575741194188595), (24, 0.02588058332912624), (42, 0.02589341322891414), (20, 0.026848890585824847), (47, 0.02807276090607047), (38, 0.03109118714928627), (39, 0.031191361835226417), (15, 0.03205838426947594), (7, 0.03244550246745348), (19, 0.032540778163820505), (37, 0.03797321207821369), (51, 0.04127101460471749), (9, 0.043376327492296696), (6, 0.046823696698993444), (14, 0.04789772164076567), (4, 0.04852241417393088), (2, 0.05457740509882569), (3, 0.05784992780536413), (13, 0.059144286904484034), (11, 0.05970003502443433), (17, 0.06132525485008955), (0, 0.06337464740499854), (52, 0.0649335184134543), (1, 0.06593216210603714), (8, 0.07466361485421658), (10, 0.08082299865782261), (16, 0.08527506235986948), (12, 0.09039537608623505), (5, 0.1067114369943738), (36, 0.4339806064963341), (18, 0.5117433071136475), (53, 0.8063970655202866)]
computing accuracy for after removing block 32 . block score: 0.009399589500389993
removed block 32 current accuracy 0.9482 loss from initial  0.0031999999999999806
since last training loss: 0.0031999999999999806 threshold 999.0 training needed False
start iteration 2
[activation diff]: block to remove picked: 30, with score 0.010011. All blocks and scores: [(30, 0.010011187638156116), (31, 0.010232581407763064), (34, 0.012758881784975529), (29, 0.013421116629615426), (35, 0.01591842109337449), (26, 0.01607214123941958), (28, 0.017636860720813274), (27, 0.01902279770001769), (43, 0.019850464770570397), (46, 0.02041191584430635), (41, 0.021827629301697016), (25, 0.022078294539824128), (23, 0.022228715242817998), (44, 0.02289147791452706), (40, 0.023602579021826386), (45, 0.023770848521962762), (48, 0.024519873317331076), (50, 0.024639350594952703), (21, 0.024941090028733015), (22, 0.025151390815153718), (49, 0.025392549112439156), (42, 0.025712220696732402), (24, 0.025880582397803664), (20, 0.026848891517147422), (47, 0.028052504174411297), (38, 0.0309358739759773), (39, 0.03117303643375635), (15, 0.0320583856664598), (7, 0.03244550293311477), (19, 0.032540778163820505), (37, 0.0383431906811893), (51, 0.04113080771639943), (9, 0.043376327492296696), (6, 0.04682369623333216), (14, 0.04789772070944309), (4, 0.04852241277694702), (2, 0.05457740556448698), (3, 0.057849927339702845), (13, 0.059144286438822746), (11, 0.05970003409311175), (17, 0.06132525345310569), (0, 0.06337464647367597), (52, 0.06441722949966788), (1, 0.06593216024339199), (8, 0.07466361671686172), (10, 0.08082299493253231), (16, 0.08527506608515978), (12, 0.09039537701755762), (5, 0.1067114369943738), (36, 0.4350203089416027), (18, 0.5117433071136475), (53, 0.8136166781187057)]
computing accuracy for after removing block 30 . block score: 0.010011187638156116
removed block 30 current accuracy 0.9466 loss from initial  0.0048000000000000265
since last training loss: 0.0048000000000000265 threshold 999.0 training needed False
start iteration 3
[activation diff]: block to remove picked: 31, with score 0.010245. All blocks and scores: [(31, 0.010244824574328959), (34, 0.012400160310789943), (29, 0.013421116746030748), (35, 0.015918649500235915), (26, 0.01607214123941958), (28, 0.017636861419305205), (27, 0.019022797467187047), (43, 0.01986735058017075), (46, 0.020279744174331427), (41, 0.021756020840257406), (25, 0.022078295005485415), (23, 0.02222871547564864), (44, 0.02300137677229941), (40, 0.023739926517009735), (45, 0.02379016880877316), (48, 0.02435004571452737), (50, 0.024463105713948607), (21, 0.024941089563071728), (22, 0.02515139104798436), (49, 0.025246930308640003), (42, 0.025273551465943456), (24, 0.025880582397803664), (20, 0.026848891749978065), (47, 0.027727575274184346), (38, 0.030746272997930646), (39, 0.0312817960511893), (15, 0.03205838520079851), (7, 0.03244550293311477), (19, 0.03254077862948179), (37, 0.038952667731791735), (51, 0.04082479840144515), (9, 0.04337632656097412), (6, 0.04682369623333216), (14, 0.04789772070944309), (4, 0.048522412311285734), (2, 0.05457740416750312), (3, 0.057849927339702845), (13, 0.05914428737014532), (11, 0.059700033627450466), (17, 0.06132525438442826), (0, 0.06337464554235339), (52, 0.06356756249442697), (1, 0.06593216117471457), (8, 0.07466361578553915), (10, 0.08082299306988716), (16, 0.08527506422251463), (12, 0.09039537701755762), (5, 0.10671143420040607), (36, 0.4377693049609661), (18, 0.5117432996630669), (53, 0.8228829503059387)]
computing accuracy for after removing block 31 . block score: 0.010244824574328959
removed block 31 current accuracy 0.9462 loss from initial  0.005199999999999982
since last training loss: 0.005199999999999982 threshold 999.0 training needed False
start iteration 4
[activation diff]: block to remove picked: 34, with score 0.012506. All blocks and scores: [(34, 0.012506232247687876), (29, 0.013421116629615426), (35, 0.01596891228109598), (26, 0.016072141006588936), (28, 0.01763686118647456), (27, 0.019022798165678978), (43, 0.019837008323520422), (46, 0.020137188024818897), (41, 0.02158405538648367), (25, 0.022078294539824128), (23, 0.02222871547564864), (44, 0.02268732455559075), (40, 0.02356909867376089), (45, 0.023840720765292645), (48, 0.024108358891680837), (50, 0.024114209227263927), (49, 0.024870117427781224), (21, 0.024941088864579797), (42, 0.02504557464271784), (22, 0.025151389883831143), (24, 0.025880582630634308), (20, 0.026848891517147422), (47, 0.027423852356150746), (38, 0.030735649866983294), (39, 0.031410424038767815), (15, 0.03205838520079851), (7, 0.03244550246745348), (19, 0.03254077909514308), (37, 0.03908350924029946), (51, 0.04034593980759382), (9, 0.043376327492296696), (6, 0.04682369716465473), (14, 0.04789772070944309), (4, 0.04852241184562445), (2, 0.05457740509882569), (3, 0.05784992827102542), (13, 0.059144288301467896), (11, 0.059700035490095615), (17, 0.06132525485008955), (52, 0.06270107673481107), (0, 0.06337464740499854), (1, 0.06593216024339199), (8, 0.074663613922894), (10, 0.08082299493253231), (16, 0.08527506329119205), (12, 0.09039537608623505), (5, 0.10671143885701895), (36, 0.43692686036229134), (18, 0.5117432922124863), (53, 0.8283701166510582)]
computing accuracy for after removing block 34 . block score: 0.012506232247687876
removed block 34 current accuracy 0.946 loss from initial  0.005400000000000071
since last training loss: 0.005400000000000071 threshold 999.0 training needed False
start iteration 5
[activation diff]: block to remove picked: 29, with score 0.013421. All blocks and scores: [(29, 0.013421116746030748), (26, 0.01607214123941958), (35, 0.016558772418648005), (28, 0.017636860720813274), (27, 0.01902279770001769), (43, 0.020302683347836137), (46, 0.02032419736497104), (41, 0.021962702739983797), (25, 0.022078294306993484), (23, 0.02222871547564864), (44, 0.02304507768712938), (48, 0.024024547543376684), (50, 0.024096972541883588), (40, 0.024156817235052586), (45, 0.024168408941477537), (49, 0.024922373238950968), (21, 0.02494108979590237), (22, 0.02515139104798436), (42, 0.025816059904173017), (24, 0.02588058286346495), (20, 0.026848891051486135), (47, 0.027568294666707516), (38, 0.03178726346231997), (15, 0.0320583856664598), (39, 0.03225791361182928), (7, 0.03244550293311477), (19, 0.032540778163820505), (51, 0.04008621396496892), (37, 0.04069072986021638), (9, 0.04337632842361927), (6, 0.04682369530200958), (14, 0.04789772117510438), (4, 0.04852241091430187), (2, 0.05457740509882569), (3, 0.05784992687404156), (13, 0.05914428737014532), (11, 0.059700032230466604), (17, 0.061325253918766975), (52, 0.062210948672145605), (0, 0.06337464740499854), (1, 0.06593216024339199), (8, 0.07466361578553915), (10, 0.08082299306988716), (16, 0.08527505863457918), (12, 0.09039537888020277), (5, 0.10671143885701895), (36, 0.44933703169226646), (18, 0.5117433071136475), (53, 0.827703058719635)]
computing accuracy for after removing block 29 . block score: 0.013421116746030748
removed block 29 current accuracy 0.9406 loss from initial  0.010800000000000032
since last training loss: 0.010800000000000032 threshold 999.0 training needed False
start iteration 6
[activation diff]: block to remove picked: 26, with score 0.016072. All blocks and scores: [(26, 0.016072140773758292), (35, 0.01637051091529429), (28, 0.017636860720813274), (27, 0.019022797467187047), (43, 0.019856703700497746), (46, 0.01998897665180266), (41, 0.021256205393001437), (25, 0.022078294539824128), (23, 0.02222871547564864), (44, 0.022692032624036074), (48, 0.023521371884271502), (50, 0.02353389118798077), (40, 0.023616241058334708), (45, 0.023933292366564274), (49, 0.024449915159493685), (42, 0.024838328128680587), (21, 0.02494108909741044), (22, 0.02515139034949243), (24, 0.025880582397803664), (47, 0.02681345515884459), (20, 0.02684889198280871), (38, 0.031083732144907117), (39, 0.03205688949674368), (15, 0.03205838520079851), (7, 0.03244550246745348), (19, 0.03254077862948179), (51, 0.03907974949106574), (37, 0.04015214554965496), (9, 0.04337632702663541), (6, 0.046823696698993444), (14, 0.04789772164076567), (4, 0.048522412311285734), (2, 0.05457740509882569), (3, 0.05784992594271898), (13, 0.05914428550750017), (11, 0.05970003316178918), (52, 0.06036907434463501), (17, 0.061325253918766975), (0, 0.06337464740499854), (1, 0.06593216024339199), (8, 0.07466361671686172), (10, 0.08082299306988716), (16, 0.08527506329119205), (12, 0.09039537608623505), (5, 0.10671143792569637), (36, 0.4432784505188465), (18, 0.5117432996630669), (53, 0.8375032544136047)]
computing accuracy for after removing block 26 . block score: 0.016072140773758292
removed block 26 current accuracy 0.9362 loss from initial  0.015199999999999991
since last training loss: 0.015199999999999991 threshold 999.0 training needed False
start iteration 7
[activation diff]: block to remove picked: 35, with score 0.015504. All blocks and scores: [(35, 0.015504143433645368), (28, 0.016986021539196372), (27, 0.01876970916055143), (43, 0.01940557174384594), (46, 0.019700076431035995), (41, 0.02051579928956926), (25, 0.022078295703977346), (23, 0.02222871594130993), (44, 0.022507571848109365), (48, 0.022899368777871132), (50, 0.02293772855773568), (40, 0.023057401413097978), (42, 0.02352040889672935), (45, 0.023633699864149094), (49, 0.024081918876618147), (21, 0.024941089330241084), (22, 0.025151390815153718), (24, 0.025880583096295595), (47, 0.02632279205136001), (20, 0.026848892448469996), (38, 0.030149149475619197), (39, 0.03146669710986316), (15, 0.03205838520079851), (7, 0.03244550386443734), (19, 0.03254077862948179), (51, 0.037851928267627954), (37, 0.039268904365599155), (9, 0.04337632842361927), (6, 0.04682369576767087), (14, 0.04789772070944309), (4, 0.048522413708269596), (2, 0.054577404633164406), (3, 0.05784992780536413), (52, 0.05846811830997467), (13, 0.059144285041838884), (11, 0.05970003176480532), (17, 0.061325255781412125), (0, 0.06337464740499854), (1, 0.06593216024339199), (8, 0.07466361485421658), (10, 0.08082299586385489), (16, 0.0852750651538372), (12, 0.09039537888020277), (5, 0.10671143792569637), (36, 0.43490004166960716), (18, 0.5117433071136475), (53, 0.8595060855150223)]
computing accuracy for after removing block 35 . block score: 0.015504143433645368
removed block 35 current accuracy 0.9314 loss from initial  0.020000000000000018
since last training loss: 0.020000000000000018 threshold 999.0 training needed False
start iteration 8
[activation diff]: block to remove picked: 28, with score 0.016986. All blocks and scores: [(28, 0.016986021772027016), (43, 0.018381991423666477), (27, 0.018769709393382072), (46, 0.018842301797121763), (41, 0.019016371108591557), (48, 0.02130915690213442), (50, 0.021624521352350712), (44, 0.021748854545876384), (40, 0.02191696735098958), (42, 0.021930374205112457), (25, 0.02207829523831606), (23, 0.02222871594130993), (45, 0.022736449260264635), (49, 0.02297006268054247), (21, 0.02494108909741044), (22, 0.025151390582323074), (47, 0.025355831254273653), (24, 0.025880583096295595), (20, 0.026848891517147422), (38, 0.02869188692420721), (39, 0.029624431859701872), (15, 0.03205838520079851), (7, 0.03244550293311477), (19, 0.032540778163820505), (51, 0.03601635806262493), (37, 0.03643036773428321), (9, 0.04337632888928056), (6, 0.04682369530200958), (14, 0.04789772257208824), (4, 0.04852241277694702), (2, 0.05457740603014827), (52, 0.054668578784912825), (3, 0.05784992594271898), (13, 0.059144286438822746), (11, 0.059700032230466604), (17, 0.0613252529874444), (0, 0.06337464647367597), (1, 0.06593216117471457), (8, 0.07466361578553915), (10, 0.08082299400120974), (16, 0.08527506049722433), (12, 0.09039537515491247), (5, 0.10671143792569637), (36, 0.41641609370708466), (18, 0.5117432922124863), (53, 0.8948249071836472)]
computing accuracy for after removing block 28 . block score: 0.016986021772027016
removed block 28 current accuracy 0.9286 loss from initial  0.022800000000000042
since last training loss: 0.022800000000000042 threshold 999.0 training needed False
start iteration 9
[activation diff]: block to remove picked: 43, with score 0.017988. All blocks and scores: [(43, 0.017987635219469666), (46, 0.018358625005930662), (41, 0.018467806978151202), (27, 0.018769708462059498), (48, 0.020775508601218462), (42, 0.021206470672041178), (50, 0.021302448119968176), (44, 0.02158689615316689), (40, 0.02159272227436304), (25, 0.022078294772654772), (23, 0.02222871547564864), (45, 0.022315293084830046), (49, 0.02240756736136973), (47, 0.024609397631138563), (21, 0.024941089563071728), (22, 0.025151390582323074), (24, 0.02588058286346495), (20, 0.02684889268130064), (38, 0.02789032575674355), (39, 0.029191894689574838), (15, 0.03205838520079851), (7, 0.032445503398776054), (19, 0.032540778163820505), (51, 0.03550667688250542), (37, 0.03591922717168927), (9, 0.04337632795795798), (6, 0.04682369576767087), (14, 0.047897720243781805), (4, 0.04852241324260831), (52, 0.05337408371269703), (2, 0.05457740509882569), (3, 0.057849927339702845), (13, 0.059144286438822746), (11, 0.05970003502443433), (17, 0.06132525345310569), (0, 0.06337464461103082), (1, 0.06593216117471457), (8, 0.07466361578553915), (10, 0.08082299679517746), (16, 0.0852750614285469), (12, 0.09039537608623505), (5, 0.1067114369943738), (36, 0.4126182086765766), (18, 0.5117432847619057), (53, 0.906721293926239)]
computing accuracy for after removing block 43 . block score: 0.017987635219469666
removed block 43 current accuracy 0.9278 loss from initial  0.023600000000000065
since last training loss: 0.023600000000000065 threshold 999.0 training needed False
start iteration 10
[activation diff]: block to remove picked: 41, with score 0.018468. All blocks and scores: [(41, 0.018467806978151202), (27, 0.018769708229228854), (46, 0.018994681304320693), (42, 0.021206469973549247), (48, 0.021418894873932004), (50, 0.021441322285681963), (40, 0.02159272227436304), (25, 0.022078295005485415), (23, 0.022228715009987354), (49, 0.022339017363265157), (44, 0.02278283447958529), (45, 0.023323107045143843), (21, 0.024941089330241084), (22, 0.025151390582323074), (47, 0.025386077584698796), (24, 0.025880582630634308), (20, 0.026848891749978065), (38, 0.02789032505825162), (39, 0.029191893991082907), (15, 0.03205838520079851), (7, 0.03244550293311477), (19, 0.03254077862948179), (51, 0.03521771728992462), (37, 0.03591922763735056), (9, 0.04337632842361927), (6, 0.04682369576767087), (14, 0.047897720243781805), (4, 0.048522413708269596), (52, 0.05213337391614914), (2, 0.054577404633164406), (3, 0.057849929202347994), (13, 0.059144286438822746), (11, 0.059700032230466604), (17, 0.06132525438442826), (0, 0.06337464554235339), (1, 0.06593216210603714), (8, 0.07466361578553915), (10, 0.08082299493253231), (16, 0.08527506235986948), (12, 0.0903953779488802), (5, 0.1067114369943738), (36, 0.4126182049512863), (18, 0.5117432996630669), (53, 0.9521220922470093)]
computing accuracy for after removing block 41 . block score: 0.018467806978151202
removed block 41 current accuracy 0.9244 loss from initial  0.027000000000000024
since last training loss: 0.027000000000000024 threshold 999.0 training needed False
start iteration 11
[activation diff]: block to remove picked: 27, with score 0.018770. All blocks and scores: [(27, 0.018769708927720785), (46, 0.0188285403419286), (48, 0.020589640364050865), (50, 0.021004352252930403), (40, 0.02159272157587111), (42, 0.021820761263370514), (49, 0.021985649364069104), (25, 0.02207829523831606), (23, 0.022228715708479285), (44, 0.023674041498452425), (45, 0.02375203836709261), (21, 0.024941090028733015), (22, 0.025151390815153718), (47, 0.025630727875977755), (24, 0.025880583561956882), (20, 0.02684889198280871), (38, 0.027890324825420976), (39, 0.029191894689574838), (15, 0.03205838380381465), (7, 0.03244550200179219), (19, 0.032540778163820505), (51, 0.03395493049174547), (37, 0.03591922717168927), (9, 0.04337632702663541), (6, 0.04682369623333216), (14, 0.04789772164076567), (4, 0.04852241184562445), (52, 0.04963196348398924), (2, 0.054577404633164406), (3, 0.05784992873668671), (13, 0.059144288301467896), (11, 0.05970003316178918), (17, 0.061325253918766975), (0, 0.06337464554235339), (1, 0.06593215931206942), (8, 0.07466361578553915), (10, 0.08082299586385489), (16, 0.08527506608515978), (12, 0.09039537701755762), (5, 0.10671143513172865), (36, 0.4126182049512863), (18, 0.5117432847619057), (53, 1.0119272097945213)]
computing accuracy for after removing block 27 . block score: 0.018769708927720785
removed block 27 current accuracy 0.9184 loss from initial  0.03300000000000003
since last training loss: 0.03300000000000003 threshold 999.0 training needed False
start iteration 12
[activation diff]: block to remove picked: 46, with score 0.018469. All blocks and scores: [(46, 0.01846910989843309), (48, 0.019927536603063345), (50, 0.020503759384155273), (40, 0.02087594592012465), (42, 0.021248552249744534), (49, 0.021396144526079297), (25, 0.02207829523831606), (23, 0.02222871594130993), (44, 0.02292325859889388), (45, 0.023436836898326874), (47, 0.024697703076526523), (21, 0.024941089330241084), (22, 0.025151390116661787), (24, 0.025880582397803664), (20, 0.026848891749978065), (38, 0.026989459991455078), (39, 0.028602139791473746), (15, 0.032058386132121086), (7, 0.032445503398776054), (19, 0.03254077862948179), (51, 0.03302581747993827), (37, 0.03541383519768715), (9, 0.04337632702663541), (6, 0.04682369576767087), (52, 0.04775991616770625), (14, 0.047897722106426954), (4, 0.04852241277694702), (2, 0.05457740603014827), (3, 0.05784992687404156), (13, 0.05914428783580661), (11, 0.059700033627450466), (17, 0.06132525438442826), (0, 0.06337464833632112), (1, 0.06593216024339199), (8, 0.07466361578553915), (10, 0.08082299400120974), (16, 0.08527506235986948), (12, 0.0903953779488802), (5, 0.10671143885701895), (36, 0.40587935224175453), (18, 0.5117432996630669), (53, 1.0234627649188042)]
computing accuracy for after removing block 46 . block score: 0.01846910989843309
removed block 46 current accuracy 0.9132 loss from initial  0.03820000000000001
since last training loss: 0.03820000000000001 threshold 999.0 training needed False
start iteration 13
[activation diff]: block to remove picked: 48, with score 0.020277. All blocks and scores: [(48, 0.020276608411222696), (50, 0.020639733178541064), (40, 0.020875946385785937), (42, 0.021248552482575178), (25, 0.02207829523831606), (49, 0.022116728127002716), (23, 0.02222871594130993), (44, 0.022923258366063237), (45, 0.02343683782964945), (21, 0.024941089330241084), (22, 0.025151389883831143), (24, 0.02588058216497302), (47, 0.026193965459242463), (20, 0.026848892448469996), (38, 0.026989458361640573), (39, 0.028602140257135034), (15, 0.032058384735137224), (7, 0.03244550293311477), (19, 0.03254077769815922), (51, 0.033110261894762516), (37, 0.035413835663348436), (9, 0.043376327492296696), (6, 0.04682369576767087), (52, 0.0473557966761291), (14, 0.047897720243781805), (4, 0.048522412311285734), (2, 0.054577404633164406), (3, 0.057849930599331856), (13, 0.059144286438822746), (11, 0.059700033627450466), (17, 0.061325253918766975), (0, 0.06337464833632112), (1, 0.06593216210603714), (8, 0.0746636176481843), (10, 0.08082299493253231), (16, 0.08527506049722433), (12, 0.09039537701755762), (5, 0.10671143792569637), (36, 0.40587934479117393), (18, 0.5117432922124863), (53, 1.1398278772830963)]
computing accuracy for after removing block 48 . block score: 0.020276608411222696
removed block 48 current accuracy 0.9042 loss from initial  0.04720000000000002
since last training loss: 0.04720000000000002 threshold 999.0 training needed False
start iteration 14
[activation diff]: block to remove picked: 40, with score 0.020876. All blocks and scores: [(40, 0.020875946385785937), (42, 0.021248552715405822), (25, 0.022078295471146703), (50, 0.022216576850041747), (23, 0.02222871594130993), (44, 0.02292325859889388), (45, 0.023436837131157517), (49, 0.02476162021048367), (21, 0.02494108979590237), (22, 0.0251513896510005), (24, 0.025880582630634308), (47, 0.026193964993581176), (20, 0.02684889198280871), (38, 0.026989460224285722), (39, 0.028602139092981815), (15, 0.032058386132121086), (7, 0.03244550293311477), (19, 0.032540778163820505), (51, 0.03314514039084315), (37, 0.035413835663348436), (9, 0.04337632702663541), (6, 0.04682369623333216), (14, 0.04789771977812052), (4, 0.04852241324260831), (52, 0.049928945023566484), (2, 0.05457740416750312), (3, 0.05784992827102542), (13, 0.05914428737014532), (11, 0.05970003502443433), (17, 0.061325253918766975), (0, 0.06337464647367597), (1, 0.06593216303735971), (8, 0.07466361578553915), (10, 0.08082299213856459), (16, 0.0852750614285469), (12, 0.09039537515491247), (5, 0.10671143420040607), (36, 0.40587934851646423), (18, 0.5117432922124863), (53, 1.2528756707906723)]
computing accuracy for after removing block 40 . block score: 0.020875946385785937
removed block 40 current accuracy 0.896 loss from initial  0.055400000000000005
since last training loss: 0.055400000000000005 threshold 999.0 training needed False
start iteration 15
[activation diff]: block to remove picked: 42, with score 0.020879. All blocks and scores: [(42, 0.020879492163658142), (50, 0.02111514168791473), (25, 0.02207829523831606), (23, 0.022228715242817998), (45, 0.022996684769168496), (44, 0.02390008559450507), (49, 0.024068318540230393), (21, 0.02494108979590237), (22, 0.02515139034949243), (24, 0.025880583096295595), (47, 0.02612779545597732), (20, 0.026848891284316778), (38, 0.026989459758624434), (39, 0.028602140257135034), (15, 0.0320583856664598), (51, 0.032396155409514904), (7, 0.032445503398776054), (19, 0.03254077909514308), (37, 0.035413834266364574), (9, 0.04337632795795798), (6, 0.046823694836348295), (14, 0.047897720243781805), (52, 0.04809587216004729), (4, 0.04852241324260831), (2, 0.054577406495809555), (3, 0.05784992780536413), (13, 0.05914428597316146), (11, 0.059700033627450466), (17, 0.06132525485008955), (0, 0.06337464740499854), (1, 0.06593216117471457), (8, 0.07466361578553915), (10, 0.08082299120724201), (16, 0.08527506049722433), (12, 0.09039537701755762), (5, 0.10671143606305122), (36, 0.40587934851646423), (18, 0.5117432847619057), (53, 1.3537509143352509)]
computing accuracy for after removing block 42 . block score: 0.020879492163658142
removed block 42 current accuracy 0.8888 loss from initial  0.06259999999999999
since last training loss: 0.06259999999999999 threshold 999.0 training needed False
start iteration 16
[activation diff]: block to remove picked: 50, with score 0.021091. All blocks and scores: [(50, 0.02109115314669907), (25, 0.022078294539824128), (23, 0.022228715708479285), (45, 0.0236721346154809), (49, 0.024203354958444834), (44, 0.024335478199645877), (21, 0.024941088864579797), (22, 0.025151390815153718), (47, 0.025878204265609384), (24, 0.025880582397803664), (20, 0.026848892448469996), (38, 0.026989459060132504), (39, 0.02860214002430439), (51, 0.031482727732509375), (15, 0.032058386132121086), (7, 0.03244550200179219), (19, 0.03254077862948179), (37, 0.035413835663348436), (9, 0.043376327492296696), (52, 0.045693684834986925), (6, 0.046823696698993444), (14, 0.047897722106426954), (4, 0.04852241417393088), (2, 0.054577404633164406), (3, 0.05784992827102542), (13, 0.059144288301467896), (11, 0.05970003316178918), (17, 0.06132525485008955), (0, 0.06337464461103082), (1, 0.06593216117471457), (8, 0.07466361485421658), (10, 0.08082299586385489), (16, 0.08527505770325661), (12, 0.09039537701755762), (5, 0.1067114369943738), (36, 0.4058793634176254), (18, 0.5117433071136475), (53, 1.400962918996811)]
computing accuracy for after removing block 50 . block score: 0.02109115314669907
removed block 50 current accuracy 0.876 loss from initial  0.07540000000000002
since last training loss: 0.07540000000000002 threshold 999.0 training needed False
start iteration 17
[activation diff]: block to remove picked: 25, with score 0.022078. All blocks and scores: [(25, 0.02207829523831606), (23, 0.02222871547564864), (45, 0.023672134149819613), (49, 0.02420335472561419), (44, 0.02433547773398459), (21, 0.024941088864579797), (22, 0.025151389883831143), (47, 0.025878204265609384), (24, 0.02588058332912624), (20, 0.02684889198280871), (38, 0.026989459060132504), (39, 0.02860214072279632), (15, 0.032058384735137224), (7, 0.03244550293311477), (19, 0.03254077909514308), (51, 0.033588085789233446), (37, 0.035413836129009724), (9, 0.04337632795795798), (6, 0.04682369623333216), (14, 0.047897722106426954), (4, 0.04852241277694702), (52, 0.052295893896371126), (2, 0.05457740370184183), (3, 0.05784992640838027), (13, 0.059144286904484034), (11, 0.05970003409311175), (17, 0.06132525485008955), (0, 0.06337464461103082), (1, 0.06593216024339199), (8, 0.07466361485421658), (10, 0.08082299493253231), (16, 0.08527506235986948), (12, 0.0903953779488802), (5, 0.10671143606305122), (36, 0.40587934479117393), (18, 0.5117432922124863), (53, 1.6140173524618149)]
computing accuracy for after removing block 25 . block score: 0.02207829523831606
removed block 25 current accuracy 0.8662 loss from initial  0.08520000000000005
since last training loss: 0.08520000000000005 threshold 999.0 training needed False
start iteration 18
[activation diff]: block to remove picked: 23, with score 0.022229. All blocks and scores: [(23, 0.022228715009987354), (45, 0.02333430224098265), (49, 0.023444467689841986), (44, 0.023563595488667488), (21, 0.024941089563071728), (47, 0.025034847669303417), (22, 0.025151390582323074), (24, 0.02588058286346495), (38, 0.026316354051232338), (20, 0.026848891284316778), (39, 0.028504946967586875), (15, 0.0320583856664598), (7, 0.03244550293311477), (19, 0.032540778163820505), (51, 0.032601408660411835), (37, 0.03481204807758331), (9, 0.043376327492296696), (6, 0.04682369623333216), (14, 0.04789772164076567), (4, 0.048522412311285734), (52, 0.050034448970109224), (2, 0.05457740509882569), (3, 0.05784992594271898), (13, 0.059144286904484034), (11, 0.05970003176480532), (17, 0.061325253918766975), (0, 0.06337464647367597), (1, 0.06593216117471457), (8, 0.07466361578553915), (10, 0.08082299120724201), (16, 0.08527506235986948), (12, 0.09039537515491247), (5, 0.10671143606305122), (36, 0.39897944033145905), (18, 0.5117432847619057), (53, 1.6166377812623978)]
computing accuracy for after removing block 23 . block score: 0.022228715009987354
removed block 23 current accuracy 0.8484 loss from initial  0.10299999999999998
since last training loss: 0.10299999999999998 threshold 999.0 training needed False
start iteration 19
[activation diff]: block to remove picked: 44, with score 0.023170. All blocks and scores: [(44, 0.023170327069237828), (49, 0.023312296019867063), (45, 0.02354199718683958), (47, 0.024369530845433474), (24, 0.024534435709938407), (21, 0.024941089563071728), (22, 0.025151389883831143), (38, 0.026185827562585473), (20, 0.026848891051486135), (39, 0.028445158386602998), (15, 0.03205838520079851), (7, 0.03244550293311477), (51, 0.032508190255612135), (19, 0.03254077862948179), (37, 0.035895141772925854), (9, 0.04337632795795798), (6, 0.046823694836348295), (14, 0.04789772117510438), (52, 0.048511622939258814), (4, 0.04852241184562445), (2, 0.05457740416750312), (3, 0.05784992687404156), (13, 0.059144286438822746), (11, 0.05970003316178918), (17, 0.061325253918766975), (0, 0.06337464740499854), (1, 0.06593216210603714), (8, 0.074663613922894), (10, 0.08082299400120974), (16, 0.08527506049722433), (12, 0.09039537515491247), (5, 0.10671143792569637), (36, 0.40171102806925774), (18, 0.5117433071136475), (53, 1.6037908643484116)]
computing accuracy for after removing block 44 . block score: 0.023170327069237828
removed block 44 current accuracy 0.8256 loss from initial  0.12580000000000002
since last training loss: 0.12580000000000002 threshold 999.0 training needed False
start iteration 20
[activation diff]: block to remove picked: 45, with score 0.023154. All blocks and scores: [(45, 0.02315433113835752), (49, 0.023239578353241086), (24, 0.024534436175599694), (21, 0.02494108839891851), (22, 0.025151390815153718), (47, 0.02560506551526487), (38, 0.026185827562585473), (20, 0.026848891517147422), (39, 0.02844515908509493), (15, 0.03205838520079851), (51, 0.032147211488336325), (7, 0.032445503398776054), (19, 0.03254077862948179), (37, 0.03589514223858714), (9, 0.04337632656097412), (6, 0.04682369623333216), (52, 0.0475812335498631), (14, 0.04789772117510438), (4, 0.04852241184562445), (2, 0.05457740370184183), (3, 0.057849927339702845), (13, 0.059144286904484034), (11, 0.059700033627450466), (17, 0.06132525485008955), (0, 0.06337464740499854), (1, 0.06593216117471457), (8, 0.0746636176481843), (10, 0.08082299679517746), (16, 0.08527506235986948), (12, 0.09039537701755762), (5, 0.10671143792569637), (36, 0.40171102434396744), (18, 0.5117432773113251), (53, 1.7372965812683105)]
computing accuracy for after removing block 45 . block score: 0.02315433113835752
removed block 45 current accuracy 0.7878 loss from initial  0.16360000000000008
since last training loss: 0.16360000000000008 threshold 999.0 training needed False
start iteration 21
[activation diff]: block to remove picked: 49, with score 0.023890. All blocks and scores: [(49, 0.023889958625659347), (24, 0.024534435011446476), (21, 0.024941089330241084), (22, 0.025151390116661787), (38, 0.026185827562585473), (20, 0.026848891284316778), (47, 0.02699241554364562), (39, 0.028445158852264285), (51, 0.03199382359161973), (15, 0.032058386132121086), (7, 0.03244550246745348), (19, 0.03254077909514308), (37, 0.03589514223858714), (9, 0.04337632842361927), (6, 0.04682369530200958), (14, 0.047897722106426954), (4, 0.048522412311285734), (52, 0.04859335767105222), (2, 0.054577404633164406), (3, 0.05784992594271898), (13, 0.05914428737014532), (11, 0.05970003409311175), (17, 0.061325253918766975), (0, 0.06337464740499854), (1, 0.06593216024339199), (8, 0.0746636176481843), (10, 0.08082299400120974), (16, 0.08527506235986948), (12, 0.09039537608623505), (5, 0.10671143513172865), (36, 0.40171102806925774), (18, 0.5117433071136475), (53, 1.8795002400875092)]
computing accuracy for after removing block 49 . block score: 0.023889958625659347
removed block 49 current accuracy 0.7182 loss from initial  0.23320000000000007
since last training loss: 0.23320000000000007 threshold 999.0 training needed False
start iteration 22
[activation diff]: block to remove picked: 24, with score 0.024534. All blocks and scores: [(24, 0.02453443594276905), (21, 0.02494108979590237), (22, 0.025151390582323074), (38, 0.026185827562585473), (20, 0.02684889198280871), (47, 0.026992415310814977), (39, 0.02844515861943364), (15, 0.032058384735137224), (7, 0.03244550246745348), (19, 0.03254077909514308), (51, 0.03339511016383767), (37, 0.035895141772925854), (9, 0.04337632656097412), (6, 0.04682369576767087), (14, 0.04789772164076567), (4, 0.048522413708269596), (2, 0.05457740416750312), (52, 0.05507980100810528), (3, 0.05784992640838027), (13, 0.059144286904484034), (11, 0.05970003409311175), (17, 0.0613252567127347), (0, 0.06337464787065983), (1, 0.06593216117471457), (8, 0.07466361485421658), (10, 0.08082299493253231), (16, 0.08527505956590176), (12, 0.0903953779488802), (5, 0.10671143885701895), (36, 0.40171102806925774), (18, 0.5117432922124863), (53, 2.0280015617609024)]
computing accuracy for after removing block 24 . block score: 0.02453443594276905
removed block 24 current accuracy 0.676 loss from initial  0.2754
since last training loss: 0.2754 threshold 999.0 training needed False
start iteration 23
[activation diff]: block to remove picked: 21, with score 0.024941. All blocks and scores: [(21, 0.024941089330241084), (22, 0.025151390116661787), (38, 0.025702511193230748), (47, 0.02601254591718316), (20, 0.026848891749978065), (39, 0.027984512969851494), (15, 0.032058384735137224), (7, 0.03244550246745348), (19, 0.032540778163820505), (51, 0.0326524181291461), (37, 0.035636335611343384), (9, 0.04337632795795798), (6, 0.046823696698993444), (14, 0.04789772070944309), (4, 0.04852241277694702), (52, 0.05336605478078127), (2, 0.05457740509882569), (3, 0.05784992594271898), (13, 0.05914428876712918), (11, 0.05970003409311175), (17, 0.061325252521783113), (0, 0.06337464554235339), (1, 0.06593216210603714), (8, 0.07466361485421658), (10, 0.08082299306988716), (16, 0.08527506329119205), (12, 0.0903953779488802), (5, 0.10671143792569637), (36, 0.3932289332151413), (18, 0.5117433071136475), (53, 2.0319990515708923)]
computing accuracy for after removing block 21 . block score: 0.024941089330241084
removed block 21 current accuracy 0.651 loss from initial  0.3004
since last training loss: 0.3004 threshold 999.0 training needed False
start iteration 24
[activation diff]: block to remove picked: 22, with score 0.023398. All blocks and scores: [(22, 0.02339842519722879), (38, 0.02489633485674858), (47, 0.025349842151626945), (20, 0.026848891051486135), (39, 0.027679562103003263), (15, 0.032058384735137224), (51, 0.03225779952481389), (7, 0.03244550293311477), (19, 0.03254077862948179), (37, 0.035188923589885235), (9, 0.04337632656097412), (6, 0.04682369530200958), (14, 0.04789772070944309), (4, 0.04852241463959217), (52, 0.05205858126282692), (2, 0.05457740603014827), (3, 0.057849929202347994), (13, 0.05914428597316146), (11, 0.05970003316178918), (17, 0.06132525438442826), (0, 0.06337464833632112), (1, 0.06593216117471457), (8, 0.07466361671686172), (10, 0.08082299679517746), (16, 0.08527506049722433), (12, 0.09039537515491247), (5, 0.10671143606305122), (36, 0.3806835934519768), (18, 0.5117432847619057), (53, 2.035163015127182)]
computing accuracy for after removing block 22 . block score: 0.02339842519722879
removed block 22 current accuracy 0.6 loss from initial  0.35140000000000005
since last training loss: 0.35140000000000005 threshold 999.0 training needed False
start iteration 25
[activation diff]: block to remove picked: 47, with score 0.024421. All blocks and scores: [(47, 0.024420717731118202), (38, 0.024780795676633716), (20, 0.026848891284316778), (39, 0.027085774345323443), (15, 0.0320583856664598), (51, 0.032255595084279776), (7, 0.03244550293311477), (19, 0.03254077956080437), (37, 0.03583635948598385), (9, 0.04337632656097412), (6, 0.04682369623333216), (14, 0.04789772070944309), (4, 0.04852241277694702), (52, 0.05106716137379408), (2, 0.05457740509882569), (3, 0.05784992501139641), (13, 0.05914428597316146), (11, 0.05970003409311175), (17, 0.06132525531575084), (0, 0.06337464647367597), (1, 0.06593216117471457), (8, 0.07466361578553915), (10, 0.08082299493253231), (16, 0.08527506235986948), (12, 0.09039537515491247), (5, 0.1067114369943738), (36, 0.3776939772069454), (18, 0.5117432847619057), (53, 2.01689250767231)]
computing accuracy for after removing block 47 . block score: 0.024420717731118202
removed block 47 current accuracy 0.4938 loss from initial  0.4576
since last training loss: 0.4576 threshold 999.0 training needed False
start iteration 26
[activation diff]: block to remove picked: 38, with score 0.024781. All blocks and scores: [(38, 0.024780795443803072), (20, 0.026848891749978065), (39, 0.027085773879662156), (15, 0.03205838520079851), (7, 0.03244550293311477), (19, 0.03254077909514308), (51, 0.03279675683006644), (37, 0.03583635902032256), (9, 0.04337632842361927), (6, 0.04682369623333216), (14, 0.04789772117510438), (4, 0.048522413708269596), (2, 0.05457740603014827), (52, 0.05761870555579662), (3, 0.05784992873668671), (13, 0.05914428783580661), (11, 0.05970003502443433), (17, 0.06132525438442826), (0, 0.06337464926764369), (1, 0.06593216117471457), (8, 0.0746636176481843), (10, 0.08082299493253231), (16, 0.0852750614285469), (12, 0.09039537608623505), (5, 0.1067114369943738), (36, 0.3776939772069454), (18, 0.5117432996630669), (53, 2.184432178735733)]
computing accuracy for after removing block 38 . block score: 0.024780795443803072
removed block 38 current accuracy 0.4658 loss from initial  0.48560000000000003
since last training loss: 0.48560000000000003 threshold 999.0 training needed False
start iteration 27
[activation diff]: block to remove picked: 20, with score 0.026849. All blocks and scores: [(20, 0.026848891517147422), (39, 0.027795273810625076), (15, 0.03205838426947594), (51, 0.032336399890482426), (7, 0.03244550107046962), (19, 0.03254077862948179), (37, 0.03583635902032256), (9, 0.04337632656097412), (6, 0.046823696698993444), (14, 0.047897720243781805), (4, 0.04852241417393088), (2, 0.05457740603014827), (52, 0.056336406618356705), (3, 0.05784992594271898), (13, 0.059144286438822746), (11, 0.05970003316178918), (17, 0.061325253918766975), (0, 0.06337464647367597), (1, 0.06593216303735971), (8, 0.07466361578553915), (10, 0.08082299586385489), (16, 0.0852750614285469), (12, 0.0903953779488802), (5, 0.10671143792569637), (36, 0.3776939697563648), (18, 0.5117433071136475), (53, 2.258984297513962)]
computing accuracy for after removing block 20 . block score: 0.026848891517147422
removed block 20 current accuracy 0.437 loss from initial  0.5144
since last training loss: 0.5144 threshold 999.0 training needed False
start iteration 28
[activation diff]: block to remove picked: 39, with score 0.027671. All blocks and scores: [(39, 0.027671423042193055), (15, 0.0320583856664598), (51, 0.032159439753741026), (7, 0.032445503398776054), (19, 0.03254077909514308), (37, 0.03773009870201349), (9, 0.043376327492296696), (6, 0.04682369623333216), (14, 0.047897722106426954), (4, 0.04852241324260831), (2, 0.054577404633164406), (52, 0.05520405387505889), (3, 0.05784992873668671), (13, 0.05914428783580661), (11, 0.05970003502443433), (17, 0.06132525345310569), (0, 0.06337464461103082), (1, 0.06593216117471457), (8, 0.0746636176481843), (10, 0.08082299493253231), (16, 0.08527505770325661), (12, 0.0903953742235899), (5, 0.1067114369943738), (36, 0.37973883375525475), (18, 0.5117432996630669), (53, 2.2394739985466003)]
computing accuracy for after removing block 39 . block score: 0.027671423042193055
removed block 39 current accuracy 0.3936 loss from initial  0.5578000000000001
since last training loss: 0.5578000000000001 threshold 999.0 training needed False
start iteration 29
[activation diff]: block to remove picked: 51, with score 0.031472. All blocks and scores: [(51, 0.03147237887606025), (15, 0.03205838520079851), (7, 0.03244550246745348), (19, 0.03254077956080437), (37, 0.03773009916767478), (9, 0.043376327492296696), (6, 0.04682369763031602), (14, 0.047897722106426954), (4, 0.04852241277694702), (2, 0.05457740509882569), (52, 0.055551341734826565), (3, 0.05784992594271898), (13, 0.05914428737014532), (11, 0.059700033627450466), (17, 0.0613252567127347), (0, 0.06337464740499854), (1, 0.06593216024339199), (8, 0.07466361485421658), (10, 0.08082299306988716), (16, 0.08527506235986948), (12, 0.0903953742235899), (5, 0.10671143513172865), (36, 0.37973883375525475), (18, 0.5117432996630669), (53, 2.307855546474457)]
computing accuracy for after removing block 51 . block score: 0.03147237887606025
removed block 51 current accuracy 0.3444 loss from initial  0.607
since last training loss: 0.607 threshold 999.0 training needed False
start iteration 30
[activation diff]: block to remove picked: 15, with score 0.032058. All blocks and scores: [(15, 0.03205838520079851), (7, 0.03244550246745348), (19, 0.03254077956080437), (37, 0.03773009916767478), (9, 0.04337632842361927), (6, 0.04682369576767087), (14, 0.04789772164076567), (4, 0.04852241277694702), (2, 0.05457740416750312), (3, 0.05784992966800928), (13, 0.05914428597316146), (11, 0.05970003176480532), (17, 0.061325253918766975), (52, 0.06303920829668641), (0, 0.06337464740499854), (1, 0.06593216117471457), (8, 0.07466361485421658), (10, 0.08082299586385489), (16, 0.08527506049722433), (12, 0.09039537515491247), (5, 0.10671143792569637), (36, 0.37973883002996445), (18, 0.5117433071136475), (53, 2.115850865840912)]
computing accuracy for after removing block 15 . block score: 0.03205838520079851
removed block 15 current accuracy 0.3366 loss from initial  0.6148
since last training loss: 0.6148 threshold 999.0 training needed False
start iteration 31
[activation diff]: block to remove picked: 7, with score 0.032446. All blocks and scores: [(7, 0.032445503398776054), (19, 0.03279330814257264), (37, 0.03829699335619807), (9, 0.04337632888928056), (6, 0.04682369576767087), (14, 0.04789772117510438), (4, 0.04852241324260831), (2, 0.05457740370184183), (3, 0.05784992594271898), (13, 0.05914428923279047), (11, 0.05970003455877304), (52, 0.06273363064974546), (0, 0.06337464554235339), (17, 0.06512581277638674), (1, 0.06593216117471457), (8, 0.07466361485421658), (10, 0.08082299493253231), (12, 0.09039537701755762), (16, 0.09525770787149668), (5, 0.10671143792569637), (36, 0.3794775456190109), (18, 0.49915019050240517), (53, 2.111018717288971)]
computing accuracy for after removing block 7 . block score: 0.032445503398776054
removed block 7 current accuracy 0.2992 loss from initial  0.6522
since last training loss: 0.6522 threshold 999.0 training needed False
start iteration 32
[activation diff]: block to remove picked: 19, with score 0.032532. All blocks and scores: [(19, 0.032531521283090115), (37, 0.03785898117348552), (9, 0.043304409831762314), (14, 0.0442295647226274), (6, 0.04682369576767087), (4, 0.04852241184562445), (13, 0.05131126334890723), (2, 0.05457740556448698), (17, 0.05512783210724592), (11, 0.05635858187451959), (3, 0.05784992780536413), (52, 0.06323922658339143), (0, 0.06337464740499854), (1, 0.06593216117471457), (8, 0.07265629805624485), (10, 0.08300740644335747), (12, 0.08430038020014763), (16, 0.0866648843511939), (5, 0.1067114369943738), (36, 0.36962924897670746), (18, 0.4824951961636543), (53, 2.1644020676612854)]
computing accuracy for after removing block 19 . block score: 0.032531521283090115
removed block 19 current accuracy 0.2772 loss from initial  0.6742
since last training loss: 0.6742 threshold 999.0 training needed False
start iteration 33
[activation diff]: block to remove picked: 37, with score 0.039987. All blocks and scores: [(37, 0.03998737083747983), (9, 0.04330441076308489), (14, 0.0442295647226274), (6, 0.046823694836348295), (4, 0.048522412311285734), (13, 0.05131126241758466), (2, 0.05457740556448698), (17, 0.055127829778939486), (11, 0.05635858327150345), (3, 0.05784992687404156), (52, 0.06244313903152943), (0, 0.06337464554235339), (1, 0.06593216024339199), (8, 0.07265629991889), (10, 0.08300740644335747), (12, 0.08430038020014763), (16, 0.0866648843511939), (5, 0.10671143513172865), (36, 0.3686497360467911), (18, 0.4824952036142349), (53, 2.127133399248123)]
computing accuracy for after removing block 37 . block score: 0.03998737083747983
removed block 37 current accuracy 0.2794 loss from initial  0.672
since last training loss: 0.672 threshold 999.0 training needed False
start iteration 34
[activation diff]: block to remove picked: 9, with score 0.043304. All blocks and scores: [(9, 0.0433044102974236), (14, 0.04422956518828869), (6, 0.046823694836348295), (4, 0.04852241277694702), (13, 0.05131126195192337), (2, 0.054577404633164406), (17, 0.05512783210724592), (11, 0.05635858420282602), (3, 0.05784992501139641), (52, 0.06159692443907261), (0, 0.06337464740499854), (1, 0.06593216117471457), (8, 0.07265630085021257), (10, 0.08300740551203489), (12, 0.08430038206279278), (16, 0.08666488341987133), (5, 0.10671143606305122), (36, 0.3686497323215008), (18, 0.48249518871307373), (53, 2.1725645661354065)]
computing accuracy for after removing block 9 . block score: 0.0433044102974236
removed block 9 current accuracy 0.2698 loss from initial  0.6816
since last training loss: 0.6816 threshold 999.0 training needed False
start iteration 35
[activation diff]: block to remove picked: 14, with score 0.040101. All blocks and scores: [(14, 0.040101193357259035), (6, 0.04682369716465473), (4, 0.04852241277694702), (13, 0.048681370448321104), (17, 0.049202424474060535), (11, 0.05203389935195446), (2, 0.054577406495809555), (3, 0.05784992640838027), (52, 0.06127366190776229), (0, 0.06337464740499854), (1, 0.06593216396868229), (16, 0.07129026763141155), (12, 0.07246417738497257), (8, 0.07265629991889), (10, 0.08135402761399746), (5, 0.1067114369943738), (36, 0.3467654921114445), (18, 0.4637942798435688), (53, 2.259740710258484)]
computing accuracy for after removing block 14 . block score: 0.040101193357259035
removed block 14 current accuracy 0.228 loss from initial  0.7234
since last training loss: 0.7234 threshold 999.0 training needed False
start iteration 36
[activation diff]: block to remove picked: 6, with score 0.046824. All blocks and scores: [(6, 0.04682369576767087), (4, 0.048522412311285734), (13, 0.048681368585675955), (17, 0.048765205312520266), (11, 0.0520338979549706), (2, 0.054577404633164406), (3, 0.05784992687404156), (52, 0.060692342929542065), (0, 0.06337464647367597), (1, 0.06593216117471457), (12, 0.07246417831629515), (8, 0.07265629805624485), (10, 0.08135402854532003), (16, 0.09396332688629627), (5, 0.10671143606305122), (36, 0.35454605147242546), (18, 0.46163247898221016), (53, 2.327344238758087)]
computing accuracy for after removing block 6 . block score: 0.04682369576767087
removed block 6 current accuracy 0.1834 loss from initial  0.768
since last training loss: 0.768 threshold 999.0 training needed False
start iteration 37
[activation diff]: block to remove picked: 17, with score 0.045347. All blocks and scores: [(17, 0.04534666100516915), (13, 0.04583631083369255), (11, 0.04758233996108174), (4, 0.04852241324260831), (2, 0.05457740603014827), (3, 0.057849927339702845), (52, 0.06070501171052456), (0, 0.06337464647367597), (1, 0.06593215931206942), (12, 0.0673904363065958), (8, 0.07155622635036707), (16, 0.0773526057600975), (10, 0.08496761415153742), (5, 0.1067114407196641), (36, 0.36861881613731384), (18, 0.45650912821292877), (53, 2.401894748210907)]
computing accuracy for after removing block 17 . block score: 0.04534666100516915
removed block 17 current accuracy 0.197 loss from initial  0.7544
since last training loss: 0.7544 threshold 999.0 training needed False
start iteration 38
[activation diff]: block to remove picked: 13, with score 0.045836. All blocks and scores: [(13, 0.04583630943670869), (11, 0.047582341358065605), (4, 0.04852241324260831), (2, 0.05457740416750312), (52, 0.0575317507609725), (3, 0.05784992594271898), (0, 0.06337464647367597), (1, 0.06593216117471457), (12, 0.06739043723791838), (8, 0.07155622728168964), (16, 0.07735260762274265), (10, 0.08496761508285999), (5, 0.10671143606305122), (36, 0.3433842584490776), (18, 0.43070901930332184), (53, 2.5057159066200256)]
computing accuracy for after removing block 13 . block score: 0.04583630943670869
removed block 13 current accuracy 0.1604 loss from initial  0.791
since last training loss: 0.791 threshold 999.0 training needed False
start iteration 39
[activation diff]: block to remove picked: 11, with score 0.047582. All blocks and scores: [(11, 0.04758233996108174), (4, 0.048522413708269596), (2, 0.054577404633164406), (52, 0.05776288406923413), (3, 0.05784992873668671), (0, 0.06337464647367597), (1, 0.06593216117471457), (12, 0.06739043723791838), (8, 0.07155622821301222), (10, 0.08496761415153742), (16, 0.0936900619417429), (5, 0.1067114369943738), (36, 0.3634735830128193), (18, 0.4482332803308964), (53, 2.5989274382591248)]
computing accuracy for after removing block 11 . block score: 0.04758233996108174
removed block 11 current accuracy 0.1516 loss from initial  0.7998000000000001
since last training loss: 0.7998000000000001 threshold 999.0 training needed False
start iteration 40
[activation diff]: block to remove picked: 4, with score 0.048522. All blocks and scores: [(4, 0.04852241277694702), (2, 0.05457740416750312), (52, 0.05666986666619778), (12, 0.05737890303134918), (3, 0.05784992780536413), (0, 0.06337464647367597), (1, 0.06593215931206942), (16, 0.06831849832087755), (8, 0.07155623007565737), (10, 0.08496761508285999), (5, 0.10671143978834152), (36, 0.3732268773019314), (18, 0.4631049782037735), (53, 2.6798461377620697)]
computing accuracy for after removing block 4 . block score: 0.04852241277694702
removed block 4 current accuracy 0.1326 loss from initial  0.8188
since last training loss: 0.8188 threshold 999.0 training needed False
start iteration 41
[activation diff]: block to remove picked: 2, with score 0.054577. All blocks and scores: [(2, 0.05457740742713213), (3, 0.05784992780536413), (12, 0.05832128785550594), (52, 0.05989856040105224), (16, 0.061494333669543266), (0, 0.06337464740499854), (1, 0.06593216024339199), (8, 0.07770832628011703), (10, 0.0867101913318038), (5, 0.11118694208562374), (36, 0.4001452885568142), (18, 0.48430299013853073), (53, 2.6357489824295044)]
computing accuracy for after removing block 2 . block score: 0.05457740742713213
removed block 2 current accuracy 0.1318 loss from initial  0.8196
since last training loss: 0.8196 threshold 999.0 training needed False
start iteration 42
[activation diff]: block to remove picked: 3, with score 0.052834. All blocks and scores: [(3, 0.05283353989943862), (12, 0.054059040267020464), (16, 0.054587765596807), (52, 0.057342220563441515), (0, 0.06337464740499854), (1, 0.06593216024339199), (8, 0.07861093617975712), (10, 0.08458110131323338), (5, 0.10775264631956816), (36, 0.3884984254837036), (18, 0.45711148902773857), (53, 2.6733318865299225)]
computing accuracy for after removing block 3 . block score: 0.05283353989943862
removed block 3 current accuracy 0.1126 loss from initial  0.8388
since last training loss: 0.8388 threshold 999.0 training needed False
start iteration 43
[activation diff]: block to remove picked: 16, with score 0.042458. All blocks and scores: [(16, 0.04245817521587014), (12, 0.0550756324082613), (0, 0.06337464461103082), (52, 0.06362959370017052), (1, 0.06593216117471457), (8, 0.07531067542731762), (10, 0.08848739136010408), (5, 0.1113701369613409), (36, 0.42842140048742294), (18, 0.4566718228161335), (53, 2.5199026465415955)]
computing accuracy for after removing block 16 . block score: 0.04245817521587014
removed block 16 current accuracy 0.1128 loss from initial  0.8386
since last training loss: 0.8386 threshold 999.0 training needed False
start iteration 44
[activation diff]: block to remove picked: 12, with score 0.055076. All blocks and scores: [(12, 0.055075633339583874), (0, 0.06337464554235339), (1, 0.06593215931206942), (52, 0.0670879827812314), (8, 0.07531067542731762), (10, 0.08848738949745893), (5, 0.11137013416737318), (36, 0.4331531599164009), (18, 0.47831685468554497), (53, 2.4951522946357727)]
computing accuracy for after removing block 12 . block score: 0.055075633339583874
removed block 12 current accuracy 0.1002 loss from initial  0.8512000000000001
training start
training epoch 0 val accuracy 0.739 topk_dict {'top1': 0.739} is_best True lr [0.1]
training epoch 1 val accuracy 0.7594 topk_dict {'top1': 0.7594} is_best True lr [0.1]
training epoch 2 val accuracy 0.765 topk_dict {'top1': 0.765} is_best True lr [0.1]
training epoch 3 val accuracy 0.7992 topk_dict {'top1': 0.7992} is_best True lr [0.1]
training epoch 4 val accuracy 0.837 topk_dict {'top1': 0.837} is_best True lr [0.1]
training epoch 5 val accuracy 0.7936 topk_dict {'top1': 0.7936} is_best False lr [0.1]
training epoch 6 val accuracy 0.8114 topk_dict {'top1': 0.8114} is_best False lr [0.1]
training epoch 7 val accuracy 0.8092 topk_dict {'top1': 0.8092} is_best False lr [0.1]
training epoch 8 val accuracy 0.8376 topk_dict {'top1': 0.8376} is_best True lr [0.1]
training epoch 9 val accuracy 0.8634 topk_dict {'top1': 0.8634} is_best True lr [0.1]
training epoch 10 val accuracy 0.8408 topk_dict {'top1': 0.8408} is_best False lr [0.1]
training epoch 11 val accuracy 0.834 topk_dict {'top1': 0.834} is_best False lr [0.1]
training epoch 12 val accuracy 0.8592 topk_dict {'top1': 0.8592} is_best False lr [0.1]
training epoch 13 val accuracy 0.8454 topk_dict {'top1': 0.8454} is_best False lr [0.1]
training epoch 14 val accuracy 0.8456 topk_dict {'top1': 0.8456} is_best False lr [0.1]
training epoch 15 val accuracy 0.8512 topk_dict {'top1': 0.8512} is_best False lr [0.1]
training epoch 16 val accuracy 0.8388 topk_dict {'top1': 0.8388} is_best False lr [0.1]
training epoch 17 val accuracy 0.8432 topk_dict {'top1': 0.8432} is_best False lr [0.1]
training epoch 18 val accuracy 0.8766 topk_dict {'top1': 0.8766} is_best True lr [0.1]
training epoch 19 val accuracy 0.7972 topk_dict {'top1': 0.7972} is_best False lr [0.1]
training epoch 20 val accuracy 0.8506 topk_dict {'top1': 0.8506} is_best False lr [0.1]
training epoch 21 val accuracy 0.8378 topk_dict {'top1': 0.8378} is_best False lr [0.1]
training epoch 22 val accuracy 0.8696 topk_dict {'top1': 0.8696} is_best False lr [0.1]
training epoch 23 val accuracy 0.8498 topk_dict {'top1': 0.8498} is_best False lr [0.1]
training epoch 24 val accuracy 0.8396 topk_dict {'top1': 0.8396} is_best False lr [0.1]
training epoch 25 val accuracy 0.8672 topk_dict {'top1': 0.8672} is_best False lr [0.1]
training epoch 26 val accuracy 0.827 topk_dict {'top1': 0.827} is_best False lr [0.1]
training epoch 27 val accuracy 0.8698 topk_dict {'top1': 0.8698} is_best False lr [0.1]
training epoch 28 val accuracy 0.8652 topk_dict {'top1': 0.8652} is_best False lr [0.1]
training epoch 29 val accuracy 0.8298 topk_dict {'top1': 0.8298} is_best False lr [0.1]
training epoch 30 val accuracy 0.8594 topk_dict {'top1': 0.8594} is_best False lr [0.1]
training epoch 31 val accuracy 0.8576 topk_dict {'top1': 0.8576} is_best False lr [0.1]
training epoch 32 val accuracy 0.86 topk_dict {'top1': 0.86} is_best False lr [0.1]
training epoch 33 val accuracy 0.867 topk_dict {'top1': 0.867} is_best False lr [0.1]
training epoch 34 val accuracy 0.8516 topk_dict {'top1': 0.8516} is_best False lr [0.1]
training epoch 35 val accuracy 0.8624 topk_dict {'top1': 0.8624} is_best False lr [0.1]
training epoch 36 val accuracy 0.8614 topk_dict {'top1': 0.8614} is_best False lr [0.1]
training epoch 37 val accuracy 0.8752 topk_dict {'top1': 0.8752} is_best False lr [0.1]
training epoch 38 val accuracy 0.866 topk_dict {'top1': 0.866} is_best False lr [0.1]
training epoch 39 val accuracy 0.8682 topk_dict {'top1': 0.8682} is_best False lr [0.1]
training epoch 40 val accuracy 0.8706 topk_dict {'top1': 0.8706} is_best False lr [0.1]
training epoch 41 val accuracy 0.8596 topk_dict {'top1': 0.8596} is_best False lr [0.1]
training epoch 42 val accuracy 0.8762 topk_dict {'top1': 0.8762} is_best False lr [0.1]
training epoch 43 val accuracy 0.8676 topk_dict {'top1': 0.8676} is_best False lr [0.1]
training epoch 44 val accuracy 0.8694 topk_dict {'top1': 0.8694} is_best False lr [0.1]
training epoch 45 val accuracy 0.8642 topk_dict {'top1': 0.8642} is_best False lr [0.1]
training epoch 46 val accuracy 0.877 topk_dict {'top1': 0.877} is_best True lr [0.1]
training epoch 47 val accuracy 0.8538 topk_dict {'top1': 0.8538} is_best False lr [0.1]
training epoch 48 val accuracy 0.8562 topk_dict {'top1': 0.8562} is_best False lr [0.1]
training epoch 49 val accuracy 0.8666 topk_dict {'top1': 0.8666} is_best False lr [0.1]
training epoch 50 val accuracy 0.8742 topk_dict {'top1': 0.8742} is_best False lr [0.1]
training epoch 51 val accuracy 0.8784 topk_dict {'top1': 0.8784} is_best True lr [0.1]
training epoch 52 val accuracy 0.8592 topk_dict {'top1': 0.8592} is_best False lr [0.1]
training epoch 53 val accuracy 0.8554 topk_dict {'top1': 0.8554} is_best False lr [0.1]
training epoch 54 val accuracy 0.8638 topk_dict {'top1': 0.8638} is_best False lr [0.1]
training epoch 55 val accuracy 0.8712 topk_dict {'top1': 0.8712} is_best False lr [0.1]
training epoch 56 val accuracy 0.8672 topk_dict {'top1': 0.8672} is_best False lr [0.1]
training epoch 57 val accuracy 0.8748 topk_dict {'top1': 0.8748} is_best False lr [0.1]
training epoch 58 val accuracy 0.8668 topk_dict {'top1': 0.8668} is_best False lr [0.1]
training epoch 59 val accuracy 0.8774 topk_dict {'top1': 0.8774} is_best False lr [0.1]
training epoch 60 val accuracy 0.8566 topk_dict {'top1': 0.8566} is_best False lr [0.1]
training epoch 61 val accuracy 0.8712 topk_dict {'top1': 0.8712} is_best False lr [0.1]
training epoch 62 val accuracy 0.8682 topk_dict {'top1': 0.8682} is_best False lr [0.1]
training epoch 63 val accuracy 0.8588 topk_dict {'top1': 0.8588} is_best False lr [0.1]
training epoch 64 val accuracy 0.8532 topk_dict {'top1': 0.8532} is_best False lr [0.1]
training epoch 65 val accuracy 0.851 topk_dict {'top1': 0.851} is_best False lr [0.1]
training epoch 66 val accuracy 0.8846 topk_dict {'top1': 0.8846} is_best True lr [0.1]
training epoch 67 val accuracy 0.8624 topk_dict {'top1': 0.8624} is_best False lr [0.1]
training epoch 68 val accuracy 0.848 topk_dict {'top1': 0.848} is_best False lr [0.1]
training epoch 69 val accuracy 0.8792 topk_dict {'top1': 0.8792} is_best False lr [0.1]
training epoch 70 val accuracy 0.8628 topk_dict {'top1': 0.8628} is_best False lr [0.1]
training epoch 71 val accuracy 0.8726 topk_dict {'top1': 0.8726} is_best False lr [0.1]
training epoch 72 val accuracy 0.8586 topk_dict {'top1': 0.8586} is_best False lr [0.1]
training epoch 73 val accuracy 0.8674 topk_dict {'top1': 0.8674} is_best False lr [0.1]
training epoch 74 val accuracy 0.8786 topk_dict {'top1': 0.8786} is_best False lr [0.1]
training epoch 75 val accuracy 0.8754 topk_dict {'top1': 0.8754} is_best False lr [0.1]
training epoch 76 val accuracy 0.8884 topk_dict {'top1': 0.8884} is_best True lr [0.1]
training epoch 77 val accuracy 0.8784 topk_dict {'top1': 0.8784} is_best False lr [0.1]
training epoch 78 val accuracy 0.874 topk_dict {'top1': 0.874} is_best False lr [0.1]
training epoch 79 val accuracy 0.8638 topk_dict {'top1': 0.8638} is_best False lr [0.1]
training epoch 80 val accuracy 0.8718 topk_dict {'top1': 0.8718} is_best False lr [0.1]
training epoch 81 val accuracy 0.8812 topk_dict {'top1': 0.8812} is_best False lr [0.1]
training epoch 82 val accuracy 0.8688 topk_dict {'top1': 0.8688} is_best False lr [0.1]
training epoch 83 val accuracy 0.8706 topk_dict {'top1': 0.8706} is_best False lr [0.1]
training epoch 84 val accuracy 0.8576 topk_dict {'top1': 0.8576} is_best False lr [0.1]
training epoch 85 val accuracy 0.8686 topk_dict {'top1': 0.8686} is_best False lr [0.1]
training epoch 86 val accuracy 0.8682 topk_dict {'top1': 0.8682} is_best False lr [0.1]
training epoch 87 val accuracy 0.8562 topk_dict {'top1': 0.8562} is_best False lr [0.1]
training epoch 88 val accuracy 0.888 topk_dict {'top1': 0.888} is_best False lr [0.1]
training epoch 89 val accuracy 0.849 topk_dict {'top1': 0.849} is_best False lr [0.1]
training epoch 90 val accuracy 0.8792 topk_dict {'top1': 0.8792} is_best False lr [0.1]
training epoch 91 val accuracy 0.8398 topk_dict {'top1': 0.8398} is_best False lr [0.1]
training epoch 92 val accuracy 0.8838 topk_dict {'top1': 0.8838} is_best False lr [0.1]
training epoch 93 val accuracy 0.8824 topk_dict {'top1': 0.8824} is_best False lr [0.1]
training epoch 94 val accuracy 0.8382 topk_dict {'top1': 0.8382} is_best False lr [0.1]
training epoch 95 val accuracy 0.8378 topk_dict {'top1': 0.8378} is_best False lr [0.1]
training epoch 96 val accuracy 0.8756 topk_dict {'top1': 0.8756} is_best False lr [0.1]
training epoch 97 val accuracy 0.8532 topk_dict {'top1': 0.8532} is_best False lr [0.1]
training epoch 98 val accuracy 0.863 topk_dict {'top1': 0.863} is_best False lr [0.1]
training epoch 99 val accuracy 0.8812 topk_dict {'top1': 0.8812} is_best False lr [0.1]
training epoch 100 val accuracy 0.8576 topk_dict {'top1': 0.8576} is_best False lr [0.1]
training epoch 101 val accuracy 0.8368 topk_dict {'top1': 0.8368} is_best False lr [0.1]
training epoch 102 val accuracy 0.882 topk_dict {'top1': 0.882} is_best False lr [0.1]
training epoch 103 val accuracy 0.8726 topk_dict {'top1': 0.8726} is_best False lr [0.1]
training epoch 104 val accuracy 0.8816 topk_dict {'top1': 0.8816} is_best False lr [0.1]
training epoch 105 val accuracy 0.8716 topk_dict {'top1': 0.8716} is_best False lr [0.1]
training epoch 106 val accuracy 0.8756 topk_dict {'top1': 0.8756} is_best False lr [0.1]
training epoch 107 val accuracy 0.8856 topk_dict {'top1': 0.8856} is_best False lr [0.1]
training epoch 108 val accuracy 0.8494 topk_dict {'top1': 0.8494} is_best False lr [0.1]
training epoch 109 val accuracy 0.8764 topk_dict {'top1': 0.8764} is_best False lr [0.1]
training epoch 110 val accuracy 0.8624 topk_dict {'top1': 0.8624} is_best False lr [0.1]
training epoch 111 val accuracy 0.8764 topk_dict {'top1': 0.8764} is_best False lr [0.1]
training epoch 112 val accuracy 0.8822 topk_dict {'top1': 0.8822} is_best False lr [0.1]
training epoch 113 val accuracy 0.8618 topk_dict {'top1': 0.8618} is_best False lr [0.1]
training epoch 114 val accuracy 0.8818 topk_dict {'top1': 0.8818} is_best False lr [0.1]
training epoch 115 val accuracy 0.8844 topk_dict {'top1': 0.8844} is_best False lr [0.1]
training epoch 116 val accuracy 0.8674 topk_dict {'top1': 0.8674} is_best False lr [0.1]
training epoch 117 val accuracy 0.8902 topk_dict {'top1': 0.8902} is_best True lr [0.1]
training epoch 118 val accuracy 0.8866 topk_dict {'top1': 0.8866} is_best False lr [0.1]
training epoch 119 val accuracy 0.8788 topk_dict {'top1': 0.8788} is_best False lr [0.1]
training epoch 120 val accuracy 0.8744 topk_dict {'top1': 0.8744} is_best False lr [0.1]
training epoch 121 val accuracy 0.874 topk_dict {'top1': 0.874} is_best False lr [0.1]
training epoch 122 val accuracy 0.877 topk_dict {'top1': 0.877} is_best False lr [0.1]
training epoch 123 val accuracy 0.8884 topk_dict {'top1': 0.8884} is_best False lr [0.1]
training epoch 124 val accuracy 0.8552 topk_dict {'top1': 0.8552} is_best False lr [0.1]
training epoch 125 val accuracy 0.8828 topk_dict {'top1': 0.8828} is_best False lr [0.1]
training epoch 126 val accuracy 0.8864 topk_dict {'top1': 0.8864} is_best False lr [0.1]
training epoch 127 val accuracy 0.877 topk_dict {'top1': 0.877} is_best False lr [0.1]
training epoch 128 val accuracy 0.8764 topk_dict {'top1': 0.8764} is_best False lr [0.1]
training epoch 129 val accuracy 0.8682 topk_dict {'top1': 0.8682} is_best False lr [0.1]
training epoch 130 val accuracy 0.8772 topk_dict {'top1': 0.8772} is_best False lr [0.1]
training epoch 131 val accuracy 0.8736 topk_dict {'top1': 0.8736} is_best False lr [0.1]
training epoch 132 val accuracy 0.881 topk_dict {'top1': 0.881} is_best False lr [0.1]
training epoch 133 val accuracy 0.8728 topk_dict {'top1': 0.8728} is_best False lr [0.1]
training epoch 134 val accuracy 0.882 topk_dict {'top1': 0.882} is_best False lr [0.1]
training epoch 135 val accuracy 0.8674 topk_dict {'top1': 0.8674} is_best False lr [0.1]
training epoch 136 val accuracy 0.8838 topk_dict {'top1': 0.8838} is_best False lr [0.1]
training epoch 137 val accuracy 0.8642 topk_dict {'top1': 0.8642} is_best False lr [0.1]
training epoch 138 val accuracy 0.879 topk_dict {'top1': 0.879} is_best False lr [0.1]
training epoch 139 val accuracy 0.8728 topk_dict {'top1': 0.8728} is_best False lr [0.1]
training epoch 140 val accuracy 0.8842 topk_dict {'top1': 0.8842} is_best False lr [0.1]
training epoch 141 val accuracy 0.8784 topk_dict {'top1': 0.8784} is_best False lr [0.1]
training epoch 142 val accuracy 0.8666 topk_dict {'top1': 0.8666} is_best False lr [0.1]
training epoch 143 val accuracy 0.8776 topk_dict {'top1': 0.8776} is_best False lr [0.1]
training epoch 144 val accuracy 0.8834 topk_dict {'top1': 0.8834} is_best False lr [0.1]
training epoch 145 val accuracy 0.8728 topk_dict {'top1': 0.8728} is_best False lr [0.1]
training epoch 146 val accuracy 0.8796 topk_dict {'top1': 0.8796} is_best False lr [0.1]
training epoch 147 val accuracy 0.86 topk_dict {'top1': 0.86} is_best False lr [0.1]
training epoch 148 val accuracy 0.8794 topk_dict {'top1': 0.8794} is_best False lr [0.1]
training epoch 149 val accuracy 0.8928 topk_dict {'top1': 0.8928} is_best True lr [0.1]
training epoch 150 val accuracy 0.8716 topk_dict {'top1': 0.8716} is_best False lr [0.1]
training epoch 151 val accuracy 0.8868 topk_dict {'top1': 0.8868} is_best False lr [0.1]
training epoch 152 val accuracy 0.8796 topk_dict {'top1': 0.8796} is_best False lr [0.1]
training epoch 153 val accuracy 0.874 topk_dict {'top1': 0.874} is_best False lr [0.1]
training epoch 154 val accuracy 0.8502 topk_dict {'top1': 0.8502} is_best False lr [0.1]
training epoch 155 val accuracy 0.8738 topk_dict {'top1': 0.8738} is_best False lr [0.1]
training epoch 156 val accuracy 0.8752 topk_dict {'top1': 0.8752} is_best False lr [0.1]
training epoch 157 val accuracy 0.8664 topk_dict {'top1': 0.8664} is_best False lr [0.1]
training epoch 158 val accuracy 0.8674 topk_dict {'top1': 0.8674} is_best False lr [0.1]
training epoch 159 val accuracy 0.8412 topk_dict {'top1': 0.8412} is_best False lr [0.1]
training epoch 160 val accuracy 0.873 topk_dict {'top1': 0.873} is_best False lr [0.1]
training epoch 161 val accuracy 0.8714 topk_dict {'top1': 0.8714} is_best False lr [0.1]
training epoch 162 val accuracy 0.862 topk_dict {'top1': 0.862} is_best False lr [0.1]
training epoch 163 val accuracy 0.8706 topk_dict {'top1': 0.8706} is_best False lr [0.1]
training epoch 164 val accuracy 0.8912 topk_dict {'top1': 0.8912} is_best False lr [0.1]
training epoch 165 val accuracy 0.8776 topk_dict {'top1': 0.8776} is_best False lr [0.1]
training epoch 166 val accuracy 0.8496 topk_dict {'top1': 0.8496} is_best False lr [0.1]
training epoch 167 val accuracy 0.866 topk_dict {'top1': 0.866} is_best False lr [0.1]
training epoch 168 val accuracy 0.8722 topk_dict {'top1': 0.8722} is_best False lr [0.1]
training epoch 169 val accuracy 0.8738 topk_dict {'top1': 0.8738} is_best False lr [0.1]
training epoch 170 val accuracy 0.8642 topk_dict {'top1': 0.8642} is_best False lr [0.1]
training epoch 171 val accuracy 0.8802 topk_dict {'top1': 0.8802} is_best False lr [0.1]
training epoch 172 val accuracy 0.8682 topk_dict {'top1': 0.8682} is_best False lr [0.1]
training epoch 173 val accuracy 0.8808 topk_dict {'top1': 0.8808} is_best False lr [0.1]
training epoch 174 val accuracy 0.8868 topk_dict {'top1': 0.8868} is_best False lr [0.1]
training epoch 175 val accuracy 0.8848 topk_dict {'top1': 0.8848} is_best False lr [0.1]
training epoch 176 val accuracy 0.8284 topk_dict {'top1': 0.8284} is_best False lr [0.1]
training epoch 177 val accuracy 0.8478 topk_dict {'top1': 0.8478} is_best False lr [0.1]
training epoch 178 val accuracy 0.8808 topk_dict {'top1': 0.8808} is_best False lr [0.1]
training epoch 179 val accuracy 0.8688 topk_dict {'top1': 0.8688} is_best False lr [0.1]
training epoch 180 val accuracy 0.8842 topk_dict {'top1': 0.8842} is_best False lr [0.1]
training epoch 181 val accuracy 0.8758 topk_dict {'top1': 0.8758} is_best False lr [0.1]
training epoch 182 val accuracy 0.8728 topk_dict {'top1': 0.8728} is_best False lr [0.1]
training epoch 183 val accuracy 0.876 topk_dict {'top1': 0.876} is_best False lr [0.1]
training epoch 184 val accuracy 0.8874 topk_dict {'top1': 0.8874} is_best False lr [0.1]
training epoch 185 val accuracy 0.8826 topk_dict {'top1': 0.8826} is_best False lr [0.1]
training epoch 186 val accuracy 0.863 topk_dict {'top1': 0.863} is_best False lr [0.1]
training epoch 187 val accuracy 0.8808 topk_dict {'top1': 0.8808} is_best False lr [0.1]
training epoch 188 val accuracy 0.884 topk_dict {'top1': 0.884} is_best False lr [0.1]
training epoch 189 val accuracy 0.8746 topk_dict {'top1': 0.8746} is_best False lr [0.1]
training epoch 190 val accuracy 0.8638 topk_dict {'top1': 0.8638} is_best False lr [0.1]
training epoch 191 val accuracy 0.8844 topk_dict {'top1': 0.8844} is_best False lr [0.1]
training epoch 192 val accuracy 0.8882 topk_dict {'top1': 0.8882} is_best False lr [0.1]
training epoch 193 val accuracy 0.8596 topk_dict {'top1': 0.8596} is_best False lr [0.1]
training epoch 194 val accuracy 0.8676 topk_dict {'top1': 0.8676} is_best False lr [0.1]
training epoch 195 val accuracy 0.8744 topk_dict {'top1': 0.8744} is_best False lr [0.1]
training epoch 196 val accuracy 0.9026 topk_dict {'top1': 0.9026} is_best True lr [0.1]
training epoch 197 val accuracy 0.8842 topk_dict {'top1': 0.8842} is_best False lr [0.1]
training epoch 198 val accuracy 0.8562 topk_dict {'top1': 0.8562} is_best False lr [0.1]
training epoch 199 val accuracy 0.8618 topk_dict {'top1': 0.8618} is_best False lr [0.1]
training epoch 200 val accuracy 0.8704 topk_dict {'top1': 0.8704} is_best False lr [0.1]
training epoch 201 val accuracy 0.8594 topk_dict {'top1': 0.8594} is_best False lr [0.1]
training epoch 202 val accuracy 0.8854 topk_dict {'top1': 0.8854} is_best False lr [0.1]
training epoch 203 val accuracy 0.8712 topk_dict {'top1': 0.8712} is_best False lr [0.1]
training epoch 204 val accuracy 0.8868 topk_dict {'top1': 0.8868} is_best False lr [0.1]
training epoch 205 val accuracy 0.8702 topk_dict {'top1': 0.8702} is_best False lr [0.1]
training epoch 206 val accuracy 0.8792 topk_dict {'top1': 0.8792} is_best False lr [0.1]
training epoch 207 val accuracy 0.8636 topk_dict {'top1': 0.8636} is_best False lr [0.1]
training epoch 208 val accuracy 0.871 topk_dict {'top1': 0.871} is_best False lr [0.1]
training epoch 209 val accuracy 0.8796 topk_dict {'top1': 0.8796} is_best False lr [0.1]
training epoch 210 val accuracy 0.8804 topk_dict {'top1': 0.8804} is_best False lr [0.1]
training epoch 211 val accuracy 0.8908 topk_dict {'top1': 0.8908} is_best False lr [0.1]
training epoch 212 val accuracy 0.8656 topk_dict {'top1': 0.8656} is_best False lr [0.1]
training epoch 213 val accuracy 0.8892 topk_dict {'top1': 0.8892} is_best False lr [0.1]
training epoch 214 val accuracy 0.8724 topk_dict {'top1': 0.8724} is_best False lr [0.1]
training epoch 215 val accuracy 0.877 topk_dict {'top1': 0.877} is_best False lr [0.1]
training epoch 216 val accuracy 0.8682 topk_dict {'top1': 0.8682} is_best False lr [0.1]
training epoch 217 val accuracy 0.874 topk_dict {'top1': 0.874} is_best False lr [0.1]
training epoch 218 val accuracy 0.8754 topk_dict {'top1': 0.8754} is_best False lr [0.1]
training epoch 219 val accuracy 0.8808 topk_dict {'top1': 0.8808} is_best False lr [0.1]
training epoch 220 val accuracy 0.8804 topk_dict {'top1': 0.8804} is_best False lr [0.1]
training epoch 221 val accuracy 0.8612 topk_dict {'top1': 0.8612} is_best False lr [0.1]
training epoch 222 val accuracy 0.8884 topk_dict {'top1': 0.8884} is_best False lr [0.1]
training epoch 223 val accuracy 0.8848 topk_dict {'top1': 0.8848} is_best False lr [0.1]
training epoch 224 val accuracy 0.8836 topk_dict {'top1': 0.8836} is_best False lr [0.1]
training epoch 225 val accuracy 0.8668 topk_dict {'top1': 0.8668} is_best False lr [0.1]
training epoch 226 val accuracy 0.8716 topk_dict {'top1': 0.8716} is_best False lr [0.1]
training epoch 227 val accuracy 0.8954 topk_dict {'top1': 0.8954} is_best False lr [0.1]
training epoch 228 val accuracy 0.885 topk_dict {'top1': 0.885} is_best False lr [0.1]
training epoch 229 val accuracy 0.8834 topk_dict {'top1': 0.8834} is_best False lr [0.1]
training epoch 230 val accuracy 0.8554 topk_dict {'top1': 0.8554} is_best False lr [0.1]
training epoch 231 val accuracy 0.881 topk_dict {'top1': 0.881} is_best False lr [0.1]
training epoch 232 val accuracy 0.8804 topk_dict {'top1': 0.8804} is_best False lr [0.1]
training epoch 233 val accuracy 0.8818 topk_dict {'top1': 0.8818} is_best False lr [0.1]
training epoch 234 val accuracy 0.8896 topk_dict {'top1': 0.8896} is_best False lr [0.1]
training epoch 235 val accuracy 0.8848 topk_dict {'top1': 0.8848} is_best False lr [0.1]
training epoch 236 val accuracy 0.8746 topk_dict {'top1': 0.8746} is_best False lr [0.1]
training epoch 237 val accuracy 0.8844 topk_dict {'top1': 0.8844} is_best False lr [0.1]
training epoch 238 val accuracy 0.8862 topk_dict {'top1': 0.8862} is_best False lr [0.1]
training epoch 239 val accuracy 0.8802 topk_dict {'top1': 0.8802} is_best False lr [0.1]
training epoch 240 val accuracy 0.8902 topk_dict {'top1': 0.8902} is_best False lr [0.1]
training epoch 241 val accuracy 0.8716 topk_dict {'top1': 0.8716} is_best False lr [0.1]
training epoch 242 val accuracy 0.8804 topk_dict {'top1': 0.8804} is_best False lr [0.1]
training epoch 243 val accuracy 0.8842 topk_dict {'top1': 0.8842} is_best False lr [0.1]
training epoch 244 val accuracy 0.8764 topk_dict {'top1': 0.8764} is_best False lr [0.1]
training epoch 245 val accuracy 0.858 topk_dict {'top1': 0.858} is_best False lr [0.1]
training epoch 246 val accuracy 0.8838 topk_dict {'top1': 0.8838} is_best False lr [0.1]
training epoch 247 val accuracy 0.8868 topk_dict {'top1': 0.8868} is_best False lr [0.1]
training epoch 248 val accuracy 0.8826 topk_dict {'top1': 0.8826} is_best False lr [0.1]
training epoch 249 val accuracy 0.8672 topk_dict {'top1': 0.8672} is_best False lr [0.1]
training epoch 250 val accuracy 0.923 topk_dict {'top1': 0.923} is_best True lr [0.010000000000000002]
training epoch 251 val accuracy 0.925 topk_dict {'top1': 0.925} is_best True lr [0.010000000000000002]
training epoch 252 val accuracy 0.9268 topk_dict {'top1': 0.9268} is_best True lr [0.010000000000000002]
training epoch 253 val accuracy 0.9258 topk_dict {'top1': 0.9258} is_best False lr [0.010000000000000002]
training epoch 254 val accuracy 0.923 topk_dict {'top1': 0.923} is_best False lr [0.010000000000000002]
training epoch 255 val accuracy 0.9242 topk_dict {'top1': 0.9242} is_best False lr [0.010000000000000002]
training epoch 256 val accuracy 0.9276 topk_dict {'top1': 0.9276} is_best True lr [0.010000000000000002]
training epoch 257 val accuracy 0.9228 topk_dict {'top1': 0.9228} is_best False lr [0.010000000000000002]
training epoch 258 val accuracy 0.9218 topk_dict {'top1': 0.9218} is_best False lr [0.010000000000000002]
training epoch 259 val accuracy 0.9262 topk_dict {'top1': 0.9262} is_best False lr [0.010000000000000002]
training epoch 260 val accuracy 0.9258 topk_dict {'top1': 0.9258} is_best False lr [0.010000000000000002]
training epoch 261 val accuracy 0.9226 topk_dict {'top1': 0.9226} is_best False lr [0.010000000000000002]
training epoch 262 val accuracy 0.9266 topk_dict {'top1': 0.9266} is_best False lr [0.010000000000000002]
training epoch 263 val accuracy 0.9244 topk_dict {'top1': 0.9244} is_best False lr [0.010000000000000002]
training epoch 264 val accuracy 0.9236 topk_dict {'top1': 0.9236} is_best False lr [0.010000000000000002]
training epoch 265 val accuracy 0.928 topk_dict {'top1': 0.928} is_best True lr [0.010000000000000002]
training epoch 266 val accuracy 0.9274 topk_dict {'top1': 0.9274} is_best False lr [0.010000000000000002]
training epoch 267 val accuracy 0.9284 topk_dict {'top1': 0.9284} is_best True lr [0.010000000000000002]
training epoch 268 val accuracy 0.9252 topk_dict {'top1': 0.9252} is_best False lr [0.010000000000000002]
training epoch 269 val accuracy 0.9282 topk_dict {'top1': 0.9282} is_best False lr [0.010000000000000002]
training epoch 270 val accuracy 0.928 topk_dict {'top1': 0.928} is_best False lr [0.010000000000000002]
training epoch 271 val accuracy 0.9272 topk_dict {'top1': 0.9272} is_best False lr [0.010000000000000002]
training epoch 272 val accuracy 0.9256 topk_dict {'top1': 0.9256} is_best False lr [0.010000000000000002]
training epoch 273 val accuracy 0.9276 topk_dict {'top1': 0.9276} is_best False lr [0.010000000000000002]
training epoch 274 val accuracy 0.9254 topk_dict {'top1': 0.9254} is_best False lr [0.010000000000000002]
training epoch 275 val accuracy 0.9264 topk_dict {'top1': 0.9264} is_best False lr [0.010000000000000002]
training epoch 276 val accuracy 0.9254 topk_dict {'top1': 0.9254} is_best False lr [0.010000000000000002]
training epoch 277 val accuracy 0.9272 topk_dict {'top1': 0.9272} is_best False lr [0.010000000000000002]
training epoch 278 val accuracy 0.927 topk_dict {'top1': 0.927} is_best False lr [0.010000000000000002]
training epoch 279 val accuracy 0.925 topk_dict {'top1': 0.925} is_best False lr [0.010000000000000002]
training epoch 280 val accuracy 0.9228 topk_dict {'top1': 0.9228} is_best False lr [0.010000000000000002]
training epoch 281 val accuracy 0.9254 topk_dict {'top1': 0.9254} is_best False lr [0.010000000000000002]
training epoch 282 val accuracy 0.9276 topk_dict {'top1': 0.9276} is_best False lr [0.010000000000000002]
training epoch 283 val accuracy 0.9268 topk_dict {'top1': 0.9268} is_best False lr [0.010000000000000002]
training epoch 284 val accuracy 0.927 topk_dict {'top1': 0.927} is_best False lr [0.010000000000000002]
training epoch 285 val accuracy 0.9258 topk_dict {'top1': 0.9258} is_best False lr [0.010000000000000002]
training epoch 286 val accuracy 0.9256 topk_dict {'top1': 0.9256} is_best False lr [0.010000000000000002]
training epoch 287 val accuracy 0.9268 topk_dict {'top1': 0.9268} is_best False lr [0.010000000000000002]
training epoch 288 val accuracy 0.9272 topk_dict {'top1': 0.9272} is_best False lr [0.010000000000000002]
training epoch 289 val accuracy 0.926 topk_dict {'top1': 0.926} is_best False lr [0.010000000000000002]
training epoch 290 val accuracy 0.9248 topk_dict {'top1': 0.9248} is_best False lr [0.010000000000000002]
training epoch 291 val accuracy 0.9256 topk_dict {'top1': 0.9256} is_best False lr [0.010000000000000002]
training epoch 292 val accuracy 0.928 topk_dict {'top1': 0.928} is_best False lr [0.010000000000000002]
training epoch 293 val accuracy 0.9268 topk_dict {'top1': 0.9268} is_best False lr [0.010000000000000002]
training epoch 294 val accuracy 0.9244 topk_dict {'top1': 0.9244} is_best False lr [0.010000000000000002]
training epoch 295 val accuracy 0.9256 topk_dict {'top1': 0.9256} is_best False lr [0.010000000000000002]
training epoch 296 val accuracy 0.9236 topk_dict {'top1': 0.9236} is_best False lr [0.010000000000000002]
training epoch 297 val accuracy 0.926 topk_dict {'top1': 0.926} is_best False lr [0.010000000000000002]
training epoch 298 val accuracy 0.921 topk_dict {'top1': 0.921} is_best False lr [0.010000000000000002]
training epoch 299 val accuracy 0.9256 topk_dict {'top1': 0.9256} is_best False lr [0.010000000000000002]
training epoch 300 val accuracy 0.925 topk_dict {'top1': 0.925} is_best False lr [0.010000000000000002]
training epoch 301 val accuracy 0.9236 topk_dict {'top1': 0.9236} is_best False lr [0.010000000000000002]
training epoch 302 val accuracy 0.9262 topk_dict {'top1': 0.9262} is_best False lr [0.010000000000000002]
training epoch 303 val accuracy 0.9258 topk_dict {'top1': 0.9258} is_best False lr [0.010000000000000002]
training epoch 304 val accuracy 0.926 topk_dict {'top1': 0.926} is_best False lr [0.010000000000000002]
training epoch 305 val accuracy 0.9288 topk_dict {'top1': 0.9288} is_best True lr [0.010000000000000002]
training epoch 306 val accuracy 0.9252 topk_dict {'top1': 0.9252} is_best False lr [0.010000000000000002]
training epoch 307 val accuracy 0.9258 topk_dict {'top1': 0.9258} is_best False lr [0.010000000000000002]
training epoch 308 val accuracy 0.9236 topk_dict {'top1': 0.9236} is_best False lr [0.010000000000000002]
training epoch 309 val accuracy 0.9222 topk_dict {'top1': 0.9222} is_best False lr [0.010000000000000002]
training epoch 310 val accuracy 0.9268 topk_dict {'top1': 0.9268} is_best False lr [0.010000000000000002]
training epoch 311 val accuracy 0.9262 topk_dict {'top1': 0.9262} is_best False lr [0.010000000000000002]
training epoch 312 val accuracy 0.9254 topk_dict {'top1': 0.9254} is_best False lr [0.010000000000000002]
training epoch 313 val accuracy 0.926 topk_dict {'top1': 0.926} is_best False lr [0.010000000000000002]
training epoch 314 val accuracy 0.9278 topk_dict {'top1': 0.9278} is_best False lr [0.010000000000000002]
training epoch 315 val accuracy 0.9238 topk_dict {'top1': 0.9238} is_best False lr [0.010000000000000002]
training epoch 316 val accuracy 0.9234 topk_dict {'top1': 0.9234} is_best False lr [0.010000000000000002]
training epoch 317 val accuracy 0.9226 topk_dict {'top1': 0.9226} is_best False lr [0.010000000000000002]
training epoch 318 val accuracy 0.9274 topk_dict {'top1': 0.9274} is_best False lr [0.010000000000000002]
training epoch 319 val accuracy 0.9218 topk_dict {'top1': 0.9218} is_best False lr [0.010000000000000002]
training epoch 320 val accuracy 0.9246 topk_dict {'top1': 0.9246} is_best False lr [0.010000000000000002]
training epoch 321 val accuracy 0.9266 topk_dict {'top1': 0.9266} is_best False lr [0.010000000000000002]
training epoch 322 val accuracy 0.9258 topk_dict {'top1': 0.9258} is_best False lr [0.010000000000000002]
training epoch 323 val accuracy 0.9224 topk_dict {'top1': 0.9224} is_best False lr [0.010000000000000002]
training epoch 324 val accuracy 0.928 topk_dict {'top1': 0.928} is_best False lr [0.010000000000000002]
training epoch 325 val accuracy 0.9248 topk_dict {'top1': 0.9248} is_best False lr [0.010000000000000002]
training epoch 326 val accuracy 0.9212 topk_dict {'top1': 0.9212} is_best False lr [0.010000000000000002]
training epoch 327 val accuracy 0.9264 topk_dict {'top1': 0.9264} is_best False lr [0.010000000000000002]
training epoch 328 val accuracy 0.9222 topk_dict {'top1': 0.9222} is_best False lr [0.010000000000000002]
training epoch 329 val accuracy 0.9262 topk_dict {'top1': 0.9262} is_best False lr [0.010000000000000002]
training epoch 330 val accuracy 0.9234 topk_dict {'top1': 0.9234} is_best False lr [0.010000000000000002]
training epoch 331 val accuracy 0.9246 topk_dict {'top1': 0.9246} is_best False lr [0.010000000000000002]
training epoch 332 val accuracy 0.9234 topk_dict {'top1': 0.9234} is_best False lr [0.010000000000000002]
training epoch 333 val accuracy 0.9236 topk_dict {'top1': 0.9236} is_best False lr [0.010000000000000002]
training epoch 334 val accuracy 0.9242 topk_dict {'top1': 0.9242} is_best False lr [0.010000000000000002]
training epoch 335 val accuracy 0.9234 topk_dict {'top1': 0.9234} is_best False lr [0.010000000000000002]
training epoch 336 val accuracy 0.9234 topk_dict {'top1': 0.9234} is_best False lr [0.010000000000000002]
training epoch 337 val accuracy 0.9242 topk_dict {'top1': 0.9242} is_best False lr [0.010000000000000002]
training epoch 338 val accuracy 0.92 topk_dict {'top1': 0.92} is_best False lr [0.010000000000000002]
training epoch 339 val accuracy 0.9234 topk_dict {'top1': 0.9234} is_best False lr [0.010000000000000002]
training epoch 340 val accuracy 0.9212 topk_dict {'top1': 0.9212} is_best False lr [0.010000000000000002]
training epoch 341 val accuracy 0.9216 topk_dict {'top1': 0.9216} is_best False lr [0.010000000000000002]
training epoch 342 val accuracy 0.9216 topk_dict {'top1': 0.9216} is_best False lr [0.010000000000000002]
training epoch 343 val accuracy 0.9218 topk_dict {'top1': 0.9218} is_best False lr [0.010000000000000002]
training epoch 344 val accuracy 0.92 topk_dict {'top1': 0.92} is_best False lr [0.010000000000000002]
training epoch 345 val accuracy 0.9226 topk_dict {'top1': 0.9226} is_best False lr [0.010000000000000002]
training epoch 346 val accuracy 0.923 topk_dict {'top1': 0.923} is_best False lr [0.010000000000000002]
training epoch 347 val accuracy 0.9236 topk_dict {'top1': 0.9236} is_best False lr [0.010000000000000002]
training epoch 348 val accuracy 0.922 topk_dict {'top1': 0.922} is_best False lr [0.010000000000000002]
training epoch 349 val accuracy 0.922 topk_dict {'top1': 0.922} is_best False lr [0.010000000000000002]
training epoch 350 val accuracy 0.9196 topk_dict {'top1': 0.9196} is_best False lr [0.010000000000000002]
training epoch 351 val accuracy 0.9198 topk_dict {'top1': 0.9198} is_best False lr [0.010000000000000002]
training epoch 352 val accuracy 0.9186 topk_dict {'top1': 0.9186} is_best False lr [0.010000000000000002]
training epoch 353 val accuracy 0.918 topk_dict {'top1': 0.918} is_best False lr [0.010000000000000002]
training epoch 354 val accuracy 0.9228 topk_dict {'top1': 0.9228} is_best False lr [0.010000000000000002]
training epoch 355 val accuracy 0.9244 topk_dict {'top1': 0.9244} is_best False lr [0.010000000000000002]
training epoch 356 val accuracy 0.9228 topk_dict {'top1': 0.9228} is_best False lr [0.010000000000000002]
training epoch 357 val accuracy 0.9224 topk_dict {'top1': 0.9224} is_best False lr [0.010000000000000002]
training epoch 358 val accuracy 0.9202 topk_dict {'top1': 0.9202} is_best False lr [0.010000000000000002]
training epoch 359 val accuracy 0.9198 topk_dict {'top1': 0.9198} is_best False lr [0.010000000000000002]
training epoch 360 val accuracy 0.919 topk_dict {'top1': 0.919} is_best False lr [0.010000000000000002]
training epoch 361 val accuracy 0.9222 topk_dict {'top1': 0.9222} is_best False lr [0.010000000000000002]
training epoch 362 val accuracy 0.9216 topk_dict {'top1': 0.9216} is_best False lr [0.010000000000000002]
training epoch 363 val accuracy 0.9174 topk_dict {'top1': 0.9174} is_best False lr [0.010000000000000002]
training epoch 364 val accuracy 0.922 topk_dict {'top1': 0.922} is_best False lr [0.010000000000000002]
training epoch 365 val accuracy 0.925 topk_dict {'top1': 0.925} is_best False lr [0.010000000000000002]
training epoch 366 val accuracy 0.9194 topk_dict {'top1': 0.9194} is_best False lr [0.010000000000000002]
training epoch 367 val accuracy 0.9184 topk_dict {'top1': 0.9184} is_best False lr [0.010000000000000002]
training epoch 368 val accuracy 0.9194 topk_dict {'top1': 0.9194} is_best False lr [0.010000000000000002]
training epoch 369 val accuracy 0.9238 topk_dict {'top1': 0.9238} is_best False lr [0.010000000000000002]
training epoch 370 val accuracy 0.9246 topk_dict {'top1': 0.9246} is_best False lr [0.010000000000000002]
training epoch 371 val accuracy 0.9196 topk_dict {'top1': 0.9196} is_best False lr [0.010000000000000002]
training epoch 372 val accuracy 0.921 topk_dict {'top1': 0.921} is_best False lr [0.010000000000000002]
training epoch 373 val accuracy 0.9202 topk_dict {'top1': 0.9202} is_best False lr [0.010000000000000002]
training epoch 374 val accuracy 0.9206 topk_dict {'top1': 0.9206} is_best False lr [0.010000000000000002]
training epoch 375 val accuracy 0.9234 topk_dict {'top1': 0.9234} is_best False lr [0.0010000000000000002]
training epoch 376 val accuracy 0.9258 topk_dict {'top1': 0.9258} is_best False lr [0.0010000000000000002]
training epoch 377 val accuracy 0.9246 topk_dict {'top1': 0.9246} is_best False lr [0.0010000000000000002]
training epoch 378 val accuracy 0.9264 topk_dict {'top1': 0.9264} is_best False lr [0.0010000000000000002]
training epoch 379 val accuracy 0.9252 topk_dict {'top1': 0.9252} is_best False lr [0.0010000000000000002]
training epoch 380 val accuracy 0.9266 topk_dict {'top1': 0.9266} is_best False lr [0.0010000000000000002]
training epoch 381 val accuracy 0.9268 topk_dict {'top1': 0.9268} is_best False lr [0.0010000000000000002]
training epoch 382 val accuracy 0.928 topk_dict {'top1': 0.928} is_best False lr [0.0010000000000000002]
training epoch 383 val accuracy 0.927 topk_dict {'top1': 0.927} is_best False lr [0.0010000000000000002]
training epoch 384 val accuracy 0.9286 topk_dict {'top1': 0.9286} is_best False lr [0.0010000000000000002]
training epoch 385 val accuracy 0.9284 topk_dict {'top1': 0.9284} is_best False lr [0.0010000000000000002]
training epoch 386 val accuracy 0.9276 topk_dict {'top1': 0.9276} is_best False lr [0.0010000000000000002]
training epoch 387 val accuracy 0.9268 topk_dict {'top1': 0.9268} is_best False lr [0.0010000000000000002]
training epoch 388 val accuracy 0.9284 topk_dict {'top1': 0.9284} is_best False lr [0.0010000000000000002]
training epoch 389 val accuracy 0.9292 topk_dict {'top1': 0.9292} is_best True lr [0.0010000000000000002]
training epoch 390 val accuracy 0.9274 topk_dict {'top1': 0.9274} is_best False lr [0.0010000000000000002]
training epoch 391 val accuracy 0.9298 topk_dict {'top1': 0.9298} is_best True lr [0.0010000000000000002]
training epoch 392 val accuracy 0.9286 topk_dict {'top1': 0.9286} is_best False lr [0.0010000000000000002]
training epoch 393 val accuracy 0.9286 topk_dict {'top1': 0.9286} is_best False lr [0.0010000000000000002]
training epoch 394 val accuracy 0.9288 topk_dict {'top1': 0.9288} is_best False lr [0.0010000000000000002]
training epoch 395 val accuracy 0.9274 topk_dict {'top1': 0.9274} is_best False lr [0.0010000000000000002]
training epoch 396 val accuracy 0.928 topk_dict {'top1': 0.928} is_best False lr [0.0010000000000000002]
training epoch 397 val accuracy 0.929 topk_dict {'top1': 0.929} is_best False lr [0.0010000000000000002]
training epoch 398 val accuracy 0.927 topk_dict {'top1': 0.927} is_best False lr [0.0010000000000000002]
training epoch 399 val accuracy 0.9274 topk_dict {'top1': 0.9274} is_best False lr [0.0010000000000000002]
training epoch 400 val accuracy 0.9274 topk_dict {'top1': 0.9274} is_best False lr [0.0010000000000000002]
training epoch 401 val accuracy 0.927 topk_dict {'top1': 0.927} is_best False lr [0.0010000000000000002]
training epoch 402 val accuracy 0.9294 topk_dict {'top1': 0.9294} is_best False lr [0.0010000000000000002]
training epoch 403 val accuracy 0.9294 topk_dict {'top1': 0.9294} is_best False lr [0.0010000000000000002]
training epoch 404 val accuracy 0.9278 topk_dict {'top1': 0.9278} is_best False lr [0.0010000000000000002]
training epoch 405 val accuracy 0.9296 topk_dict {'top1': 0.9296} is_best False lr [0.0010000000000000002]
training epoch 406 val accuracy 0.9284 topk_dict {'top1': 0.9284} is_best False lr [0.0010000000000000002]
training epoch 407 val accuracy 0.9292 topk_dict {'top1': 0.9292} is_best False lr [0.0010000000000000002]
training epoch 408 val accuracy 0.9292 topk_dict {'top1': 0.9292} is_best False lr [0.0010000000000000002]
training epoch 409 val accuracy 0.9296 topk_dict {'top1': 0.9296} is_best False lr [0.0010000000000000002]
training epoch 410 val accuracy 0.929 topk_dict {'top1': 0.929} is_best False lr [0.0010000000000000002]
training epoch 411 val accuracy 0.9286 topk_dict {'top1': 0.9286} is_best False lr [0.0010000000000000002]
training epoch 412 val accuracy 0.93 topk_dict {'top1': 0.93} is_best True lr [0.0010000000000000002]
training epoch 413 val accuracy 0.9292 topk_dict {'top1': 0.9292} is_best False lr [0.0010000000000000002]
training epoch 414 val accuracy 0.9288 topk_dict {'top1': 0.9288} is_best False lr [0.0010000000000000002]
training epoch 415 val accuracy 0.9296 topk_dict {'top1': 0.9296} is_best False lr [0.0010000000000000002]
training epoch 416 val accuracy 0.9274 topk_dict {'top1': 0.9274} is_best False lr [0.0010000000000000002]
training epoch 417 val accuracy 0.9276 topk_dict {'top1': 0.9276} is_best False lr [0.0010000000000000002]
training epoch 418 val accuracy 0.9298 topk_dict {'top1': 0.9298} is_best False lr [0.0010000000000000002]
training epoch 419 val accuracy 0.929 topk_dict {'top1': 0.929} is_best False lr [0.0010000000000000002]
training epoch 420 val accuracy 0.9294 topk_dict {'top1': 0.9294} is_best False lr [0.0010000000000000002]
training epoch 421 val accuracy 0.9286 topk_dict {'top1': 0.9286} is_best False lr [0.0010000000000000002]
training epoch 422 val accuracy 0.9288 topk_dict {'top1': 0.9288} is_best False lr [0.0010000000000000002]
training epoch 423 val accuracy 0.9276 topk_dict {'top1': 0.9276} is_best False lr [0.0010000000000000002]
training epoch 424 val accuracy 0.9276 topk_dict {'top1': 0.9276} is_best False lr [0.0010000000000000002]
training epoch 425 val accuracy 0.9284 topk_dict {'top1': 0.9284} is_best False lr [0.0010000000000000002]
training epoch 426 val accuracy 0.9276 topk_dict {'top1': 0.9276} is_best False lr [0.0010000000000000002]
training epoch 427 val accuracy 0.9278 topk_dict {'top1': 0.9278} is_best False lr [0.0010000000000000002]
training epoch 428 val accuracy 0.9304 topk_dict {'top1': 0.9304} is_best True lr [0.0010000000000000002]
training epoch 429 val accuracy 0.9296 topk_dict {'top1': 0.9296} is_best False lr [0.0010000000000000002]
training epoch 430 val accuracy 0.928 topk_dict {'top1': 0.928} is_best False lr [0.0010000000000000002]
training epoch 431 val accuracy 0.9294 topk_dict {'top1': 0.9294} is_best False lr [0.0010000000000000002]
training epoch 432 val accuracy 0.9284 topk_dict {'top1': 0.9284} is_best False lr [0.0010000000000000002]
training epoch 433 val accuracy 0.9296 topk_dict {'top1': 0.9296} is_best False lr [0.0010000000000000002]
training epoch 434 val accuracy 0.9288 topk_dict {'top1': 0.9288} is_best False lr [0.0010000000000000002]
training epoch 435 val accuracy 0.9302 topk_dict {'top1': 0.9302} is_best False lr [0.0010000000000000002]
training epoch 436 val accuracy 0.9294 topk_dict {'top1': 0.9294} is_best False lr [0.0010000000000000002]
training epoch 437 val accuracy 0.9294 topk_dict {'top1': 0.9294} is_best False lr [0.0010000000000000002]
training epoch 438 val accuracy 0.93 topk_dict {'top1': 0.93} is_best False lr [0.0010000000000000002]
training epoch 439 val accuracy 0.9288 topk_dict {'top1': 0.9288} is_best False lr [0.0010000000000000002]
training epoch 440 val accuracy 0.9292 topk_dict {'top1': 0.9292} is_best False lr [0.0010000000000000002]
training epoch 441 val accuracy 0.9298 topk_dict {'top1': 0.9298} is_best False lr [0.0010000000000000002]
training epoch 442 val accuracy 0.9292 topk_dict {'top1': 0.9292} is_best False lr [0.0010000000000000002]
training epoch 443 val accuracy 0.9284 topk_dict {'top1': 0.9284} is_best False lr [0.0010000000000000002]
training epoch 444 val accuracy 0.9294 topk_dict {'top1': 0.9294} is_best False lr [0.0010000000000000002]
training epoch 445 val accuracy 0.9288 topk_dict {'top1': 0.9288} is_best False lr [0.0010000000000000002]
training epoch 446 val accuracy 0.9292 topk_dict {'top1': 0.9292} is_best False lr [0.0010000000000000002]
training epoch 447 val accuracy 0.929 topk_dict {'top1': 0.929} is_best False lr [0.0010000000000000002]
training epoch 448 val accuracy 0.9288 topk_dict {'top1': 0.9288} is_best False lr [0.0010000000000000002]
training epoch 449 val accuracy 0.9304 topk_dict {'top1': 0.9304} is_best False lr [0.0010000000000000002]
training epoch 450 val accuracy 0.9278 topk_dict {'top1': 0.9278} is_best False lr [0.0010000000000000002]
training epoch 451 val accuracy 0.929 topk_dict {'top1': 0.929} is_best False lr [0.0010000000000000002]
training epoch 452 val accuracy 0.9302 topk_dict {'top1': 0.9302} is_best False lr [0.0010000000000000002]
training epoch 453 val accuracy 0.9294 topk_dict {'top1': 0.9294} is_best False lr [0.0010000000000000002]
training epoch 454 val accuracy 0.9296 topk_dict {'top1': 0.9296} is_best False lr [0.0010000000000000002]
training epoch 455 val accuracy 0.9298 topk_dict {'top1': 0.9298} is_best False lr [0.0010000000000000002]
training epoch 456 val accuracy 0.9298 topk_dict {'top1': 0.9298} is_best False lr [0.0010000000000000002]
training epoch 457 val accuracy 0.9282 topk_dict {'top1': 0.9282} is_best False lr [0.0010000000000000002]
training epoch 458 val accuracy 0.929 topk_dict {'top1': 0.929} is_best False lr [0.0010000000000000002]
training epoch 459 val accuracy 0.9262 topk_dict {'top1': 0.9262} is_best False lr [0.0010000000000000002]
training epoch 460 val accuracy 0.9274 topk_dict {'top1': 0.9274} is_best False lr [0.0010000000000000002]
training epoch 461 val accuracy 0.9286 topk_dict {'top1': 0.9286} is_best False lr [0.0010000000000000002]
training epoch 462 val accuracy 0.9284 topk_dict {'top1': 0.9284} is_best False lr [0.0010000000000000002]
training epoch 463 val accuracy 0.928 topk_dict {'top1': 0.928} is_best False lr [0.0010000000000000002]
training epoch 464 val accuracy 0.9302 topk_dict {'top1': 0.9302} is_best False lr [0.0010000000000000002]
training epoch 465 val accuracy 0.9274 topk_dict {'top1': 0.9274} is_best False lr [0.0010000000000000002]
training epoch 466 val accuracy 0.929 topk_dict {'top1': 0.929} is_best False lr [0.0010000000000000002]
training epoch 467 val accuracy 0.928 topk_dict {'top1': 0.928} is_best False lr [0.0010000000000000002]
training epoch 468 val accuracy 0.9286 topk_dict {'top1': 0.9286} is_best False lr [0.0010000000000000002]
training epoch 469 val accuracy 0.9292 topk_dict {'top1': 0.9292} is_best False lr [0.0010000000000000002]
training epoch 470 val accuracy 0.9284 topk_dict {'top1': 0.9284} is_best False lr [0.0010000000000000002]
training epoch 471 val accuracy 0.9286 topk_dict {'top1': 0.9286} is_best False lr [0.0010000000000000002]
training epoch 472 val accuracy 0.929 topk_dict {'top1': 0.929} is_best False lr [0.0010000000000000002]
training epoch 473 val accuracy 0.9288 topk_dict {'top1': 0.9288} is_best False lr [0.0010000000000000002]
training epoch 474 val accuracy 0.9282 topk_dict {'top1': 0.9282} is_best False lr [0.0010000000000000002]
training epoch 475 val accuracy 0.9274 topk_dict {'top1': 0.9274} is_best False lr [0.0010000000000000002]
training epoch 476 val accuracy 0.93 topk_dict {'top1': 0.93} is_best False lr [0.0010000000000000002]
training epoch 477 val accuracy 0.929 topk_dict {'top1': 0.929} is_best False lr [0.0010000000000000002]
training epoch 478 val accuracy 0.9262 topk_dict {'top1': 0.9262} is_best False lr [0.0010000000000000002]
training epoch 479 val accuracy 0.9286 topk_dict {'top1': 0.9286} is_best False lr [0.0010000000000000002]
training epoch 480 val accuracy 0.9292 topk_dict {'top1': 0.9292} is_best False lr [0.0010000000000000002]
training epoch 481 val accuracy 0.928 topk_dict {'top1': 0.928} is_best False lr [0.0010000000000000002]
training epoch 482 val accuracy 0.9272 topk_dict {'top1': 0.9272} is_best False lr [0.0010000000000000002]
training epoch 483 val accuracy 0.9284 topk_dict {'top1': 0.9284} is_best False lr [0.0010000000000000002]
training epoch 484 val accuracy 0.9278 topk_dict {'top1': 0.9278} is_best False lr [0.0010000000000000002]
training epoch 485 val accuracy 0.9284 topk_dict {'top1': 0.9284} is_best False lr [0.0010000000000000002]
training epoch 486 val accuracy 0.927 topk_dict {'top1': 0.927} is_best False lr [0.0010000000000000002]
training epoch 487 val accuracy 0.9272 topk_dict {'top1': 0.9272} is_best False lr [0.0010000000000000002]
training epoch 488 val accuracy 0.9266 topk_dict {'top1': 0.9266} is_best False lr [0.0010000000000000002]
training epoch 489 val accuracy 0.927 topk_dict {'top1': 0.927} is_best False lr [0.0010000000000000002]
training epoch 490 val accuracy 0.9278 topk_dict {'top1': 0.9278} is_best False lr [0.0010000000000000002]
training epoch 491 val accuracy 0.9274 topk_dict {'top1': 0.9274} is_best False lr [0.0010000000000000002]
training epoch 492 val accuracy 0.9284 topk_dict {'top1': 0.9284} is_best False lr [0.0010000000000000002]
training epoch 493 val accuracy 0.9282 topk_dict {'top1': 0.9282} is_best False lr [0.0010000000000000002]
training epoch 494 val accuracy 0.9282 topk_dict {'top1': 0.9282} is_best False lr [0.0010000000000000002]
training epoch 495 val accuracy 0.9272 topk_dict {'top1': 0.9272} is_best False lr [0.0010000000000000002]
training epoch 496 val accuracy 0.9294 topk_dict {'top1': 0.9294} is_best False lr [0.0010000000000000002]
training epoch 497 val accuracy 0.9278 topk_dict {'top1': 0.9278} is_best False lr [0.0010000000000000002]
training epoch 498 val accuracy 0.928 topk_dict {'top1': 0.928} is_best False lr [0.0010000000000000002]
training epoch 499 val accuracy 0.9284 topk_dict {'top1': 0.9284} is_best False lr [0.0010000000000000002]
loading model_best from epoch 428 (acc 0.930400)
finished training. finished 500 epochs. accuracy 0.9304 topk_dict {'top1': 0.9304}
